{
  "components": {
    "comp-bq-query-to-table": {
      "executorLabel": "exec-bq-query-to-table",
      "inputDefinitions": {
        "parameters": {
          "bq_client_project_id": {
            "parameterType": "STRING"
          },
          "dataset_id": {
            "isOptional": true,
            "parameterType": "STRING"
          },
          "dataset_location": {
            "defaultValue": "EU",
            "isOptional": true,
            "parameterType": "STRING"
          },
          "destination_project_id": {
            "parameterType": "STRING"
          },
          "query": {
            "parameterType": "STRING"
          },
          "query_job_config": {
            "isOptional": true,
            "parameterType": "STRUCT"
          },
          "table_id": {
            "isOptional": true,
            "parameterType": "STRING"
          }
        }
      }
    },
    "comp-compare-forecasts": {
      "executorLabel": "exec-compare-forecasts",
      "inputDefinitions": {
        "artifacts": {
          "baseline_predictions": {
            "artifactType": {
              "schemaTitle": "system.Dataset",
              "schemaVersion": "0.0.1"
            }
          },
          "multi_dc_predictions": {
            "artifactType": {
              "schemaTitle": "system.Dataset",
              "schemaVersion": "0.0.1"
            }
          },
          "single_dc_predictions": {
            "artifactType": {
              "schemaTitle": "system.Dataset",
              "schemaVersion": "0.0.1"
            }
          }
        }
      },
      "outputDefinitions": {
        "artifacts": {
          "metrics": {
            "artifactType": {
              "schemaTitle": "system.Metrics",
              "schemaVersion": "0.0.1"
            }
          },
          "report": {
            "artifactType": {
              "schemaTitle": "system.Dataset",
              "schemaVersion": "0.0.1"
            }
          },
          "visualization": {
            "artifactType": {
              "schemaTitle": "system.HTML",
              "schemaVersion": "0.0.1"
            }
          }
        }
      }
    },
    "comp-concat-threshold-features": {
      "executorLabel": "exec-concat-threshold-features",
      "inputDefinitions": {
        "artifacts": {
          "test_datasets": {
            "artifactType": {
              "schemaTitle": "system.Dataset",
              "schemaVersion": "0.0.1"
            },
            "isArtifactList": true
          },
          "train_datasets": {
            "artifactType": {
              "schemaTitle": "system.Dataset",
              "schemaVersion": "0.0.1"
            },
            "isArtifactList": true
          },
          "valid_datasets": {
            "artifactType": {
              "schemaTitle": "system.Dataset",
              "schemaVersion": "0.0.1"
            },
            "isArtifactList": true
          }
        },
        "parameters": {
          "common_feature_names": {
            "defaultValue": [
              "PRICE_std",
              "vol_quote_std",
              "cvd_quote_std"
            ],
            "isOptional": true,
            "parameterType": "LIST"
          },
          "compression": {
            "defaultValue": "GZIP",
            "isOptional": true,
            "parameterType": "STRING"
          },
          "feature_names": {
            "defaultValue": [
              "PRICE_std",
              "vol_quote_std",
              "cvd_quote_std",
              "PDCC_Down",
              "OSV_Down_std",
              "PDCC2_UP",
              "regime_up",
              "regime_down"
            ],
            "isOptional": true,
            "parameterType": "LIST"
          },
          "per_threshold_feature_names": {
            "defaultValue": [
              "PDCC_Down",
              "OSV_Down_std",
              "PDCC2_UP",
              "regime_up",
              "regime_down"
            ],
            "isOptional": true,
            "parameterType": "LIST"
          },
          "validate_y_equal": {
            "defaultValue": true,
            "isOptional": true,
            "parameterType": "BOOLEAN"
          },
          "y_atol": {
            "defaultValue": 1e-06,
            "isOptional": true,
            "parameterType": "NUMBER_DOUBLE"
          },
          "y_rtol": {
            "defaultValue": 1e-06,
            "isOptional": true,
            "parameterType": "NUMBER_DOUBLE"
          }
        }
      },
      "outputDefinitions": {
        "artifacts": {
          "joint_test": {
            "artifactType": {
              "schemaTitle": "system.Dataset",
              "schemaVersion": "0.0.1"
            }
          },
          "joint_train": {
            "artifactType": {
              "schemaTitle": "system.Dataset",
              "schemaVersion": "0.0.1"
            }
          },
          "joint_valid": {
            "artifactType": {
              "schemaTitle": "system.Dataset",
              "schemaVersion": "0.0.1"
            }
          },
          "metrics": {
            "artifactType": {
              "schemaTitle": "system.Metrics",
              "schemaVersion": "0.0.1"
            }
          }
        }
      }
    },
    "comp-concat-threshold-features-2": {
      "executorLabel": "exec-concat-threshold-features-2",
      "inputDefinitions": {
        "artifacts": {
          "test_datasets": {
            "artifactType": {
              "schemaTitle": "system.Dataset",
              "schemaVersion": "0.0.1"
            },
            "isArtifactList": true
          },
          "train_datasets": {
            "artifactType": {
              "schemaTitle": "system.Dataset",
              "schemaVersion": "0.0.1"
            },
            "isArtifactList": true
          },
          "valid_datasets": {
            "artifactType": {
              "schemaTitle": "system.Dataset",
              "schemaVersion": "0.0.1"
            },
            "isArtifactList": true
          }
        },
        "parameters": {
          "common_feature_names": {
            "defaultValue": [
              "PRICE_std",
              "vol_quote_std",
              "cvd_quote_std"
            ],
            "isOptional": true,
            "parameterType": "LIST"
          },
          "compression": {
            "defaultValue": "GZIP",
            "isOptional": true,
            "parameterType": "STRING"
          },
          "feature_names": {
            "defaultValue": [
              "PRICE_std",
              "vol_quote_std",
              "cvd_quote_std",
              "PDCC_Down",
              "OSV_Down_std",
              "PDCC2_UP",
              "regime_up",
              "regime_down"
            ],
            "isOptional": true,
            "parameterType": "LIST"
          },
          "per_threshold_feature_names": {
            "defaultValue": [
              "PDCC_Down",
              "OSV_Down_std",
              "PDCC2_UP",
              "regime_up",
              "regime_down"
            ],
            "isOptional": true,
            "parameterType": "LIST"
          },
          "validate_y_equal": {
            "defaultValue": true,
            "isOptional": true,
            "parameterType": "BOOLEAN"
          },
          "y_atol": {
            "defaultValue": 1e-06,
            "isOptional": true,
            "parameterType": "NUMBER_DOUBLE"
          },
          "y_rtol": {
            "defaultValue": 1e-06,
            "isOptional": true,
            "parameterType": "NUMBER_DOUBLE"
          }
        }
      },
      "outputDefinitions": {
        "artifacts": {
          "joint_test": {
            "artifactType": {
              "schemaTitle": "system.Dataset",
              "schemaVersion": "0.0.1"
            }
          },
          "joint_train": {
            "artifactType": {
              "schemaTitle": "system.Dataset",
              "schemaVersion": "0.0.1"
            }
          },
          "joint_valid": {
            "artifactType": {
              "schemaTitle": "system.Dataset",
              "schemaVersion": "0.0.1"
            }
          },
          "metrics": {
            "artifactType": {
              "schemaTitle": "system.Metrics",
              "schemaVersion": "0.0.1"
            }
          }
        }
      }
    },
    "comp-custom-forecast-train-job": {
      "executorLabel": "exec-custom-forecast-train-job",
      "inputDefinitions": {
        "artifacts": {
          "test_data": {
            "artifactType": {
              "schemaTitle": "system.Dataset",
              "schemaVersion": "0.0.1"
            },
            "description": "Test data (passed as an argument to train script)."
          },
          "train_data": {
            "artifactType": {
              "schemaTitle": "system.Dataset",
              "schemaVersion": "0.0.1"
            },
            "description": "Training data (passed as an argument to train script)"
          },
          "valid_data": {
            "artifactType": {
              "schemaTitle": "system.Dataset",
              "schemaVersion": "0.0.1"
            },
            "description": "Validation data (passed as an argument to train script)"
          }
        },
        "parameters": {
          "accelerator_count": {
            "defaultValue": 0.0,
            "description": "Accelerator count (increase for GPU cores).",
            "isOptional": true,
            "parameterType": "NUMBER_INTEGER"
          },
          "accelerator_type": {
            "defaultValue": "ACCELERATOR_TYPE_UNSPECIFIED",
            "description": "Accelerator type (change for GPU support).",
            "isOptional": true,
            "parameterType": "STRING"
          },
          "hparams": {
            "description": "Hyperparameters (passed as a JSON serialised argument\nto train script)",
            "isOptional": true,
            "parameterType": "STRUCT"
          },
          "job_name": {
            "description": "Name of training job.",
            "isOptional": true,
            "parameterType": "STRING"
          },
          "machine_type": {
            "defaultValue": "n1-standard-4",
            "description": "Machine type of compute.",
            "isOptional": true,
            "parameterType": "STRING"
          },
          "model_display_name": {
            "description": "Name of the new trained model version.",
            "isOptional": true,
            "parameterType": "STRING"
          },
          "parent_model": {
            "description": "Resource name of existing parent model (optional).\nIf `None`, a new model will be uploaded. Otherwise, a new model version\nfor the parent model will be uploaded.",
            "isOptional": true,
            "parameterType": "STRING"
          },
          "project_id": {
            "description": "project id of the Google Cloud project.",
            "parameterType": "STRING"
          },
          "project_location": {
            "description": "location of the Google Cloud project.",
            "parameterType": "STRING"
          },
          "replica_count": {
            "defaultValue": 1.0,
            "description": "Number of replicas (increase for distributed training).",
            "isOptional": true,
            "parameterType": "NUMBER_INTEGER"
          },
          "requirements": {
            "description": "Additional python dependencies for training script.",
            "isOptional": true,
            "parameterType": "LIST"
          },
          "service_account": {
            "defaultValue": "vertex-pipelines@derivatives-417104.iam.gserviceaccount.com",
            "isOptional": true,
            "parameterType": "STRING"
          },
          "serving_container_uri": {
            "description": "Container URI for deploying the output model.",
            "parameterType": "STRING"
          },
          "staging_bucket": {
            "description": "Staging bucket for CustomTrainingJob.",
            "parameterType": "STRING"
          },
          "tensorboard_resource_name": {
            "defaultValue": "projects/derivatives-417104/locations/us-central1/tensorboards/2135763953559535616",
            "isOptional": true,
            "parameterType": "STRING"
          },
          "threshold": {
            "parameterType": "NUMBER_DOUBLE"
          },
          "train_container_uri": {
            "description": "Container URI for running train script.",
            "parameterType": "STRING"
          },
          "train_script_uri": {
            "description": "gs:// uri to python train script. See:\nhttps://cloud.google.com/vertex-ai/docs/training/code-requirements.",
            "parameterType": "STRING"
          }
        }
      },
      "outputDefinitions": {
        "artifacts": {
          "metrics": {
            "artifactType": {
              "schemaTitle": "system.Metrics",
              "schemaVersion": "0.0.1"
            }
          },
          "model": {
            "artifactType": {
              "schemaTitle": "system.Artifact",
              "schemaVersion": "0.0.1"
            }
          },
          "predictions": {
            "artifactType": {
              "schemaTitle": "system.Dataset",
              "schemaVersion": "0.0.1"
            }
          }
        }
      }
    },
    "comp-custom-forecast-train-job-2": {
      "executorLabel": "exec-custom-forecast-train-job-2",
      "inputDefinitions": {
        "artifacts": {
          "test_data": {
            "artifactType": {
              "schemaTitle": "system.Dataset",
              "schemaVersion": "0.0.1"
            },
            "description": "Test data (passed as an argument to train script)."
          },
          "train_data": {
            "artifactType": {
              "schemaTitle": "system.Dataset",
              "schemaVersion": "0.0.1"
            },
            "description": "Training data (passed as an argument to train script)"
          },
          "valid_data": {
            "artifactType": {
              "schemaTitle": "system.Dataset",
              "schemaVersion": "0.0.1"
            },
            "description": "Validation data (passed as an argument to train script)"
          }
        },
        "parameters": {
          "accelerator_count": {
            "defaultValue": 0.0,
            "description": "Accelerator count (increase for GPU cores).",
            "isOptional": true,
            "parameterType": "NUMBER_INTEGER"
          },
          "accelerator_type": {
            "defaultValue": "ACCELERATOR_TYPE_UNSPECIFIED",
            "description": "Accelerator type (change for GPU support).",
            "isOptional": true,
            "parameterType": "STRING"
          },
          "hparams": {
            "description": "Hyperparameters (passed as a JSON serialised argument\nto train script)",
            "isOptional": true,
            "parameterType": "STRUCT"
          },
          "job_name": {
            "description": "Name of training job.",
            "isOptional": true,
            "parameterType": "STRING"
          },
          "machine_type": {
            "defaultValue": "n1-standard-4",
            "description": "Machine type of compute.",
            "isOptional": true,
            "parameterType": "STRING"
          },
          "model_display_name": {
            "description": "Name of the new trained model version.",
            "isOptional": true,
            "parameterType": "STRING"
          },
          "parent_model": {
            "description": "Resource name of existing parent model (optional).\nIf `None`, a new model will be uploaded. Otherwise, a new model version\nfor the parent model will be uploaded.",
            "isOptional": true,
            "parameterType": "STRING"
          },
          "project_id": {
            "description": "project id of the Google Cloud project.",
            "parameterType": "STRING"
          },
          "project_location": {
            "description": "location of the Google Cloud project.",
            "parameterType": "STRING"
          },
          "replica_count": {
            "defaultValue": 1.0,
            "description": "Number of replicas (increase for distributed training).",
            "isOptional": true,
            "parameterType": "NUMBER_INTEGER"
          },
          "requirements": {
            "description": "Additional python dependencies for training script.",
            "isOptional": true,
            "parameterType": "LIST"
          },
          "service_account": {
            "defaultValue": "vertex-pipelines@derivatives-417104.iam.gserviceaccount.com",
            "isOptional": true,
            "parameterType": "STRING"
          },
          "serving_container_uri": {
            "description": "Container URI for deploying the output model.",
            "parameterType": "STRING"
          },
          "staging_bucket": {
            "description": "Staging bucket for CustomTrainingJob.",
            "parameterType": "STRING"
          },
          "tensorboard_resource_name": {
            "defaultValue": "projects/derivatives-417104/locations/us-central1/tensorboards/2135763953559535616",
            "isOptional": true,
            "parameterType": "STRING"
          },
          "threshold": {
            "parameterType": "NUMBER_DOUBLE"
          },
          "train_container_uri": {
            "description": "Container URI for running train script.",
            "parameterType": "STRING"
          },
          "train_script_uri": {
            "description": "gs:// uri to python train script. See:\nhttps://cloud.google.com/vertex-ai/docs/training/code-requirements.",
            "parameterType": "STRING"
          }
        }
      },
      "outputDefinitions": {
        "artifacts": {
          "metrics": {
            "artifactType": {
              "schemaTitle": "system.Metrics",
              "schemaVersion": "0.0.1"
            }
          },
          "model": {
            "artifactType": {
              "schemaTitle": "system.Artifact",
              "schemaVersion": "0.0.1"
            }
          },
          "predictions": {
            "artifactType": {
              "schemaTitle": "system.Dataset",
              "schemaVersion": "0.0.1"
            }
          }
        }
      }
    },
    "comp-custom-forecast-train-job-3": {
      "executorLabel": "exec-custom-forecast-train-job-3",
      "inputDefinitions": {
        "artifacts": {
          "test_data": {
            "artifactType": {
              "schemaTitle": "system.Dataset",
              "schemaVersion": "0.0.1"
            },
            "description": "Test data (passed as an argument to train script)."
          },
          "train_data": {
            "artifactType": {
              "schemaTitle": "system.Dataset",
              "schemaVersion": "0.0.1"
            },
            "description": "Training data (passed as an argument to train script)"
          },
          "valid_data": {
            "artifactType": {
              "schemaTitle": "system.Dataset",
              "schemaVersion": "0.0.1"
            },
            "description": "Validation data (passed as an argument to train script)"
          }
        },
        "parameters": {
          "accelerator_count": {
            "defaultValue": 0.0,
            "description": "Accelerator count (increase for GPU cores).",
            "isOptional": true,
            "parameterType": "NUMBER_INTEGER"
          },
          "accelerator_type": {
            "defaultValue": "ACCELERATOR_TYPE_UNSPECIFIED",
            "description": "Accelerator type (change for GPU support).",
            "isOptional": true,
            "parameterType": "STRING"
          },
          "hparams": {
            "description": "Hyperparameters (passed as a JSON serialised argument\nto train script)",
            "isOptional": true,
            "parameterType": "STRUCT"
          },
          "job_name": {
            "description": "Name of training job.",
            "isOptional": true,
            "parameterType": "STRING"
          },
          "machine_type": {
            "defaultValue": "n1-standard-4",
            "description": "Machine type of compute.",
            "isOptional": true,
            "parameterType": "STRING"
          },
          "model_display_name": {
            "description": "Name of the new trained model version.",
            "isOptional": true,
            "parameterType": "STRING"
          },
          "parent_model": {
            "description": "Resource name of existing parent model (optional).\nIf `None`, a new model will be uploaded. Otherwise, a new model version\nfor the parent model will be uploaded.",
            "isOptional": true,
            "parameterType": "STRING"
          },
          "project_id": {
            "description": "project id of the Google Cloud project.",
            "parameterType": "STRING"
          },
          "project_location": {
            "description": "location of the Google Cloud project.",
            "parameterType": "STRING"
          },
          "replica_count": {
            "defaultValue": 1.0,
            "description": "Number of replicas (increase for distributed training).",
            "isOptional": true,
            "parameterType": "NUMBER_INTEGER"
          },
          "requirements": {
            "description": "Additional python dependencies for training script.",
            "isOptional": true,
            "parameterType": "LIST"
          },
          "service_account": {
            "defaultValue": "vertex-pipelines@derivatives-417104.iam.gserviceaccount.com",
            "isOptional": true,
            "parameterType": "STRING"
          },
          "serving_container_uri": {
            "description": "Container URI for deploying the output model.",
            "parameterType": "STRING"
          },
          "staging_bucket": {
            "description": "Staging bucket for CustomTrainingJob.",
            "parameterType": "STRING"
          },
          "tensorboard_resource_name": {
            "defaultValue": "projects/derivatives-417104/locations/us-central1/tensorboards/2135763953559535616",
            "isOptional": true,
            "parameterType": "STRING"
          },
          "threshold": {
            "parameterType": "NUMBER_DOUBLE"
          },
          "train_container_uri": {
            "description": "Container URI for running train script.",
            "parameterType": "STRING"
          },
          "train_script_uri": {
            "description": "gs:// uri to python train script. See:\nhttps://cloud.google.com/vertex-ai/docs/training/code-requirements.",
            "parameterType": "STRING"
          }
        }
      },
      "outputDefinitions": {
        "artifacts": {
          "metrics": {
            "artifactType": {
              "schemaTitle": "system.Metrics",
              "schemaVersion": "0.0.1"
            }
          },
          "model": {
            "artifactType": {
              "schemaTitle": "system.Artifact",
              "schemaVersion": "0.0.1"
            }
          },
          "predictions": {
            "artifactType": {
              "schemaTitle": "system.Dataset",
              "schemaVersion": "0.0.1"
            }
          }
        }
      }
    },
    "comp-directional-change-detector": {
      "executorLabel": "exec-directional-change-detector",
      "inputDefinitions": {
        "artifacts": {
          "df": {
            "artifactType": {
              "schemaTitle": "system.Dataset",
              "schemaVersion": "0.0.1"
            }
          }
        },
        "parameters": {
          "dataset_id": {
            "parameterType": "STRING"
          },
          "fast_plot": {
            "parameterType": "BOOLEAN"
          },
          "load_to_bq": {
            "parameterType": "BOOLEAN"
          },
          "max_event_markers": {
            "parameterType": "NUMBER_INTEGER"
          },
          "price_col": {
            "parameterType": "STRING"
          },
          "project_id": {
            "parameterType": "STRING"
          },
          "thresholds": {
            "parameterType": "NUMBER_DOUBLE"
          },
          "time_col": {
            "parameterType": "STRING"
          }
        }
      },
      "outputDefinitions": {
        "artifacts": {
          "directional_change_events": {
            "artifactType": {
              "schemaTitle": "system.Dataset",
              "schemaVersion": "0.0.1"
            }
          },
          "figures_output": {
            "artifactType": {
              "schemaTitle": "system.Dataset",
              "schemaVersion": "0.0.1"
            }
          }
        }
      }
    },
    "comp-directional-change-detector-2": {
      "executorLabel": "exec-directional-change-detector-2",
      "inputDefinitions": {
        "artifacts": {
          "df": {
            "artifactType": {
              "schemaTitle": "system.Dataset",
              "schemaVersion": "0.0.1"
            }
          }
        },
        "parameters": {
          "dataset_id": {
            "parameterType": "STRING"
          },
          "fast_plot": {
            "parameterType": "BOOLEAN"
          },
          "load_to_bq": {
            "parameterType": "BOOLEAN"
          },
          "max_event_markers": {
            "parameterType": "NUMBER_INTEGER"
          },
          "price_col": {
            "parameterType": "STRING"
          },
          "project_id": {
            "parameterType": "STRING"
          },
          "thresholds": {
            "parameterType": "NUMBER_DOUBLE"
          },
          "time_col": {
            "parameterType": "STRING"
          }
        }
      },
      "outputDefinitions": {
        "artifacts": {
          "directional_change_events": {
            "artifactType": {
              "schemaTitle": "system.Dataset",
              "schemaVersion": "0.0.1"
            }
          },
          "figures_output": {
            "artifactType": {
              "schemaTitle": "system.Dataset",
              "schemaVersion": "0.0.1"
            }
          }
        }
      }
    },
    "comp-extract-bq-to-dataset": {
      "executorLabel": "exec-extract-bq-to-dataset",
      "inputDefinitions": {
        "parameters": {
          "bq_client_project_id": {
            "description": "project id that will be used by the bq client",
            "parameterType": "STRING"
          },
          "dataset_id": {
            "description": "dataset id from where BQ table will be extracted",
            "parameterType": "STRING"
          },
          "dataset_location": {
            "defaultValue": "US",
            "description": "bq dataset location. Defaults to \"EU\".",
            "isOptional": true,
            "parameterType": "STRING"
          },
          "destination_gcs_uri": {
            "description": "GCS URI to use for saving query results (optional).",
            "isOptional": true,
            "parameterType": "STRING"
          },
          "extract_job_config": {
            "description": "dict containing optional parameters\nrequired by the bq extract operation. Defaults to None.\nSee available parameters here\nhttps://googleapis.dev/python/bigquery/latest/generated/google.cloud.bigquery.job.ExtractJobConfig.html # noqa",
            "isOptional": true,
            "parameterType": "STRUCT"
          },
          "skip_if_exists": {
            "defaultValue": true,
            "isOptional": true,
            "parameterType": "BOOLEAN"
          },
          "source_project_id": {
            "description": "project id from where BQ table will be extracted",
            "parameterType": "STRING"
          },
          "table_name": {
            "description": "table name (without project id and dataset id)",
            "parameterType": "STRING"
          }
        }
      },
      "outputDefinitions": {
        "artifacts": {
          "dataset": {
            "artifactType": {
              "schemaTitle": "system.Dataset",
              "schemaVersion": "0.0.1"
            }
          }
        }
      }
    },
    "comp-feature-engineering": {
      "executorLabel": "exec-feature-engineering",
      "inputDefinitions": {
        "artifacts": {
          "dc_summary": {
            "artifactType": {
              "schemaTitle": "system.Dataset",
              "schemaVersion": "0.0.1"
            }
          },
          "raw_ticks": {
            "artifactType": {
              "schemaTitle": "system.Dataset",
              "schemaVersion": "0.0.1"
            }
          }
        },
        "parameters": {
          "columns_json": {
            "defaultValue": "",
            "isOptional": true,
            "parameterType": "STRING"
          },
          "threshold": {
            "parameterType": "NUMBER_DOUBLE"
          }
        }
      },
      "outputDefinitions": {
        "artifacts": {
          "dc_ticks": {
            "artifactType": {
              "schemaTitle": "system.Dataset",
              "schemaVersion": "0.0.1"
            }
          },
          "metrics": {
            "artifactType": {
              "schemaTitle": "system.Metrics",
              "schemaVersion": "0.0.1"
            }
          }
        }
      }
    },
    "comp-feature-engineering-2": {
      "executorLabel": "exec-feature-engineering-2",
      "inputDefinitions": {
        "artifacts": {
          "dc_summary": {
            "artifactType": {
              "schemaTitle": "system.Dataset",
              "schemaVersion": "0.0.1"
            }
          },
          "raw_ticks": {
            "artifactType": {
              "schemaTitle": "system.Dataset",
              "schemaVersion": "0.0.1"
            }
          }
        },
        "parameters": {
          "columns_json": {
            "defaultValue": "",
            "isOptional": true,
            "parameterType": "STRING"
          },
          "threshold": {
            "parameterType": "NUMBER_DOUBLE"
          }
        }
      },
      "outputDefinitions": {
        "artifacts": {
          "dc_ticks": {
            "artifactType": {
              "schemaTitle": "system.Dataset",
              "schemaVersion": "0.0.1"
            }
          },
          "metrics": {
            "artifactType": {
              "schemaTitle": "system.Metrics",
              "schemaVersion": "0.0.1"
            }
          }
        }
      }
    },
    "comp-for-loop-1": {
      "dag": {
        "outputs": {
          "artifacts": {
            "pipelinechannel--window-dataset-test_windowed": {
              "artifactSelectors": [
                {
                  "outputArtifactKey": "test_windowed",
                  "producerSubtask": "window-dataset"
                }
              ]
            },
            "pipelinechannel--window-dataset-train_windowed": {
              "artifactSelectors": [
                {
                  "outputArtifactKey": "train_windowed",
                  "producerSubtask": "window-dataset"
                }
              ]
            },
            "pipelinechannel--window-dataset-valid_windowed": {
              "artifactSelectors": [
                {
                  "outputArtifactKey": "valid_windowed",
                  "producerSubtask": "window-dataset"
                }
              ]
            }
          }
        },
        "tasks": {
          "directional-change-detector": {
            "cachingOptions": {
              "enableCache": true
            },
            "componentRef": {
              "name": "comp-directional-change-detector"
            },
            "inputs": {
              "artifacts": {
                "df": {
                  "componentInputArtifact": "pipelinechannel--extract-bq-to-dataset-dataset"
                }
              },
              "parameters": {
                "dataset_id": {
                  "componentInputParameter": "pipelinechannel--dataset_id"
                },
                "fast_plot": {
                  "runtimeValue": {
                    "constant": true
                  }
                },
                "load_to_bq": {
                  "runtimeValue": {
                    "constant": true
                  }
                },
                "max_event_markers": {
                  "runtimeValue": {
                    "constant": 10.0
                  }
                },
                "price_col": {
                  "runtimeValue": {
                    "constant": "PRICE"
                  }
                },
                "project_id": {
                  "componentInputParameter": "pipelinechannel--project_id"
                },
                "thresholds": {
                  "componentInputParameter": "pipelinechannel--dc_thresholds-loop-item"
                },
                "time_col": {
                  "runtimeValue": {
                    "constant": "trade_ts"
                  }
                }
              }
            },
            "taskInfo": {
              "name": "DC Detection"
            }
          },
          "feature-engineering": {
            "cachingOptions": {
              "enableCache": true
            },
            "componentRef": {
              "name": "comp-feature-engineering"
            },
            "dependentTasks": [
              "directional-change-detector"
            ],
            "inputs": {
              "artifacts": {
                "dc_summary": {
                  "taskOutputArtifact": {
                    "outputArtifactKey": "directional_change_events",
                    "producerTask": "directional-change-detector"
                  }
                },
                "raw_ticks": {
                  "componentInputArtifact": "pipelinechannel--extract-bq-to-dataset-dataset"
                }
              },
              "parameters": {
                "threshold": {
                  "componentInputParameter": "pipelinechannel--dc_thresholds-loop-item"
                }
              }
            },
            "taskInfo": {
              "name": "Feature Engineering"
            }
          },
          "tf-data-splitter": {
            "cachingOptions": {
              "enableCache": true
            },
            "componentRef": {
              "name": "comp-tf-data-splitter"
            },
            "dependentTasks": [
              "feature-engineering"
            ],
            "inputs": {
              "artifacts": {
                "dataset": {
                  "taskOutputArtifact": {
                    "outputArtifactKey": "dc_ticks",
                    "producerTask": "feature-engineering"
                  }
                }
              }
            },
            "taskInfo": {
              "name": "Train/Val/Test Split"
            }
          },
          "window-dataset": {
            "cachingOptions": {
              "enableCache": true
            },
            "componentRef": {
              "name": "comp-window-dataset"
            },
            "dependentTasks": [
              "tf-data-splitter"
            ],
            "inputs": {
              "artifacts": {
                "test_data": {
                  "taskOutputArtifact": {
                    "outputArtifactKey": "test_data",
                    "producerTask": "tf-data-splitter"
                  }
                },
                "train_data": {
                  "taskOutputArtifact": {
                    "outputArtifactKey": "train_data",
                    "producerTask": "tf-data-splitter"
                  }
                },
                "valid_data": {
                  "taskOutputArtifact": {
                    "outputArtifactKey": "valid_data",
                    "producerTask": "tf-data-splitter"
                  }
                }
              },
              "parameters": {
                "batch_size": {
                  "runtimeValue": {
                    "constant": 32.0
                  }
                },
                "feature_names": {
                  "runtimeValue": {
                    "constant": [
                      "PRICE_std",
                      "vol_quote_std",
                      "cvd_quote_std",
                      "PDCC_Down",
                      "OSV_Down_std",
                      "OSV_Up_std",
                      "PDCC2_UP",
                      "regime_up",
                      "regime_down"
                    ]
                  }
                },
                "input_width": {
                  "componentInputParameter": "pipelinechannel--input_width"
                },
                "label_columns": {
                  "runtimeValue": {
                    "constant": [
                      "PRICE_std"
                    ]
                  }
                },
                "label_width": {
                  "componentInputParameter": "pipelinechannel--label_width"
                },
                "shift": {
                  "componentInputParameter": "pipelinechannel--shift"
                }
              }
            },
            "taskInfo": {
              "name": "Window (9 features)"
            }
          }
        }
      },
      "inputDefinitions": {
        "artifacts": {
          "pipelinechannel--extract-bq-to-dataset-dataset": {
            "artifactType": {
              "schemaTitle": "system.Dataset",
              "schemaVersion": "0.0.1"
            }
          }
        },
        "parameters": {
          "pipelinechannel--dataset_id": {
            "parameterType": "STRING"
          },
          "pipelinechannel--dc_thresholds": {
            "parameterType": "LIST"
          },
          "pipelinechannel--dc_thresholds-loop-item": {
            "parameterType": "STRING"
          },
          "pipelinechannel--input_width": {
            "parameterType": "NUMBER_INTEGER"
          },
          "pipelinechannel--label_width": {
            "parameterType": "NUMBER_INTEGER"
          },
          "pipelinechannel--project_id": {
            "parameterType": "STRING"
          },
          "pipelinechannel--shift": {
            "parameterType": "NUMBER_INTEGER"
          }
        }
      },
      "outputDefinitions": {
        "artifacts": {
          "pipelinechannel--window-dataset-test_windowed": {
            "artifactType": {
              "schemaTitle": "system.Dataset",
              "schemaVersion": "0.0.1"
            },
            "isArtifactList": true
          },
          "pipelinechannel--window-dataset-train_windowed": {
            "artifactType": {
              "schemaTitle": "system.Dataset",
              "schemaVersion": "0.0.1"
            },
            "isArtifactList": true
          },
          "pipelinechannel--window-dataset-valid_windowed": {
            "artifactType": {
              "schemaTitle": "system.Dataset",
              "schemaVersion": "0.0.1"
            },
            "isArtifactList": true
          }
        }
      }
    },
    "comp-tf-data-splitter": {
      "executorLabel": "exec-tf-data-splitter",
      "inputDefinitions": {
        "artifacts": {
          "dataset": {
            "artifactType": {
              "schemaTitle": "system.Dataset",
              "schemaVersion": "0.0.1"
            },
            "description": "Input dataset dir (expects files under dataset.uri)"
          }
        },
        "parameters": {
          "columns_json": {
            "defaultValue": "",
            "description": "Optional JSON mapping of column names to adapt to other schemas.",
            "isOptional": true,
            "parameterType": "STRING"
          },
          "compress": {
            "defaultValue": true,
            "description": "If True, uses GZIP compression for TFRecords",
            "isOptional": true,
            "parameterType": "BOOLEAN"
          },
          "num_shards": {
            "defaultValue": 16.0,
            "description": "Number of TFRecord shards per split (default 16)",
            "isOptional": true,
            "parameterType": "NUMBER_INTEGER"
          },
          "scale_indicators": {
            "defaultValue": false,
            "isOptional": true,
            "parameterType": "BOOLEAN"
          }
        }
      },
      "outputDefinitions": {
        "artifacts": {
          "scaler_artifact": {
            "artifactType": {
              "schemaTitle": "system.Artifact",
              "schemaVersion": "0.0.1"
            }
          },
          "test_data": {
            "artifactType": {
              "schemaTitle": "system.Dataset",
              "schemaVersion": "0.0.1"
            }
          },
          "train_data": {
            "artifactType": {
              "schemaTitle": "system.Dataset",
              "schemaVersion": "0.0.1"
            }
          },
          "valid_data": {
            "artifactType": {
              "schemaTitle": "system.Dataset",
              "schemaVersion": "0.0.1"
            }
          }
        }
      }
    },
    "comp-tf-data-splitter-2": {
      "executorLabel": "exec-tf-data-splitter-2",
      "inputDefinitions": {
        "artifacts": {
          "dataset": {
            "artifactType": {
              "schemaTitle": "system.Dataset",
              "schemaVersion": "0.0.1"
            },
            "description": "Input dataset dir (expects files under dataset.uri)"
          }
        },
        "parameters": {
          "columns_json": {
            "defaultValue": "",
            "description": "Optional JSON mapping of column names to adapt to other schemas.",
            "isOptional": true,
            "parameterType": "STRING"
          },
          "compress": {
            "defaultValue": true,
            "description": "If True, uses GZIP compression for TFRecords",
            "isOptional": true,
            "parameterType": "BOOLEAN"
          },
          "num_shards": {
            "defaultValue": 16.0,
            "description": "Number of TFRecord shards per split (default 16)",
            "isOptional": true,
            "parameterType": "NUMBER_INTEGER"
          },
          "scale_indicators": {
            "defaultValue": false,
            "isOptional": true,
            "parameterType": "BOOLEAN"
          }
        }
      },
      "outputDefinitions": {
        "artifacts": {
          "scaler_artifact": {
            "artifactType": {
              "schemaTitle": "system.Artifact",
              "schemaVersion": "0.0.1"
            }
          },
          "test_data": {
            "artifactType": {
              "schemaTitle": "system.Dataset",
              "schemaVersion": "0.0.1"
            }
          },
          "train_data": {
            "artifactType": {
              "schemaTitle": "system.Dataset",
              "schemaVersion": "0.0.1"
            }
          },
          "valid_data": {
            "artifactType": {
              "schemaTitle": "system.Dataset",
              "schemaVersion": "0.0.1"
            }
          }
        }
      }
    },
    "comp-window-dataset": {
      "executorLabel": "exec-window-dataset",
      "inputDefinitions": {
        "artifacts": {
          "test_data": {
            "artifactType": {
              "schemaTitle": "system.Dataset",
              "schemaVersion": "0.0.1"
            }
          },
          "train_data": {
            "artifactType": {
              "schemaTitle": "system.Dataset",
              "schemaVersion": "0.0.1"
            }
          },
          "valid_data": {
            "artifactType": {
              "schemaTitle": "system.Dataset",
              "schemaVersion": "0.0.1"
            }
          }
        },
        "parameters": {
          "batch_size": {
            "defaultValue": 32.0,
            "isOptional": true,
            "parameterType": "NUMBER_INTEGER"
          },
          "feature_names": {
            "defaultValue": [
              "PRICE_std",
              "vol_quote_std",
              "cvd_quote_std",
              "PDCC_Down",
              "OSV_Down_std",
              "PDCC2_UP",
              "regime_up",
              "regime_down"
            ],
            "isOptional": true,
            "parameterType": "LIST"
          },
          "input_width": {
            "defaultValue": 50.0,
            "isOptional": true,
            "parameterType": "NUMBER_INTEGER"
          },
          "label_columns": {
            "defaultValue": [
              "PRICE_std"
            ],
            "isOptional": true,
            "parameterType": "LIST"
          },
          "label_width": {
            "defaultValue": 1.0,
            "isOptional": true,
            "parameterType": "NUMBER_INTEGER"
          },
          "shift": {
            "defaultValue": 50.0,
            "isOptional": true,
            "parameterType": "NUMBER_INTEGER"
          }
        }
      },
      "outputDefinitions": {
        "artifacts": {
          "metrics": {
            "artifactType": {
              "schemaTitle": "system.Metrics",
              "schemaVersion": "0.0.1"
            }
          },
          "plots_output": {
            "artifactType": {
              "schemaTitle": "system.HTML",
              "schemaVersion": "0.0.1"
            }
          },
          "test_windowed": {
            "artifactType": {
              "schemaTitle": "system.Dataset",
              "schemaVersion": "0.0.1"
            }
          },
          "train_windowed": {
            "artifactType": {
              "schemaTitle": "system.Dataset",
              "schemaVersion": "0.0.1"
            }
          },
          "valid_windowed": {
            "artifactType": {
              "schemaTitle": "system.Dataset",
              "schemaVersion": "0.0.1"
            }
          }
        }
      }
    },
    "comp-window-dataset-2": {
      "executorLabel": "exec-window-dataset-2",
      "inputDefinitions": {
        "artifacts": {
          "test_data": {
            "artifactType": {
              "schemaTitle": "system.Dataset",
              "schemaVersion": "0.0.1"
            }
          },
          "train_data": {
            "artifactType": {
              "schemaTitle": "system.Dataset",
              "schemaVersion": "0.0.1"
            }
          },
          "valid_data": {
            "artifactType": {
              "schemaTitle": "system.Dataset",
              "schemaVersion": "0.0.1"
            }
          }
        },
        "parameters": {
          "batch_size": {
            "defaultValue": 32.0,
            "isOptional": true,
            "parameterType": "NUMBER_INTEGER"
          },
          "feature_names": {
            "defaultValue": [
              "PRICE_std",
              "vol_quote_std",
              "cvd_quote_std",
              "PDCC_Down",
              "OSV_Down_std",
              "PDCC2_UP",
              "regime_up",
              "regime_down"
            ],
            "isOptional": true,
            "parameterType": "LIST"
          },
          "input_width": {
            "defaultValue": 50.0,
            "isOptional": true,
            "parameterType": "NUMBER_INTEGER"
          },
          "label_columns": {
            "defaultValue": [
              "PRICE_std"
            ],
            "isOptional": true,
            "parameterType": "LIST"
          },
          "label_width": {
            "defaultValue": 1.0,
            "isOptional": true,
            "parameterType": "NUMBER_INTEGER"
          },
          "shift": {
            "defaultValue": 50.0,
            "isOptional": true,
            "parameterType": "NUMBER_INTEGER"
          }
        }
      },
      "outputDefinitions": {
        "artifacts": {
          "metrics": {
            "artifactType": {
              "schemaTitle": "system.Metrics",
              "schemaVersion": "0.0.1"
            }
          },
          "plots_output": {
            "artifactType": {
              "schemaTitle": "system.HTML",
              "schemaVersion": "0.0.1"
            }
          },
          "test_windowed": {
            "artifactType": {
              "schemaTitle": "system.Dataset",
              "schemaVersion": "0.0.1"
            }
          },
          "train_windowed": {
            "artifactType": {
              "schemaTitle": "system.Dataset",
              "schemaVersion": "0.0.1"
            }
          },
          "valid_windowed": {
            "artifactType": {
              "schemaTitle": "system.Dataset",
              "schemaVersion": "0.0.1"
            }
          }
        }
      }
    }
  },
  "deploymentSpec": {
    "executors": {
      "exec-bq-query-to-table": {
        "container": {
          "args": [
            "--executor_input",
            "{{$}}",
            "--function_to_execute",
            "bq_query_to_table"
          ],
          "command": [
            "sh",
            "-c",
            "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location 'google-cloud-bigquery==2.30.0'  &&  python3 -m pip install --quiet --no-warn-script-location 'kfp==2.15.2' '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"' && \"$0\" \"$@\"\n",
            "sh",
            "-ec",
            "program_path=$(mktemp -d)\n\nprintf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\n_KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n",
            "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import *\n\ndef bq_query_to_table(\n    query: str,\n    bq_client_project_id: str,\n    destination_project_id: str,\n    dataset_id: str = None,\n    table_id: str = None,\n    dataset_location: str = \"EU\",\n    query_job_config: dict = None,\n) -> None:\n    \"\"\"\n    Run query & create a new BigQuery table\n    Args:\n        query (str): SQL query to execute, results are saved in a BigQuery table\n        bq_client_project_id (str): project id that will be used by the bq client\n        destination_project_id (str): project id where BQ table will be created\n        dataset_id (str): dataset id where BQ table will be created\n        table_id (str): table name (without project id and dataset id)\n        dataset_location (str): bq dataset location\n        query_job_config (dict): dict containing optional parameters\n        required by the bq query operation. No need to specify destination param\n        See available parameters here\n        https://googleapis.dev/python/bigquery/latest/generated/google.cloud.bigquery.job.QueryJobConfig.html\n    Returns:\n        None\n    \"\"\"\n    from google.cloud.exceptions import GoogleCloudError\n    from google.cloud import bigquery\n    import logging\n\n    logging.getLogger().setLevel(logging.INFO)\n\n    if (dataset_id is not None) and (table_id is not None):\n        dest_table_ref = f\"{destination_project_id}.{dataset_id}.{table_id}\"\n    else:\n        dest_table_ref = None\n    if query_job_config is None:\n        query_job_config = {}\n    job_config = bigquery.QueryJobConfig(destination=dest_table_ref, **query_job_config)\n\n    bq_client = bigquery.client.Client(\n        project=bq_client_project_id, location=dataset_location\n    )\n    query_job = bq_client.query(query, job_config=job_config)\n\n    try:\n        result = query_job.result()\n        logging.info(f\"BQ table {dest_table_ref} created\")\n    except GoogleCloudError as e:\n        logging.error(e)\n        logging.error(query_job.error_result)\n        logging.error(query_job.errors)\n        raise e\n\n"
          ],
          "image": "python:3.10"
        }
      },
      "exec-compare-forecasts": {
        "container": {
          "args": [
            "--executor_input",
            "{{$}}",
            "--function_to_execute",
            "compare_forecasts"
          ],
          "command": [
            "sh",
            "-c",
            "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location 'pandas>=1.3.0' 'numpy>=1.21.0' 'scipy>=1.10.0'  &&  python3 -m pip install --quiet --no-warn-script-location 'kfp==2.15.2' '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"' && \"$0\" \"$@\"\n",
            "sh",
            "-ec",
            "program_path=$(mktemp -d)\n\nprintf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\n_KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n",
            "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import *\n\ndef compare_forecasts(\n    baseline_predictions: Input[Dataset],\n    single_dc_predictions: Input[Dataset],\n    multi_dc_predictions: Input[Dataset],\n    report: Output[Dataset],\n    visualization: Output[HTML],\n    metrics: Output[Metrics],\n):\n    \"\"\"Compare 3 ablation arms and produce a statistical comparison report.\n\n    Each input Dataset should be a CSV file with columns:\n        sample_index, y_true_scaled, y_pred_scaled\n\n    Outputs:\n        report: JSON file with per-model metrics and pairwise comparisons\n        visualization: HTML table summarizing results\n        metrics: KFP Metrics logged for the pipeline UI\n    \"\"\"\n    import json\n    import math\n    import os\n    from typing import Dict, List, Optional\n\n    import numpy as np\n    import pandas as pd\n    from scipy import stats\n\n    # -----------------------------------------------------------------\n    # Metric helpers (inlined for KFP component isolation)\n    # -----------------------------------------------------------------\n    def compute_rmse(y_true, y_pred):\n        return float(np.sqrt(np.mean((y_true - y_pred) ** 2)))\n\n    def compute_mae(y_true, y_pred):\n        return float(np.mean(np.abs(y_true - y_pred)))\n\n    def compute_mape(y_true, y_pred):\n        mask = y_true != 0\n        if not mask.any():\n            return 0.0\n        return float(np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])))\n\n    def compute_smape(y_true, y_pred):\n        denom = np.abs(y_true) + np.abs(y_pred)\n        mask = denom != 0\n        if not mask.any():\n            return 0.0\n        return float(np.mean(2.0 * np.abs(y_true[mask] - y_pred[mask]) / denom[mask]))\n\n    def compute_r_squared(y_true, y_pred):\n        ss_res = np.sum((y_true - y_pred) ** 2)\n        ss_tot = np.sum((y_true - np.mean(y_true)) ** 2)\n        if ss_tot == 0:\n            return 0.0\n        return float(1.0 - ss_res / ss_tot)\n\n    def compute_directional_accuracy(y_true, y_pred):\n        if len(y_true) < 2:\n            return 0.0\n        true_dir = np.sign(np.diff(y_true))\n        pred_dir = np.sign(np.diff(y_pred))\n        return float(np.mean(true_dir == pred_dir))\n\n    def compute_all_metrics(y_true, y_pred):\n        return {\n            \"rmse\": compute_rmse(y_true, y_pred),\n            \"mae\": compute_mae(y_true, y_pred),\n            \"mape\": compute_mape(y_true, y_pred),\n            \"r_squared\": compute_r_squared(y_true, y_pred),\n            \"smape\": compute_smape(y_true, y_pred),\n            \"directional_accuracy\": compute_directional_accuracy(y_true, y_pred),\n        }\n\n    def compare_two(y_true, y_pred_a, y_pred_b):\n        \"\"\"Compare A vs B. Positive improvement means A is better.\"\"\"\n        rmse_a = compute_rmse(y_true, y_pred_a)\n        rmse_b = compute_rmse(y_true, y_pred_b)\n        rel_imp = (rmse_b - rmse_a) / rmse_b * 100.0 if rmse_b != 0 else 0.0\n\n        sq_a = (y_true - y_pred_a) ** 2\n        sq_b = (y_true - y_pred_b) ** 2\n        abs_a = np.abs(y_true - y_pred_a)\n        abs_b = np.abs(y_true - y_pred_b)\n\n        wilcoxon_p = None\n        try:\n            diff = sq_a - sq_b\n            if np.any(diff != 0) and len(diff) >= 5:\n                _, wilcoxon_p = stats.wilcoxon(sq_a, sq_b)\n                wilcoxon_p = float(wilcoxon_p)\n        except Exception:\n            pass\n\n        ttest_p = None\n        try:\n            if len(abs_a) >= 2:\n                _, ttest_p = stats.ttest_rel(abs_a, abs_b)\n                ttest_p = float(ttest_p)\n        except Exception:\n            pass\n\n        return {\n            \"rmse_relative_improvement_pct\": float(rel_imp),\n            \"wilcoxon_p\": wilcoxon_p,\n            \"paired_ttest_p\": ttest_p,\n        }\n\n    # -----------------------------------------------------------------\n    # Read prediction CSVs\n    # -----------------------------------------------------------------\n    df_base = pd.read_csv(baseline_predictions.path)\n    df_single = pd.read_csv(single_dc_predictions.path)\n    df_multi = pd.read_csv(multi_dc_predictions.path)\n\n    yt_b = df_base[\"y_true_scaled\"].to_numpy()\n    yp_b = df_base[\"y_pred_scaled\"].to_numpy()\n    yt_s = df_single[\"y_true_scaled\"].to_numpy()\n    yp_s = df_single[\"y_pred_scaled\"].to_numpy()\n    yt_m = df_multi[\"y_true_scaled\"].to_numpy()\n    yp_m = df_multi[\"y_pred_scaled\"].to_numpy()\n\n    # -----------------------------------------------------------------\n    # Per-model metrics\n    # -----------------------------------------------------------------\n    per_model = {\n        \"baseline\": compute_all_metrics(yt_b, yp_b),\n        \"single_dc\": compute_all_metrics(yt_s, yp_s),\n        \"multi_dc\": compute_all_metrics(yt_m, yp_m),\n    }\n\n    # -----------------------------------------------------------------\n    # Pairwise comparisons\n    # -----------------------------------------------------------------\n    comparisons = {\n        \"single_dc_vs_baseline\": compare_two(yt_b, yp_s, yp_b),\n        \"multi_dc_vs_baseline\": compare_two(yt_b, yp_m, yp_b),\n        \"multi_dc_vs_single_dc\": compare_two(yt_s, yp_m, yp_s),\n    }\n\n    full_report = {\n        \"per_model\": per_model,\n        \"comparisons\": comparisons,\n    }\n\n    # -----------------------------------------------------------------\n    # Write JSON report\n    # -----------------------------------------------------------------\n    os.makedirs(os.path.dirname(report.path) or \".\", exist_ok=True)\n    with open(report.path, \"w\") as f:\n        json.dump(full_report, f, indent=2)\n\n    # -----------------------------------------------------------------\n    # Log KFP metrics\n    # -----------------------------------------------------------------\n    for arm_name, arm_metrics in per_model.items():\n        for metric_name, metric_val in arm_metrics.items():\n            if metric_val is not None and math.isfinite(metric_val):\n                metrics.log_metric(f\"{arm_name}_{metric_name}\", float(metric_val))\n\n    for comp_name, comp_vals in comparisons.items():\n        for k, v in comp_vals.items():\n            if v is not None and math.isfinite(v):\n                metrics.log_metric(f\"{comp_name}_{k}\", float(v))\n\n    # -----------------------------------------------------------------\n    # Build HTML visualization\n    # -----------------------------------------------------------------\n    def _fmt(v):\n        if v is None:\n            return \"N/A\"\n        return f\"{v:.6f}\"\n\n    rows_per_model = \"\"\n    for arm in [\"baseline\", \"single_dc\", \"multi_dc\"]:\n        m = per_model[arm]\n        rows_per_model += f\"\"\"\n        <tr>\n            <td style=\"border:1px solid #ccc;padding:6px 12px;font-weight:bold;\">{arm}</td>\n            <td style=\"border:1px solid #ccc;padding:6px 12px;\">{_fmt(m['rmse'])}</td>\n            <td style=\"border:1px solid #ccc;padding:6px 12px;\">{_fmt(m['mae'])}</td>\n            <td style=\"border:1px solid #ccc;padding:6px 12px;\">{_fmt(m['mape'])}</td>\n            <td style=\"border:1px solid #ccc;padding:6px 12px;\">{_fmt(m['r_squared'])}</td>\n            <td style=\"border:1px solid #ccc;padding:6px 12px;\">{_fmt(m['smape'])}</td>\n            <td style=\"border:1px solid #ccc;padding:6px 12px;\">{_fmt(m['directional_accuracy'])}</td>\n        </tr>\"\"\"\n\n    rows_comp = \"\"\n    for comp_name, comp_vals in comparisons.items():\n        rows_comp += f\"\"\"\n        <tr>\n            <td style=\"border:1px solid #ccc;padding:6px 12px;font-weight:bold;\">{comp_name}</td>\n            <td style=\"border:1px solid #ccc;padding:6px 12px;\">{_fmt(comp_vals['rmse_relative_improvement_pct'])}%</td>\n            <td style=\"border:1px solid #ccc;padding:6px 12px;\">{_fmt(comp_vals['wilcoxon_p'])}</td>\n            <td style=\"border:1px solid #ccc;padding:6px 12px;\">{_fmt(comp_vals['paired_ttest_p'])}</td>\n        </tr>\"\"\"\n\n    html_content = f\"\"\"\n    <html><body style=\"font-family:Inter,system-ui,Arial,sans-serif;padding:16px;\">\n    <h2>DC Feature Ablation Study - Comparison Report</h2>\n\n    <h3>Per-Model Metrics</h3>\n    <table style=\"border-collapse:collapse;margin:10px 0;\">\n        <tr>\n            <th style=\"border:1px solid #ccc;padding:6px 12px;\">Model</th>\n            <th style=\"border:1px solid #ccc;padding:6px 12px;\">RMSE</th>\n            <th style=\"border:1px solid #ccc;padding:6px 12px;\">MAE</th>\n            <th style=\"border:1px solid #ccc;padding:6px 12px;\">MAPE</th>\n            <th style=\"border:1px solid #ccc;padding:6px 12px;\">R\u00b2</th>\n            <th style=\"border:1px solid #ccc;padding:6px 12px;\">sMAPE</th>\n            <th style=\"border:1px solid #ccc;padding:6px 12px;\">Dir. Acc.</th>\n        </tr>\n        {rows_per_model}\n    </table>\n\n    <h3>Pairwise Comparisons</h3>\n    <p><em>Positive RMSE improvement means the first model is better.</em></p>\n    <table style=\"border-collapse:collapse;margin:10px 0;\">\n        <tr>\n            <th style=\"border:1px solid #ccc;padding:6px 12px;\">Comparison</th>\n            <th style=\"border:1px solid #ccc;padding:6px 12px;\">RMSE Improvement</th>\n            <th style=\"border:1px solid #ccc;padding:6px 12px;\">Wilcoxon p</th>\n            <th style=\"border:1px solid #ccc;padding:6px 12px;\">Paired t-test p</th>\n        </tr>\n        {rows_comp}\n    </table>\n\n    <h3>Note on Parameter Counts</h3>\n    <p>The Flatten\u2192Dense architecture means the first Dense layer has different parameter counts\n    across arms (baseline: ~9.6K, single-DC: ~28.8K, multi-DC: ~86.4K). This ablation tests\n    whether DC features help <em>at all</em>, not whether they help given equal capacity.</p>\n    </body></html>\n    \"\"\"\n    os.makedirs(os.path.dirname(visualization.path) or \".\", exist_ok=True)\n    with open(visualization.path, \"w\", encoding=\"utf-8\") as f:\n        f.write(html_content)\n\n"
          ],
          "image": "python:3.10"
        }
      },
      "exec-concat-threshold-features": {
        "container": {
          "args": [
            "--executor_input",
            "{{$}}",
            "--function_to_execute",
            "concat_threshold_features"
          ],
          "command": [
            "sh",
            "-c",
            "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location '--upgrade' 'pip' 'setuptools' 'wheel' 'cython<3.0.0' '--no-build-isolation' 'pyyaml==5.4.1' 'numpy>=1.21.0' 'tensorflow==2.17.0'  &&  python3 -m pip install --quiet --no-warn-script-location 'kfp==2.15.2' '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"' && \"$0\" \"$@\"\n",
            "sh",
            "-ec",
            "program_path=$(mktemp -d)\n\nprintf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\n_KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n",
            "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import *\n\ndef concat_threshold_features(\n    # Per-split lists of snapshots saved by window_dataset (elements are (x:[L,F], y:[H]))\n    train_datasets: Input[List[Dataset]],\n    valid_datasets: Input[List[Dataset]],\n    test_datasets: Input[List[Dataset]],\n\n    # Outputs\n    joint_train: Output[Dataset],\n    joint_valid: Output[Dataset],\n    joint_test: Output[Dataset],\n    metrics: Output[Metrics],\n\n    # Feature config\n    feature_names: List[str] = [\n        \"PRICE_std\",\"vol_quote_std\",\"cvd_quote_std\",\n        \"PDCC_Down\",\"OSV_Down_std\",\"PDCC2_UP\",\"regime_up\",\"regime_down\",\n    ],\n    common_feature_names: List[str] = [\"PRICE_std\",\"vol_quote_std\",\"cvd_quote_std\"],\n    per_threshold_feature_names: List[str] = [\"PDCC_Down\",\"OSV_Down_std\",\"PDCC2_UP\",\"regime_up\",\"regime_down\"],\n\n    # Save / checks\n    compression: str = \"GZIP\",\n    validate_y_equal: bool = True,\n    y_rtol: float = 1e-6,\n    y_atol: float = 1e-6,\n):\n    \"\"\"\n    Build (x_joint, y) per example by:\n      \u2022 Keeping a single copy of `common_feature_names` from threshold 0 (first dataset)\n      \u2022 Concatenating `per_threshold_feature_names` from *every* threshold in order\n      \u2022 Preserving y from threshold 0; optionally assert all thresholds carry the same y\n\n    Output layout of x_joint:\n      [ common_feature_names(th0) , per_threshold_feature_names(th0) , ... , per_threshold_feature_names(th{T-1}) ]\n    \"\"\"\n    import tensorflow as tf\n\n    # --- index helpers\n    name_to_idx = {n: i for i, n in enumerate(feature_names)}\n    try:\n        common_idx = tf.constant([name_to_idx[n] for n in common_feature_names], dtype=tf.int32)\n        per_thr_idx = tf.constant([name_to_idx[n] for n in per_threshold_feature_names], dtype=tf.int32)\n    except KeyError as e:\n        raise ValueError(f\"Feature name not found in feature_names: {e}\")\n\n    def _load_snapshot(dir_path: str) -> tf.data.Dataset:\n        # Each element is (x:[L,F], y:[H]); already unbatched in your window_dataset\n        return tf.data.Dataset.load(dir_path, compression=compression)\n\n    def _concat_one_split(dsets: List[Dataset], out_dir: str):\n        if not dsets:\n            raise ValueError(\"Expected at least one dataset per split.\")\n        # Load datasets in threshold order\n        per_thr_ds = [_load_snapshot(d.path) for d in dsets]\n\n        # Zip across thresholds -> ((x1,y1),(x2,y2),...,(xT,yT))\n        zipped = tf.data.Dataset.zip(tuple(per_thr_ds))\n\n        def _concat_examples(*pairs):\n            xs = [p[0] for p in pairs]  # list of [L,F]\n            ys = [p[1] for p in pairs]  # list of [H]\n\n            # Keep y from first threshold\n            y0 = ys[0]\n\n            # Optionally assert all y are equal (within tol)\n            if validate_y_equal and len(ys) > 1:\n                for yk in ys[1:]:\n                    tf.debugging.assert_near(yk, y0, rtol=y_rtol, atol=y_atol,\n                                             message=\"y differs across thresholds\")\n\n            # Common features from threshold 0 only\n            x_common = tf.gather(xs[0], common_idx, axis=-1)           # [L, n_common]\n\n            # Per-threshold feature blocks concatenated across thresholds\n            per_blocks = [tf.gather(x, per_thr_idx, axis=-1) for x in xs]  # T * [L, k]\n            x_per_joint = tf.concat(per_blocks, axis=-1)                    # [L, k*T]\n\n            x_joint = tf.concat([x_common, x_per_joint], axis=-1)           # [L, n_common + k*T]\n            return (x_joint, y0)\n\n        joint = zipped.map(_concat_examples, num_parallel_calls=tf.data.AUTOTUNE)\n\n        # Inspect 1 example for shape metrics\n        first = next(iter(joint.take(1)))\n        x0, y0 = first\n        L = int(x0.shape[0])\n        F_out = int(x0.shape[1])\n        H = int(y0.shape[0]) if y0.shape.rank == 1 else int(y0.shape[-1])\n\n        # Count examples efficiently\n        count = int(\n            joint.reduce(\n                tf.constant(0, tf.int64),\n                lambda c, _: c + tf.constant(1, tf.int64)\n            ).numpy()\n        )\n\n        # Save snapshot\n        tf.io.gfile.makedirs(out_dir)\n        joint.save(out_dir, compression=compression)\n\n        return count, L, F_out, H\n\n    tr_count, tr_L, tr_F, tr_H = _concat_one_split(train_datasets, joint_train.path)\n    va_count, va_L, va_F, va_H = _concat_one_split(valid_datasets, joint_valid.path)\n    te_count, te_L, te_F, te_H = _concat_one_split(test_datasets, joint_test.path)\n\n    # Metrics\n    metrics.log_metric(\"num_thresholds\", float(len(train_datasets)))\n    metrics.log_metric(\"n_common_features\", float(len(common_feature_names)))\n    metrics.log_metric(\"n_per_threshold_features\", float(len(per_threshold_feature_names)))\n    metrics.log_metric(\"output_features\", float(tr_F))\n    metrics.log_metric(\"sequence_length_L\", float(tr_L))\n    metrics.log_metric(\"label_length_H\", float(tr_H))\n    metrics.log_metric(\"train_examples\", float(tr_count))\n    metrics.log_metric(\"valid_examples\", float(va_count))\n    metrics.log_metric(\"test_examples\", float(te_count))\n\n"
          ],
          "image": "python:3.10"
        }
      },
      "exec-concat-threshold-features-2": {
        "container": {
          "args": [
            "--executor_input",
            "{{$}}",
            "--function_to_execute",
            "concat_threshold_features"
          ],
          "command": [
            "sh",
            "-c",
            "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location '--upgrade' 'pip' 'setuptools' 'wheel' 'cython<3.0.0' '--no-build-isolation' 'pyyaml==5.4.1' 'numpy>=1.21.0' 'tensorflow==2.17.0'  &&  python3 -m pip install --quiet --no-warn-script-location 'kfp==2.15.2' '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"' && \"$0\" \"$@\"\n",
            "sh",
            "-ec",
            "program_path=$(mktemp -d)\n\nprintf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\n_KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n",
            "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import *\n\ndef concat_threshold_features(\n    # Per-split lists of snapshots saved by window_dataset (elements are (x:[L,F], y:[H]))\n    train_datasets: Input[List[Dataset]],\n    valid_datasets: Input[List[Dataset]],\n    test_datasets: Input[List[Dataset]],\n\n    # Outputs\n    joint_train: Output[Dataset],\n    joint_valid: Output[Dataset],\n    joint_test: Output[Dataset],\n    metrics: Output[Metrics],\n\n    # Feature config\n    feature_names: List[str] = [\n        \"PRICE_std\",\"vol_quote_std\",\"cvd_quote_std\",\n        \"PDCC_Down\",\"OSV_Down_std\",\"PDCC2_UP\",\"regime_up\",\"regime_down\",\n    ],\n    common_feature_names: List[str] = [\"PRICE_std\",\"vol_quote_std\",\"cvd_quote_std\"],\n    per_threshold_feature_names: List[str] = [\"PDCC_Down\",\"OSV_Down_std\",\"PDCC2_UP\",\"regime_up\",\"regime_down\"],\n\n    # Save / checks\n    compression: str = \"GZIP\",\n    validate_y_equal: bool = True,\n    y_rtol: float = 1e-6,\n    y_atol: float = 1e-6,\n):\n    \"\"\"\n    Build (x_joint, y) per example by:\n      \u2022 Keeping a single copy of `common_feature_names` from threshold 0 (first dataset)\n      \u2022 Concatenating `per_threshold_feature_names` from *every* threshold in order\n      \u2022 Preserving y from threshold 0; optionally assert all thresholds carry the same y\n\n    Output layout of x_joint:\n      [ common_feature_names(th0) , per_threshold_feature_names(th0) , ... , per_threshold_feature_names(th{T-1}) ]\n    \"\"\"\n    import tensorflow as tf\n\n    # --- index helpers\n    name_to_idx = {n: i for i, n in enumerate(feature_names)}\n    try:\n        common_idx = tf.constant([name_to_idx[n] for n in common_feature_names], dtype=tf.int32)\n        per_thr_idx = tf.constant([name_to_idx[n] for n in per_threshold_feature_names], dtype=tf.int32)\n    except KeyError as e:\n        raise ValueError(f\"Feature name not found in feature_names: {e}\")\n\n    def _load_snapshot(dir_path: str) -> tf.data.Dataset:\n        # Each element is (x:[L,F], y:[H]); already unbatched in your window_dataset\n        return tf.data.Dataset.load(dir_path, compression=compression)\n\n    def _concat_one_split(dsets: List[Dataset], out_dir: str):\n        if not dsets:\n            raise ValueError(\"Expected at least one dataset per split.\")\n        # Load datasets in threshold order\n        per_thr_ds = [_load_snapshot(d.path) for d in dsets]\n\n        # Zip across thresholds -> ((x1,y1),(x2,y2),...,(xT,yT))\n        zipped = tf.data.Dataset.zip(tuple(per_thr_ds))\n\n        def _concat_examples(*pairs):\n            xs = [p[0] for p in pairs]  # list of [L,F]\n            ys = [p[1] for p in pairs]  # list of [H]\n\n            # Keep y from first threshold\n            y0 = ys[0]\n\n            # Optionally assert all y are equal (within tol)\n            if validate_y_equal and len(ys) > 1:\n                for yk in ys[1:]:\n                    tf.debugging.assert_near(yk, y0, rtol=y_rtol, atol=y_atol,\n                                             message=\"y differs across thresholds\")\n\n            # Common features from threshold 0 only\n            x_common = tf.gather(xs[0], common_idx, axis=-1)           # [L, n_common]\n\n            # Per-threshold feature blocks concatenated across thresholds\n            per_blocks = [tf.gather(x, per_thr_idx, axis=-1) for x in xs]  # T * [L, k]\n            x_per_joint = tf.concat(per_blocks, axis=-1)                    # [L, k*T]\n\n            x_joint = tf.concat([x_common, x_per_joint], axis=-1)           # [L, n_common + k*T]\n            return (x_joint, y0)\n\n        joint = zipped.map(_concat_examples, num_parallel_calls=tf.data.AUTOTUNE)\n\n        # Inspect 1 example for shape metrics\n        first = next(iter(joint.take(1)))\n        x0, y0 = first\n        L = int(x0.shape[0])\n        F_out = int(x0.shape[1])\n        H = int(y0.shape[0]) if y0.shape.rank == 1 else int(y0.shape[-1])\n\n        # Count examples efficiently\n        count = int(\n            joint.reduce(\n                tf.constant(0, tf.int64),\n                lambda c, _: c + tf.constant(1, tf.int64)\n            ).numpy()\n        )\n\n        # Save snapshot\n        tf.io.gfile.makedirs(out_dir)\n        joint.save(out_dir, compression=compression)\n\n        return count, L, F_out, H\n\n    tr_count, tr_L, tr_F, tr_H = _concat_one_split(train_datasets, joint_train.path)\n    va_count, va_L, va_F, va_H = _concat_one_split(valid_datasets, joint_valid.path)\n    te_count, te_L, te_F, te_H = _concat_one_split(test_datasets, joint_test.path)\n\n    # Metrics\n    metrics.log_metric(\"num_thresholds\", float(len(train_datasets)))\n    metrics.log_metric(\"n_common_features\", float(len(common_feature_names)))\n    metrics.log_metric(\"n_per_threshold_features\", float(len(per_threshold_feature_names)))\n    metrics.log_metric(\"output_features\", float(tr_F))\n    metrics.log_metric(\"sequence_length_L\", float(tr_L))\n    metrics.log_metric(\"label_length_H\", float(tr_H))\n    metrics.log_metric(\"train_examples\", float(tr_count))\n    metrics.log_metric(\"valid_examples\", float(va_count))\n    metrics.log_metric(\"test_examples\", float(te_count))\n\n"
          ],
          "image": "python:3.10"
        }
      },
      "exec-custom-forecast-train-job": {
        "container": {
          "args": [
            "--executor_input",
            "{{$}}",
            "--function_to_execute",
            "custom_forecast_train_job"
          ],
          "command": [
            "sh",
            "-c",
            "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location 'setuptools>=69.0.0' 'wheel' 'cython<3.0.0' '--no-build-isolation' 'pyyaml==5.4.1' 'google-cloud-aiplatform==1.24.1' 'python-json-logger' 'google-cloud-logging' 'datetime'  &&  python3 -m pip install --quiet --no-warn-script-location 'kfp==2.15.2' '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"' && \"$0\" \"$@\"\n",
            "sh",
            "-ec",
            "program_path=$(mktemp -d)\n\nprintf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\n_KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n",
            "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import *\n\ndef custom_forecast_train_job(\n    train_script_uri: str,\n    train_data: Input[Dataset],\n    valid_data: Input[Dataset],\n    test_data: Input[Dataset],\n    project_id: str,\n    project_location: str,\n    # model_display_name: str,\n    threshold: float,\n    train_container_uri: str,\n    serving_container_uri: str,\n    model: Output[Artifact],\n    metrics: Output[Metrics],\n    predictions: Output[Dataset],\n    staging_bucket: str,\n    parent_model: str = None,\n    model_display_name: Optional[str] = None,\n    requirements: Optional[List[str]] = None,\n    job_name: Optional[str] = None,\n    hparams: Optional[Dict[str, str]] = None,\n    # threshold: Optional[List[float]] = None,\n    replica_count: int = 1,\n    machine_type: str = \"n1-standard-4\",\n    accelerator_type: str = \"ACCELERATOR_TYPE_UNSPECIFIED\",\n    accelerator_count: int = 0,\n    tensorboard_resource_name: str = \"projects/derivatives-417104/locations/us-central1/tensorboards/2135763953559535616\",\n    service_account: str = \"vertex-pipelines@derivatives-417104.iam.gserviceaccount.com\",\n):\n    \"\"\"Run a custom training job using a training script.\n\n    The provided script will be invoked by passing the following command-line arguments:\n\n    ```\n    train.py \\\n        --train_data <train_data.path> \\\n        --valid_data <valid_data.path> \\\n        --test_data <test_data.path> \\\n        --metrics <metrics.path> \\\n        --hparams json.dumps(<hparams>)\n    ```\n\n    Ensure that your train script can read these arguments and outputs metrics\n    to the provided path and the model to the correct path based on:\n    https://cloud.google.com/vertex-ai/docs/training/code-requirements.\n\n    Args:\n        train_script_uri (str): gs:// uri to python train script. See:\n            https://cloud.google.com/vertex-ai/docs/training/code-requirements.\n        train_data (Dataset): Training data (passed as an argument to train script)\n        valid_data (Dataset): Validation data (passed as an argument to train script)\n        test_data (Dataset): Test data (passed as an argument to train script).\n        staging_bucket (str): Staging bucket for CustomTrainingJob.\n        project_location (str): location of the Google Cloud project.\n        project_id (str): project id of the Google Cloud project.\n        model_display_name (str): Name of the new trained model version.\n        train_container_uri (str): Container URI for running train script.\n        serving_container_uri (str): Container URI for deploying the output model.\n        model (Model): Trained model output.\n        metrics (Metrics): Output metrics of trained model.\n        requirements (List[str]): Additional python dependencies for training script.\n        job_name (str): Name of training job.\n        hparams (Dict[str, str]): Hyperparameters (passed as a JSON serialised argument\n            to train script)\n        replica_count (int): Number of replicas (increase for distributed training).\n        machine_type (str): Machine type of compute.\n        accelerator_type (str): Accelerator type (change for GPU support).\n        accelerator_count (str): Accelerator count (increase for GPU cores).\n        parent_model (str): Resource name of existing parent model (optional).\n            If `None`, a new model will be uploaded. Otherwise, a new model version\n            for the parent model will be uploaded.\n    Returns:\n        parent_model (str): Resource URI of the parent model (empty string if the\n            trained model is the first model version of its kind).\n    \"\"\"\n    import json\n    import logging\n    import os.path\n    import time\n    import google.cloud.aiplatform as aip\n    from datetime import datetime\n\n    logging.info(f\"Using train script: {train_script_uri}\")\n    script_path = \"/gcs/\" + train_script_uri[5:]\n    if not os.path.exists(script_path):\n        raise ValueError(\n            \"Train script was not found. \"\n            f\"Check if the path is correct: {train_script_uri}\"\n        )\n\n    thr_str = str(threshold).replace('.', 'p')\n    if model_display_name is None:\n        model_display_name = f\"vanilla_forecast_model_{thr_str}_{datetime.now().strftime('%Y%m%d%H%M%S')}\"\n    else:\n        model_display_name = model_display_name\n    job_name = f\"vanilla_forecast_job_{thr_str}_{datetime.now().strftime('%Y%m%d%H%M%S')}\"\n\n    job = aip.CustomTrainingJob(\n        project=project_id,\n        location=project_location,\n        staging_bucket=staging_bucket,\n        display_name=job_name if job_name else f\"Custom job {int(time.time())}\",\n        script_path=script_path,\n        container_uri=train_container_uri,\n        requirements=requirements,\n        model_serving_container_image_uri=serving_container_uri,\n    )\n    logging.info(f\"train_data path: {train_data.path}\")\n\n    # Add threshold to hparams\n    hp = dict(hparams or {})\n    if threshold is not None:\n        hp[\"threshold\"] = float(threshold)\n    hp_json = json.dumps(hp)\n\n    cmd_args = [\n        f\"--train_data={train_data.path}\",\n        f\"--valid_data={valid_data.path}\",\n        f\"--test_data={test_data.path}\",\n        f\"--metrics={metrics.path}\",\n        f\"--hparams={hp_json}\",\n        f\"--predictions={predictions.path}\",\n    ]\n    # https://cloud.google.com/python/docs/reference/aiplatform/1.18.3/google.cloud.aiplatform.CustomTrainingJob#google_cloud_aiplatform_CustomTrainingJob_run\n    uploaded_model = job.run(\n        model_display_name=model_display_name,\n        parent_model=parent_model,\n        is_default_version=(parent_model is None),\n        args=cmd_args,\n        replica_count=replica_count,\n        machine_type=machine_type,\n        accelerator_type=accelerator_type,\n        accelerator_count=accelerator_count,\n        tensorboard=tensorboard_resource_name,\n        service_account=service_account,\n    )\n\n    # resource_name = f\"{uploaded_model.resource_name}@{uploaded_model.version_id}\"\n    resource_name = uploaded_model.resource_name\n    if \"@\" not in resource_name and getattr(uploaded_model, \"version_id\", None):\n        resource_name = f\"{resource_name}@{uploaded_model.version_id}\"\n\n    model.metadata[\"resourceName\"] = resource_name\n    model.metadata[\"displayName\"] = uploaded_model.display_name\n    model.metadata[\"containerSpec\"] = {\"imageUri\": serving_container_uri}\n    model.uri = uploaded_model.uri\n    model.TYPE_NAME = \"google.VertexModel\"\n\n    with open(metrics.path, \"r\") as fp:\n        parsed_metrics = json.load(fp)\n\n    logging.info(parsed_metrics)\n    for k, v in parsed_metrics.items():\n        if isinstance(v, (float, int)) and v is not None:\n            metrics.log_metric(k, float(v))\n\n"
          ],
          "image": "python:3.10"
        }
      },
      "exec-custom-forecast-train-job-2": {
        "container": {
          "args": [
            "--executor_input",
            "{{$}}",
            "--function_to_execute",
            "custom_forecast_train_job"
          ],
          "command": [
            "sh",
            "-c",
            "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location 'setuptools>=69.0.0' 'wheel' 'cython<3.0.0' '--no-build-isolation' 'pyyaml==5.4.1' 'google-cloud-aiplatform==1.24.1' 'python-json-logger' 'google-cloud-logging' 'datetime'  &&  python3 -m pip install --quiet --no-warn-script-location 'kfp==2.15.2' '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"' && \"$0\" \"$@\"\n",
            "sh",
            "-ec",
            "program_path=$(mktemp -d)\n\nprintf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\n_KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n",
            "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import *\n\ndef custom_forecast_train_job(\n    train_script_uri: str,\n    train_data: Input[Dataset],\n    valid_data: Input[Dataset],\n    test_data: Input[Dataset],\n    project_id: str,\n    project_location: str,\n    # model_display_name: str,\n    threshold: float,\n    train_container_uri: str,\n    serving_container_uri: str,\n    model: Output[Artifact],\n    metrics: Output[Metrics],\n    predictions: Output[Dataset],\n    staging_bucket: str,\n    parent_model: str = None,\n    model_display_name: Optional[str] = None,\n    requirements: Optional[List[str]] = None,\n    job_name: Optional[str] = None,\n    hparams: Optional[Dict[str, str]] = None,\n    # threshold: Optional[List[float]] = None,\n    replica_count: int = 1,\n    machine_type: str = \"n1-standard-4\",\n    accelerator_type: str = \"ACCELERATOR_TYPE_UNSPECIFIED\",\n    accelerator_count: int = 0,\n    tensorboard_resource_name: str = \"projects/derivatives-417104/locations/us-central1/tensorboards/2135763953559535616\",\n    service_account: str = \"vertex-pipelines@derivatives-417104.iam.gserviceaccount.com\",\n):\n    \"\"\"Run a custom training job using a training script.\n\n    The provided script will be invoked by passing the following command-line arguments:\n\n    ```\n    train.py \\\n        --train_data <train_data.path> \\\n        --valid_data <valid_data.path> \\\n        --test_data <test_data.path> \\\n        --metrics <metrics.path> \\\n        --hparams json.dumps(<hparams>)\n    ```\n\n    Ensure that your train script can read these arguments and outputs metrics\n    to the provided path and the model to the correct path based on:\n    https://cloud.google.com/vertex-ai/docs/training/code-requirements.\n\n    Args:\n        train_script_uri (str): gs:// uri to python train script. See:\n            https://cloud.google.com/vertex-ai/docs/training/code-requirements.\n        train_data (Dataset): Training data (passed as an argument to train script)\n        valid_data (Dataset): Validation data (passed as an argument to train script)\n        test_data (Dataset): Test data (passed as an argument to train script).\n        staging_bucket (str): Staging bucket for CustomTrainingJob.\n        project_location (str): location of the Google Cloud project.\n        project_id (str): project id of the Google Cloud project.\n        model_display_name (str): Name of the new trained model version.\n        train_container_uri (str): Container URI for running train script.\n        serving_container_uri (str): Container URI for deploying the output model.\n        model (Model): Trained model output.\n        metrics (Metrics): Output metrics of trained model.\n        requirements (List[str]): Additional python dependencies for training script.\n        job_name (str): Name of training job.\n        hparams (Dict[str, str]): Hyperparameters (passed as a JSON serialised argument\n            to train script)\n        replica_count (int): Number of replicas (increase for distributed training).\n        machine_type (str): Machine type of compute.\n        accelerator_type (str): Accelerator type (change for GPU support).\n        accelerator_count (str): Accelerator count (increase for GPU cores).\n        parent_model (str): Resource name of existing parent model (optional).\n            If `None`, a new model will be uploaded. Otherwise, a new model version\n            for the parent model will be uploaded.\n    Returns:\n        parent_model (str): Resource URI of the parent model (empty string if the\n            trained model is the first model version of its kind).\n    \"\"\"\n    import json\n    import logging\n    import os.path\n    import time\n    import google.cloud.aiplatform as aip\n    from datetime import datetime\n\n    logging.info(f\"Using train script: {train_script_uri}\")\n    script_path = \"/gcs/\" + train_script_uri[5:]\n    if not os.path.exists(script_path):\n        raise ValueError(\n            \"Train script was not found. \"\n            f\"Check if the path is correct: {train_script_uri}\"\n        )\n\n    thr_str = str(threshold).replace('.', 'p')\n    if model_display_name is None:\n        model_display_name = f\"vanilla_forecast_model_{thr_str}_{datetime.now().strftime('%Y%m%d%H%M%S')}\"\n    else:\n        model_display_name = model_display_name\n    job_name = f\"vanilla_forecast_job_{thr_str}_{datetime.now().strftime('%Y%m%d%H%M%S')}\"\n\n    job = aip.CustomTrainingJob(\n        project=project_id,\n        location=project_location,\n        staging_bucket=staging_bucket,\n        display_name=job_name if job_name else f\"Custom job {int(time.time())}\",\n        script_path=script_path,\n        container_uri=train_container_uri,\n        requirements=requirements,\n        model_serving_container_image_uri=serving_container_uri,\n    )\n    logging.info(f\"train_data path: {train_data.path}\")\n\n    # Add threshold to hparams\n    hp = dict(hparams or {})\n    if threshold is not None:\n        hp[\"threshold\"] = float(threshold)\n    hp_json = json.dumps(hp)\n\n    cmd_args = [\n        f\"--train_data={train_data.path}\",\n        f\"--valid_data={valid_data.path}\",\n        f\"--test_data={test_data.path}\",\n        f\"--metrics={metrics.path}\",\n        f\"--hparams={hp_json}\",\n        f\"--predictions={predictions.path}\",\n    ]\n    # https://cloud.google.com/python/docs/reference/aiplatform/1.18.3/google.cloud.aiplatform.CustomTrainingJob#google_cloud_aiplatform_CustomTrainingJob_run\n    uploaded_model = job.run(\n        model_display_name=model_display_name,\n        parent_model=parent_model,\n        is_default_version=(parent_model is None),\n        args=cmd_args,\n        replica_count=replica_count,\n        machine_type=machine_type,\n        accelerator_type=accelerator_type,\n        accelerator_count=accelerator_count,\n        tensorboard=tensorboard_resource_name,\n        service_account=service_account,\n    )\n\n    # resource_name = f\"{uploaded_model.resource_name}@{uploaded_model.version_id}\"\n    resource_name = uploaded_model.resource_name\n    if \"@\" not in resource_name and getattr(uploaded_model, \"version_id\", None):\n        resource_name = f\"{resource_name}@{uploaded_model.version_id}\"\n\n    model.metadata[\"resourceName\"] = resource_name\n    model.metadata[\"displayName\"] = uploaded_model.display_name\n    model.metadata[\"containerSpec\"] = {\"imageUri\": serving_container_uri}\n    model.uri = uploaded_model.uri\n    model.TYPE_NAME = \"google.VertexModel\"\n\n    with open(metrics.path, \"r\") as fp:\n        parsed_metrics = json.load(fp)\n\n    logging.info(parsed_metrics)\n    for k, v in parsed_metrics.items():\n        if isinstance(v, (float, int)) and v is not None:\n            metrics.log_metric(k, float(v))\n\n"
          ],
          "image": "python:3.10"
        }
      },
      "exec-custom-forecast-train-job-3": {
        "container": {
          "args": [
            "--executor_input",
            "{{$}}",
            "--function_to_execute",
            "custom_forecast_train_job"
          ],
          "command": [
            "sh",
            "-c",
            "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location 'setuptools>=69.0.0' 'wheel' 'cython<3.0.0' '--no-build-isolation' 'pyyaml==5.4.1' 'google-cloud-aiplatform==1.24.1' 'python-json-logger' 'google-cloud-logging' 'datetime'  &&  python3 -m pip install --quiet --no-warn-script-location 'kfp==2.15.2' '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"' && \"$0\" \"$@\"\n",
            "sh",
            "-ec",
            "program_path=$(mktemp -d)\n\nprintf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\n_KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n",
            "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import *\n\ndef custom_forecast_train_job(\n    train_script_uri: str,\n    train_data: Input[Dataset],\n    valid_data: Input[Dataset],\n    test_data: Input[Dataset],\n    project_id: str,\n    project_location: str,\n    # model_display_name: str,\n    threshold: float,\n    train_container_uri: str,\n    serving_container_uri: str,\n    model: Output[Artifact],\n    metrics: Output[Metrics],\n    predictions: Output[Dataset],\n    staging_bucket: str,\n    parent_model: str = None,\n    model_display_name: Optional[str] = None,\n    requirements: Optional[List[str]] = None,\n    job_name: Optional[str] = None,\n    hparams: Optional[Dict[str, str]] = None,\n    # threshold: Optional[List[float]] = None,\n    replica_count: int = 1,\n    machine_type: str = \"n1-standard-4\",\n    accelerator_type: str = \"ACCELERATOR_TYPE_UNSPECIFIED\",\n    accelerator_count: int = 0,\n    tensorboard_resource_name: str = \"projects/derivatives-417104/locations/us-central1/tensorboards/2135763953559535616\",\n    service_account: str = \"vertex-pipelines@derivatives-417104.iam.gserviceaccount.com\",\n):\n    \"\"\"Run a custom training job using a training script.\n\n    The provided script will be invoked by passing the following command-line arguments:\n\n    ```\n    train.py \\\n        --train_data <train_data.path> \\\n        --valid_data <valid_data.path> \\\n        --test_data <test_data.path> \\\n        --metrics <metrics.path> \\\n        --hparams json.dumps(<hparams>)\n    ```\n\n    Ensure that your train script can read these arguments and outputs metrics\n    to the provided path and the model to the correct path based on:\n    https://cloud.google.com/vertex-ai/docs/training/code-requirements.\n\n    Args:\n        train_script_uri (str): gs:// uri to python train script. See:\n            https://cloud.google.com/vertex-ai/docs/training/code-requirements.\n        train_data (Dataset): Training data (passed as an argument to train script)\n        valid_data (Dataset): Validation data (passed as an argument to train script)\n        test_data (Dataset): Test data (passed as an argument to train script).\n        staging_bucket (str): Staging bucket for CustomTrainingJob.\n        project_location (str): location of the Google Cloud project.\n        project_id (str): project id of the Google Cloud project.\n        model_display_name (str): Name of the new trained model version.\n        train_container_uri (str): Container URI for running train script.\n        serving_container_uri (str): Container URI for deploying the output model.\n        model (Model): Trained model output.\n        metrics (Metrics): Output metrics of trained model.\n        requirements (List[str]): Additional python dependencies for training script.\n        job_name (str): Name of training job.\n        hparams (Dict[str, str]): Hyperparameters (passed as a JSON serialised argument\n            to train script)\n        replica_count (int): Number of replicas (increase for distributed training).\n        machine_type (str): Machine type of compute.\n        accelerator_type (str): Accelerator type (change for GPU support).\n        accelerator_count (str): Accelerator count (increase for GPU cores).\n        parent_model (str): Resource name of existing parent model (optional).\n            If `None`, a new model will be uploaded. Otherwise, a new model version\n            for the parent model will be uploaded.\n    Returns:\n        parent_model (str): Resource URI of the parent model (empty string if the\n            trained model is the first model version of its kind).\n    \"\"\"\n    import json\n    import logging\n    import os.path\n    import time\n    import google.cloud.aiplatform as aip\n    from datetime import datetime\n\n    logging.info(f\"Using train script: {train_script_uri}\")\n    script_path = \"/gcs/\" + train_script_uri[5:]\n    if not os.path.exists(script_path):\n        raise ValueError(\n            \"Train script was not found. \"\n            f\"Check if the path is correct: {train_script_uri}\"\n        )\n\n    thr_str = str(threshold).replace('.', 'p')\n    if model_display_name is None:\n        model_display_name = f\"vanilla_forecast_model_{thr_str}_{datetime.now().strftime('%Y%m%d%H%M%S')}\"\n    else:\n        model_display_name = model_display_name\n    job_name = f\"vanilla_forecast_job_{thr_str}_{datetime.now().strftime('%Y%m%d%H%M%S')}\"\n\n    job = aip.CustomTrainingJob(\n        project=project_id,\n        location=project_location,\n        staging_bucket=staging_bucket,\n        display_name=job_name if job_name else f\"Custom job {int(time.time())}\",\n        script_path=script_path,\n        container_uri=train_container_uri,\n        requirements=requirements,\n        model_serving_container_image_uri=serving_container_uri,\n    )\n    logging.info(f\"train_data path: {train_data.path}\")\n\n    # Add threshold to hparams\n    hp = dict(hparams or {})\n    if threshold is not None:\n        hp[\"threshold\"] = float(threshold)\n    hp_json = json.dumps(hp)\n\n    cmd_args = [\n        f\"--train_data={train_data.path}\",\n        f\"--valid_data={valid_data.path}\",\n        f\"--test_data={test_data.path}\",\n        f\"--metrics={metrics.path}\",\n        f\"--hparams={hp_json}\",\n        f\"--predictions={predictions.path}\",\n    ]\n    # https://cloud.google.com/python/docs/reference/aiplatform/1.18.3/google.cloud.aiplatform.CustomTrainingJob#google_cloud_aiplatform_CustomTrainingJob_run\n    uploaded_model = job.run(\n        model_display_name=model_display_name,\n        parent_model=parent_model,\n        is_default_version=(parent_model is None),\n        args=cmd_args,\n        replica_count=replica_count,\n        machine_type=machine_type,\n        accelerator_type=accelerator_type,\n        accelerator_count=accelerator_count,\n        tensorboard=tensorboard_resource_name,\n        service_account=service_account,\n    )\n\n    # resource_name = f\"{uploaded_model.resource_name}@{uploaded_model.version_id}\"\n    resource_name = uploaded_model.resource_name\n    if \"@\" not in resource_name and getattr(uploaded_model, \"version_id\", None):\n        resource_name = f\"{resource_name}@{uploaded_model.version_id}\"\n\n    model.metadata[\"resourceName\"] = resource_name\n    model.metadata[\"displayName\"] = uploaded_model.display_name\n    model.metadata[\"containerSpec\"] = {\"imageUri\": serving_container_uri}\n    model.uri = uploaded_model.uri\n    model.TYPE_NAME = \"google.VertexModel\"\n\n    with open(metrics.path, \"r\") as fp:\n        parsed_metrics = json.load(fp)\n\n    logging.info(parsed_metrics)\n    for k, v in parsed_metrics.items():\n        if isinstance(v, (float, int)) and v is not None:\n            metrics.log_metric(k, float(v))\n\n"
          ],
          "image": "python:3.10"
        }
      },
      "exec-directional-change-detector": {
        "container": {
          "args": [
            "--executor_input",
            "{{$}}",
            "--function_to_execute",
            "directional_change_detector"
          ],
          "command": [
            "sh",
            "-c",
            "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location '--upgrade' 'pip' 'setuptools' 'wheel' 'cython<3.0.0' '--no-build-isolation' 'pyyaml==5.4.1' 'IPython' 'matplotlib' 'numpy' 'pandas' 'seaborn' 'tensorflow' 'google-cloud-storage' 'plotly' 'google-cloud-bigquery' 'uuid' 'typing' 'datetime' 'pyarrow'  &&  python3 -m pip install --quiet --no-warn-script-location 'kfp==2.15.2' '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"' && \"$0\" \"$@\"\n",
            "sh",
            "-ec",
            "program_path=$(mktemp -d)\n\nprintf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\n_KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n",
            "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import *\n\ndef directional_change_detector(\n    df: Input[Dataset],\n    # thresholds: List[float],\n    thresholds: float,\n    # config_gcs_uri: str,\n    # experiment_name: List[str],\n    # experiment_name: str,\n    price_col: str,\n    time_col: str,\n    project_id: str,\n    dataset_id: str,\n    # table_id: List[str],\n    # table_id: str,\n    load_to_bq: bool,\n    fast_plot: bool,\n    max_event_markers: int,\n    directional_change_events: Output[Dataset],\n    figures_output: Output[Dataset],\n):\n\n    # In the following code, we are recording both upward and downward trends.\n    # Note that PDCC2_Up and PDCC_Down refer to the directional change confirmation\n    # points for the upward and downward trends, respectively.\n    # OSV_Up and OSV_Down represent the overshoot values for the upward and downward\n    # trends. Also note that \"Upward Update\" marks the end of an upward trend,\n    # while \"Downward Update\" indicates the end of a downward trend.\n\n    import uuid\n    from typing import Tuple, Union, Optional\n    from datetime import datetime, timezone\n    import pandas as pd\n    import numpy as np\n    import plotly.graph_objects as go\n    import logging\n    from google.cloud import bigquery\n    from google.cloud.exceptions import NotFound\n    from google.cloud import storage\n    import json\n    import os\n    from pathlib import Path\n    class DirectionalChange:\n        \"\"\"\n        Class to compute and plot Directional Change (DC) events for given thresholds,\n        and load summary events into BigQuery with experiment tracking.\n\n        This class preserves the original algorithmic logic written\n        by Dr V L Raju Chinthalapati, University of London. It only\n        organizes the code and adds configurability for running multiple thresholds\n        (downward and upward) in one go, plus experiment tagging in BigQuery.\n        \"\"\"\n\n        def __init__(\n            self,\n            df: pd.DataFrame,\n            thresholds: list,\n            *,\n            price_col: str = \"PRICE\",\n            time_col: str = \"trade_ts\",\n            experiment_name: str,\n            project_id: str = \"derivatives-417104\",\n            dataset_id: str = \"coindesk\",\n            table_id: str = \"dc_events\",\n            load_to_bq: bool = True,\n            fast_plot: bool = False,\n            max_event_markers: Optional[int] = 500,\n            figures_output_path: Optional[str] = None,\n        ) -> None:\n            \"\"\"\n            Parameters\n            ----------\n            df : DataFrame\n                Must contain price and time columns\n                (configurable via `price_col` and `time_col`).\n            thresholds : sequence of floats or (down, up) pairs\n                If an element is a single float, it is used for both downward and upward\n                thresholds. If an element is a (down, up) pair, each direction uses its\n                own value.\n            price_col : str\n                Column name for prices (default 'PRICE').\n            time_col : str\n                Column name for timestamps (default 'load_time_toronto').\n            experiment_name : str\n                Human-readable label for this run; stored with the events.\n            project_id, dataset_id, table_id : str\n                BigQuery destination identifiers.\n            load_to_bq : bool\n                If True, load summary rows into BigQuery.\n                If True, load summary rows into BigQuery.\n            \"\"\"\n            # Store inputs\n            self.df_original = df.copy()\n            self.price_col = price_col\n            self.time_col = time_col\n            # Basic validation\n            missing = [\n                c\n                for c in [self.price_col, self.time_col]\n                if c not in self.df_original.columns\n            ]\n            if missing:\n                raise ValueError(\n                    f\"DirectionalChange: missing required column(s): {missing}\"\n                )\n            # dc_thresholds = [0.001, 0.005, 0.010, 0.015]\n            # It means that by default up and down thresholds are the same\n            # threshold_sets = [(0.001,0.001), (0.005,0.005), (0.010,0.010), (0.015,0.015)]\n            # Means u can have up and down thresholds separately per DC processing\n            self.threshold_sets: list[Tuple[float, float]] = [\n                (t, t)\n                if not isinstance(t, (tuple, list))\n                else (float(t[0]), float(t[1]))\n                for t in thresholds\n            ]\n            self.experiment_name = experiment_name\n            self.experiment_id = str(uuid.uuid4())  # auto-generated experiment ID\n\n            # BigQuery parameters\n            self.project_id = project_id\n            self.dataset_id = dataset_id\n            self.table_id = table_id\n            self.load_to_bq = load_to_bq\n            self.client = bigquery.Client(project=self.project_id)\n\n            # Storage for per-threshold outputs\n            # key = (down, up) tuple -> dict of DC events\n            # self.events is a dictionary\n            # whose keys are (down_threshold, up_threshold) pairs (like (0.01, 0.01)),\n            # and whose values are generic dictionaries containing event data.\n            self.events: dict[Tuple[float, float], dict] = {}\n            # self.figures is a dictionary\n            # whose keys are the same (down_threshold, up_threshold) pairs,\n            # and whose values are Plotly Figure objects corresponding to those thresholds.\n            self.figures: dict[Tuple[float, float], go.Figure] = {}\n\n            # Plot parameter options\n            self.fast_plot = fast_plot\n            self.max_event_markers = max_event_markers\n\n            # Figure output settings\n            self.figures_output_path = figures_output_path\n\n            # Ensure destination table exists with the required schema\n            if self.load_to_bq:\n                self._ensure_table()\n\n        # ---------------------------------------------------------------------\n        # Public API\n        # ---------------------------------------------------------------------\n        def run_all(self) -> None:\n            \"\"\"Run DC detection for all configured thresholds.\"\"\"\n            for down_up in self.threshold_sets:\n                self._process_threshold(down_up)\n\n            # Save all figures if output path is specified\n            if self.figures_output_path:\n                self._save_all_figures()\n\n        def get_events(self, threshold: Union[float, Tuple[float, float]]):\n            \"\"\"Retrieve raw events data and ancillary outputs for a threshold.\n\n            Accepts a single float (interpreted as (t, t)) or a (down, up) pair.\n            Returns a dict with keys: 'downward', 'upward', 'max_OSV', 'max_OSV2', 'df'.\n            \"\"\"\n            if not isinstance(threshold, (tuple, list)):\n                key = (threshold, threshold)\n            else:\n                key = (float(threshold[0]), float(threshold[1]))\n            return self.events.get(key)\n\n        def get_figure(\n            self, threshold: Union[float, Tuple[float, float]]\n        ) -> Optional[go.Figure]:\n            \"\"\"Retrieve the Plotly figure for a given threshold (float or pair).\"\"\"\n            if not isinstance(threshold, (tuple, list)):\n                key = (threshold, threshold)\n            else:\n                key = (float(threshold[0]), float(threshold[1]))\n            return self.figures.get(key)\n\n        def query_summary(\n            self,\n            *,\n            threshold: Optional[Union[float, Tuple[float, float]]] = None,\n            experiment_only: bool = True,\n            distinct: bool = True,\n        ) -> pd.DataFrame:\n            \"\"\"Query previously loaded DC summaries from BigQuery.\n\n            Parameters\n            ----------\n            threshold : float | (down, up) | None\n                If provided, filters rows whose downward==upward==threshold (float)\n                or separately by down/up when a pair is given. If None, no threshold\n                filter.\n            experiment_only : bool\n                If True, restrict to this object's experiment_id.\n            distinct : bool\n                If True, SELECT DISTINCT; else SELECT *.\n            \"\"\"\n            select_kw = \"DISTINCT\" if distinct else \"*\"\n            table_fqn = f\"`{self.project_id}.{self.dataset_id}.{self.table_id}`\"\n\n            where = []\n            if experiment_only:\n                where.append(f\"experiment_id = '{self.experiment_id}'\")\n\n            if threshold is not None:\n                if isinstance(threshold, (tuple, list)):\n                    down, up = float(threshold[0]), float(threshold[1])\n                    where.append(f\"threshold_down = {down}\")\n                    where.append(f\"threshold_up = {up}\")\n                else:\n                    t = float(threshold)\n                    where.append(f\"threshold_down = {t}\")\n                    where.append(f\"threshold_up = {t}\")\n\n            where_sql = (\" WHERE \" + \" AND \".join(where)) if where else \"\"\n            query = f\"\"\"\n            SELECT {select_kw} *\n            FROM {table_fqn}\n            {where_sql}\n            ORDER BY start_time, end_time ASC\n            \"\"\"\n            return self.client.query(query).to_dataframe()\n\n        # ---------------------------------------------------------------------\n        # Internals\n        # ---------------------------------------------------------------------\n        def _ensure_table(self) -> None:\n            \"\"\"Create destination table if it doesn't exist (idempotent).\"\"\"\n            table_ref = self.client.dataset(self.dataset_id).table(self.table_id)\n            try:\n                self.client.get_table(table_ref)\n                return\n            except NotFound:\n                pass\n            except Exception as exc:\n                message = str(exc).lower()\n                if \"notfound\" in message or \"not found\" in message:\n                    pass\n                else:\n                    raise\n\n            schema = [\n                bigquery.SchemaField(\"start_time\", \"TIMESTAMP\", mode=\"REQUIRED\"),\n                bigquery.SchemaField(\"end_time\", \"TIMESTAMP\", mode=\"REQUIRED\"),\n                bigquery.SchemaField(\"start_price\", \"FLOAT64\", mode=\"REQUIRED\"),\n                bigquery.SchemaField(\"end_price\", \"FLOAT64\", mode=\"REQUIRED\"),\n                bigquery.SchemaField(\"event_type\", \"STRING\", mode=\"REQUIRED\"),\n                bigquery.SchemaField(\"threshold_down\", \"FLOAT64\", mode=\"REQUIRED\"),\n                bigquery.SchemaField(\"threshold_up\", \"FLOAT64\", mode=\"REQUIRED\"),\n                bigquery.SchemaField(\"experiment_name\", \"STRING\"),\n                bigquery.SchemaField(\"experiment_id\", \"STRING\"),\n            ]\n            table = bigquery.Table(table_ref, schema=schema)\n            self.client.create_table(table)\n            print(\n                f\"Created table {table_ref.project}.\"\n                f\"{table_ref.dataset_id}.\"\n                f\"{table_ref.table_id}\"\n            )\n\n        def _process_threshold(self, down_up: Tuple[float, float]) -> None:\n            \"\"\"\n            Process a single (down, up) threshold pair: detect DC events, store,\n            plot, and optionally load a summary to BigQuery.\n            \"\"\"\n            threshold_down, threshold_up = float(down_up[0]), float(down_up[1])\n\n            # Copy dataframe and extract series\n            # TODO: investigate if we can use the original dataframe without copying....again!\n            # We copied already in the constructor\n            df = self.df_original.copy()\n            ask_prices = df[self.price_col]\n            dates = df[self.time_col]\n\n            # Prepare columns\n            # TODO: DO WE NEED THESE COLUMNS? We write an events table not a ticks table\n            df[\"event\"] = pd.Series(pd.NA, index=df.index, dtype=\"object\")\n            df[\"event_type\"] = pd.Series(pd.NA, index=df.index, dtype=\"object\")\n            # Date placeholders (timestamps)\n            PDCC_date = pd.NaT\n            max_date = pd.NaT\n            osv_down_date_today = pd.NaT\n            osv_up_date_today = pd.NaT\n            PDCC2_date = pd.NaT\n            min_date = pd.NaT\n\n            # Initialize tracking variables for downward trends\n            maxPrice = 0.5  # highest price seen before confirming a down-trend\n            minPrice = 10_000_000  # placeholder large number; reset once trend starts\n            PDCC = 0  # price at which down-trend is confirmed\n            OSV = 0  # instantaneous overshoot during a down-trend\n            max_OSV = 0  # maximum overshoot seen in a single down-trend\n            filter_down = 0  # flag: 0=not in down-trend, 1=in down-trend\n            filter_up = 0  # flag: prevents overlap with up-trend logic\n\n            # Initialize tracking variables for upward trends\n            maxPrice2 = 0.5  # lowest price seen before confirming an up-trend\n            minPrice2 = (\n                10_000_000  # placeholder large number; reset once up-trend starts\n            )\n            PDCC2 = 0  # price at which up-trend is confirmed\n            OSV2 = 0  # instantaneous overshoot during an up-trend\n            max_OSV2 = 0  # maximum overshoot seen in a single up-trend\n\n            # Lists to record each event for later plotting\n\n            # Initialize containers for detected DC events.\n            # Each event is stored as a tuple in the form:\n            # (start_time, end_time, start_price, end_price, event_type)\n            # Using lists of tuples keeps the inner loop lightweight and fast,\n            # avoiding the overhead of appending rows to a DataFrame during iteration.\n            # These lists are later converted to a pandas DataFrame for BigQuery loading.\n            downward_events: list[tuple] = []\n            upward_events: list[tuple] = []\n\n            # Main loop over each price/time\n            for askPrice, date_today in zip(ask_prices, dates):\n\n                # --- Section A: Detect start of a downward trend ---\n                if filter_down == 0:\n                    difference = maxPrice - askPrice\n                    # If price has fallen at least Threshold% from the previous max\n                    if maxPrice > askPrice and difference >= maxPrice * threshold_down:\n                        # Confirm PDCC_Down event\n                        minPrice = askPrice\n                        PDCC = askPrice\n                        filter_down = 1  # enter down-trend mode\n                        max_OSV = 0  # reset overshoot counter\n                        filter_up = 0  # ensure up-trend logic is off\n\n                        # Record the start of the down-trend\n                        downward_events.append(\n                            (max_date, date_today, maxPrice, askPrice, \"PDCC_Down\")\n                        )\n                        PDCC_date = date_today\n                        df.loc[df[self.time_col] == date_today, \"event\"] = \"PDCC_Down\"\n                        df.loc[\n                            df[self.time_col] == date_today, \"event_type\"\n                        ] = \"Downward Start\"\n\n                        # Also mark the last up-trend\u2019s overshoot update\n                        upward_events.append(\n                            (PDCC2_date, osv_up_date_today, PDCC2, maxPrice2, \"OSV_Up\")\n                        )\n                        df.loc[\n                            df[self.time_col] == osv_up_date_today, \"event\"\n                        ] = \"OSV_Up\"\n                        df.loc[\n                            df[self.time_col] == osv_up_date_today, \"event_type\"\n                        ] = \"Upward Update\"\n\n                        # Reset up-trend trackers\n                        minPrice2 = 10_000_000\n                        maxPrice2 = 0.5\n\n                    # If price goes above the previous max, just update maxPrice\n                    if askPrice > maxPrice:\n                        maxPrice = askPrice\n                        max_date = date_today\n\n                # --- Section B: While inside a downward trend, measure overshoot ---\n                if filter_down == 1:\n                    # If price has risen above the local low, compute overshoot\n                    if minPrice < askPrice:\n                        OSV = (PDCC - askPrice) / (PDCC * threshold_down)\n                        max_OSV = max(max_OSV, OSV)\n                        # Clamp negative overshoots to zero\n                        if OSV < 0:\n                            OSV = 0\n                            # If an upward signal is already active, exit down-trend\n                            if filter_up == 1:\n                                filter_down = 0\n                                minPrice = 10_000_000\n                                maxPrice = 0.5\n\n                    # Always update the running low and overshoot marker date\n                    if minPrice >= askPrice or filter_up == 1:\n                        minPrice = askPrice\n                        osv_down_date_today = date_today\n                        OSV = (PDCC - askPrice) / (PDCC * threshold_down)\n                        max_OSV = max(max_OSV, OSV)\n\n                # --- Section C: Detect start of an upward trend ---\n                if filter_up == 0:\n                    difference2 = askPrice - minPrice2\n                    # If price has risen at least Threshold2% from the previous min\n                    if minPrice2 < askPrice and difference2 >= minPrice2 * threshold_up:\n                        # Confirm PDCC2_UP event\n                        maxPrice2 = askPrice\n                        PDCC2 = askPrice\n                        filter_up = 1  # enter up-trend mode\n                        max_OSV2 = 0  # reset overshoot counter\n\n                        # Record the start of the up-trend\n                        upward_events.append(\n                            (min_date, date_today, minPrice2, PDCC2, \"PDCC2_UP\")\n                        )\n                        PDCC2_date = date_today\n                        df.loc[df[self.time_col] == date_today, \"event\"] = \"PDCC2_UP\"\n                        df.loc[\n                            df[self.time_col] == date_today, \"event_type\"\n                        ] = \"Upward Start\"\n\n                        # Also mark the last down-trend\u2019s overshoot update\n                        downward_events.append(\n                            (PDCC_date, osv_down_date_today, PDCC, minPrice, \"OSV_Down\")\n                        )\n                        df.loc[\n                            df[self.time_col] == osv_down_date_today, \"event\"\n                        ] = \"OSV_Down\"\n                        df.loc[\n                            df[self.time_col] == osv_down_date_today, \"event_type\"\n                        ] = \"Downward Update\"\n\n                        # Reset down-trend trackers\n                        filter_down = 0\n                        minPrice = 10_000_000\n                        maxPrice = 0.5\n\n                    # If price dips below previous low, just update minPrice2\n                    if askPrice < minPrice2:\n                        minPrice2 = askPrice\n                        min_date = date_today\n\n                # --- Section D: While inside an upward trend, measure overshoot ---\n                if filter_up == 1:\n                    # If price has fallen back below the running max, compute overshoot\n                    if askPrice < maxPrice2:\n                        OSV2 = (askPrice - PDCC2) / (PDCC2 * threshold_up)\n                        max_OSV2 = max(max_OSV2, OSV2)\n                        # Clamp negative overshoots to zero\n                        if OSV2 < 0:\n                            OSV2 = 0\n                            # If a down-signal is already active, exit up-trend\n                            if filter_down == 1:\n                                filter_up = 0\n                                minPrice2 = 10_000_000\n                                maxPrice2 = 0.5\n\n                    # Always update the running high and overshoot marker date\n                    if askPrice >= maxPrice2 or filter_down == 1:\n                        maxPrice2 = askPrice\n                        osv_up_date_today = date_today\n                        OSV2 = (askPrice - PDCC2) / (PDCC2 * threshold_up)\n                        max_OSV2 = max(max_OSV2, OSV2)\n\n            # Final results: the largest overshoot seen in any down- or up-trend\n            print(\n                f\"(down={threshold_down}, up={threshold_up}) \"\n                f\"Final maximum OSV for downward trends: {max_OSV}\"\n            )\n            print(\n                f\"(down={threshold_down}, up={threshold_up}) \"\n                f\"Final maximum OSV2 for upward trends:   {max_OSV2}\"\n            )\n\n            # Store events for this threshold pair\n            key = (threshold_down, threshold_up)\n            self.events[key] = {\n                \"downward\": downward_events,\n                \"upward\": upward_events,\n                \"max_OSV\": max_OSV,\n                \"max_OSV2\": max_OSV2,\n                \"df\": df,\n            }\n\n            # Build a Plotly figure marking all PDCC and OSV events\n            fig = self._build_plot(\n                df=df,\n                downward_events=downward_events,\n                upward_events=upward_events,\n                threshold_pair=key,\n            )\n            self.figures[key] = fig\n\n            # Load summary to BigQuery\n            if self.load_to_bq:\n                self._load_dc_events(\n                    downward_events + upward_events, threshold_pair=key\n                )\n\n        def _build_plot(\n            self,\n            *,\n            df: pd.DataFrame,\n            downward_events: list[tuple],\n            upward_events: list[tuple],\n            threshold_pair: Tuple[float, float],\n        ) -> go.Figure:\n            \"\"\"\n            Build a Plotly figure marking PDCC and OSV events on prices.\n            \"\"\"\n            dates = df[self.time_col]\n            prices = df[self.price_col]\n            th_down, th_up = threshold_pair\n            fig = go.Figure()\n            # Use WebGL for faster rendering when requested\n            price_trace_cls = go.Scattergl if self.fast_plot else go.Scatter\n            fig.add_trace(\n                price_trace_cls(x=dates, y=prices, mode=\"lines\", name=\"Ask Prices\")\n            )\n\n            # Plot downward-trend starts and updates\n            downward_start_added = False\n            downward_update_added = False\n            down_iter = (\n                downward_events[-self.max_event_markers :]\n                if (self.fast_plot and self.max_event_markers)\n                else downward_events\n            )\n            for prev, today, price0, price1, event in down_iter:\n                if event == \"PDCC_Down\":\n                    fig.add_trace(\n                        go.Scatter(\n                            x=[today],\n                            y=[price1],\n                            mode=\"markers\",\n                            marker=dict(color=\"red\", symbol=\"triangle-down\"),\n                            name=\"Downward Start\",\n                            showlegend=not downward_start_added,\n                        )\n                    )\n                    downward_start_added = True\n                elif event == \"OSV_Down\":\n                    fig.add_trace(\n                        go.Scatter(\n                            x=[today],\n                            y=[price1],\n                            mode=\"markers\",\n                            marker=dict(color=\"orange\", symbol=\"triangle-down\"),\n                            name=\"Downward Update\",\n                            showlegend=not downward_update_added,\n                        )\n                    )\n                    downward_update_added = True\n\n                if pd.notna(prev) and pd.notna(today):\n                    fig.add_shape(\n                        type=\"line\",\n                        x0=prev,\n                        y0=price0,\n                        x1=today,\n                        y1=price1,\n                        line=dict(\n                            color=\"red\" if event == \"PDCC_Down\" else \"black\",\n                            width=2,\n                            dash=\"dashdot\",\n                        ),\n                        opacity=0.7,\n                    )\n\n            # Plot upward-trend starts and updates on the *same* figure\n            upward_start_added = False\n            upward_update_added = False\n            up_iter = (\n                upward_events[-self.max_event_markers :]\n                if (self.fast_plot and self.max_event_markers)\n                else upward_events\n            )\n            for prev, today, price0, price1, event in up_iter:\n                if event == \"PDCC2_UP\":\n                    fig.add_trace(\n                        go.Scatter(\n                            x=[today],\n                            y=[price1],\n                            mode=\"markers\",\n                            marker=dict(color=\"green\", symbol=\"triangle-up\"),\n                            name=\"Upward Start\",\n                            showlegend=not upward_start_added,\n                        )\n                    )\n                    upward_start_added = True\n                elif event == \"OSV_Up\":\n                    fig.add_trace(\n                        go.Scatter(\n                            x=[today],\n                            y=[price1],\n                            mode=\"markers\",\n                            marker=dict(color=\"magenta\", symbol=\"triangle-up\"),\n                            name=\"Upward Update\",\n                            showlegend=not upward_update_added,\n                        )\n                    )\n                    upward_update_added = True\n\n                if pd.notna(prev) and pd.notna(today):\n                    fig.add_shape(\n                        type=\"line\",\n                        x0=prev,\n                        y0=price0,\n                        x1=today,\n                        y1=price1,\n                        line=dict(\n                            color=\"green\" if event == \"PDCC2_UP\" else \"magenta\",\n                            width=2,\n                            dash=\"dashdot\",\n                        ),\n                        opacity=0.7,\n                    )\n\n            fig.update_layout(\n                title=f\"DC Events (Thresholds: down={th_down}, up={th_up})\",\n                xaxis_title=\"Date\",\n                yaxis_title=\"Price\",\n                legend_title=\"Events\",\n                legend=dict(itemsizing=\"constant\"),\n            )\n            return fig\n\n        def _load_dc_events(\n            self, events: list[tuple], *, threshold_pair: Tuple[float, float]\n        ) -> Optional[pd.DataFrame]:\n            \"\"\"\n            Loads DC summary events to BigQuery with experiment metadata.\n\n            - Remove the default null start/end dates.\n            - Create DC summary events DataFrame and load it to BQ.\n            \"\"\"\n            if len(events) <= 1:\n                print(\"No DC events to load\")\n                return None\n\n            # Remove the default null start/end dates\n            clean_events = [ev for ev in events if pd.notna(ev[0]) and pd.notna(ev[1])]\n            if not clean_events:\n                print(\"No DC events to load\")\n                return None\n\n            # Create DC summary events dataframe\n            df_ev = pd.DataFrame(\n                clean_events,\n                columns=[\n                    \"start_time\",\n                    \"end_time\",\n                    \"start_price\",\n                    \"end_price\",\n                    \"event_type\",\n                ],\n            )\n            df_ev[\"threshold_down\"], df_ev[\"threshold_up\"] = threshold_pair\n            df_ev[\"experiment_name\"] = self.experiment_name\n            df_ev[\"experiment_id\"] = self.experiment_id\n\n            # # Ensure pandas Timestamps become UTC tz-aware\n            # df_ev[\"start_time\"] = pd.to_datetime(df_ev[\"start_time\"], utc=True, errors=\"coerce\")\n            # df_ev[\"end_time\"] = pd.to_datetime(df_ev[\"end_time\"], utc=True, errors=\"coerce\")\n\n            # Ensure timestamps are timezone-aware but do NOT drop any rows\n            df_ev[\"start_time\"] = pd.to_datetime(\n                df_ev[\"start_time\"], utc=True, format=\"mixed\", errors=\"raise\"\n            )\n            df_ev[\"end_time\"] = pd.to_datetime(\n                df_ev[\"end_time\"], utc=True, format=\"mixed\", errors=\"raise\"\n            )\n\n            table_ref = self.client.dataset(self.dataset_id).table(self.table_id)\n            job = self.client.load_table_from_dataframe(df_ev, table_ref)\n            job.result()  # wait for load\n            print(\"Loaded DC events to BigQuery.\")\n            return df_ev\n\n        def _save_figure_to_gcs(\n            self, fig: go.Figure, threshold_pair: Tuple[float, float]\n        ) -> str:\n            \"\"\"Save a Plotly figure to GCS as HTML and return the GCS path.\"\"\"\n            if not self.figures_output_path:\n                return \"\"\n\n            # Create filename based on threshold\n            down_th, up_th = threshold_pair\n            filename = f\"dc_events_down_{down_th}_up_{up_th}.html\"\n\n            # Parse GCS path\n            if self.figures_output_path.startswith(\"gs://\"):\n                bucket_name = self.figures_output_path[5:].split(\"/\")[0]\n                prefix = \"/\".join(self.figures_output_path[5:].split(\"/\")[1:])\n                if prefix and not prefix.endswith(\"/\"):\n                    prefix += \"/\"\n                gcs_path = f\"{prefix}{filename}\"\n            else:\n                # Assume it's a local path for testing\n                gcs_path = os.path.join(self.figures_output_path, filename)\n                os.makedirs(os.path.dirname(gcs_path), exist_ok=True)\n                fig.write_html(gcs_path)\n                return gcs_path\n\n            # Save to GCS\n            client = storage.Client(project=self.project_id)\n            bucket = client.bucket(bucket_name)\n            blob = bucket.blob(gcs_path)\n\n            # Convert figure to HTML string\n            html_content = fig.to_html(include_plotlyjs=\"cdn\")\n            blob.upload_from_string(html_content, content_type=\"text/html\")\n\n            full_gcs_path = f\"gs://{bucket_name}/{gcs_path}\"\n            print(f\"Saved figure to: {full_gcs_path}\")\n            return full_gcs_path\n\n        def _save_all_figures(self) -> None:\n            \"\"\"Save all generated figures to GCS.\"\"\"\n            if not self.figures_output_path:\n                return\n\n            saved_paths = []\n            for threshold_pair, fig in self.figures.items():\n                gcs_path = self._save_figure_to_gcs(fig, threshold_pair)\n                if gcs_path:\n                    saved_paths.append(gcs_path)\n\n            # Save metadata about saved figures\n            if saved_paths:\n                metadata = {\n                    \"experiment_name\": self.experiment_name,\n                    \"experiment_id\": self.experiment_id,\n                    \"saved_figures\": saved_paths,\n                    \"timestamp\": datetime.now(timezone.utc).isoformat(),\n                }\n\n                # Save metadata to GCS\n                if self.figures_output_path.startswith(\"gs://\"):\n                    bucket_name = self.figures_output_path[5:].split(\"/\")[0]\n                    prefix = \"/\".join(self.figures_output_path[5:].split(\"/\")[1:])\n                    if prefix and not prefix.endswith(\"/\"):\n                        prefix += \"/\"\n                    metadata_path = f\"{prefix}figures_metadata.json\"\n\n                    client = storage.Client(project=self.project_id)\n                    bucket = client.bucket(bucket_name)\n                    blob = bucket.blob(metadata_path)\n                    blob.upload_from_string(\n                        json.dumps(metadata, indent=2), content_type=\"application/json\"\n                    )\n                    print(f\"Saved metadata to: gs://{bucket_name}/{metadata_path}\")\n\n    # Start here\n    # Create df from input dataset\n    df = pd.read_csv(df.path)\n\n    # Define threshold string e.g. 0p01\n    thr_str = str(thresholds).replace('.', 'p')\n    # Define experiment name e.g. exp_dc_detection_0p01_20251010173542\n    experiment_name = f\"exp_dc_detection_{thr_str}_{datetime.now().strftime('%Y%m%d%H%M%S')}\"\n    # Define BQ table id e.g. dc_events_threshold_0p01\n    table_id = f\"dc_events_threshold_{thr_str}\"\n\n    # Create a DirectionalChange object\n    dc = DirectionalChange(\n        df=df,  # must have columns PRICE and load_time_toronto\n        thresholds=[thresholds],  # used for both down & up\n        experiment_name=experiment_name,\n        price_col=price_col,\n        time_col=time_col,\n        project_id=project_id,\n        dataset_id=dataset_id,\n        table_id=table_id,  # optional, defaults to \"dc_events\"\n        load_to_bq=load_to_bq,  # set False to skip BQ loads\n        fast_plot=fast_plot,\n        max_event_markers=max_event_markers,\n        figures_output_path=figures_output.path,\n    )\n    dc.run_all()\n\n    # Save events data to output\n    events_data = []\n    for threshold_pair in dc.threshold_sets:\n        events = dc.get_events(threshold_pair)\n        if events:\n            # Convert events to DataFrame format for output\n            down_events = events[\"downward\"]\n            up_events = events[\"upward\"]\n\n            for event in down_events + up_events:\n                # Filter out events with missing start/end times.\n                # These are typically the first PDCC events where start time is NaN.\n                if len(event) >= 5 and pd.notna(event[0]) and pd.notna(event[1]):\n                    events_data.append(\n                        {\n                            \"start_time\": event[0],\n                            \"end_time\": event[1],\n                            \"start_price\": event[2],\n                            \"end_price\": event[3],\n                            \"event_type\": event[4],\n                            \"threshold_down\": threshold_pair[0],\n                            \"threshold_up\": threshold_pair[1],\n                            \"experiment_name\": experiment_name,\n                            \"experiment_id\": dc.experiment_id,\n                        }\n                    )\n    # Save events data to output\n    if events_data:\n        events_df = pd.DataFrame(events_data)\n        events_df.to_csv(directional_change_events.path, index=False)\n        print(f\"Saved {len(events_data)} events to {directional_change_events.path}\")\n\n    # Figures_output.path is a directory like: /gcs/.../figures_output\n    out_dir = Path(figures_output.path)\n    out_dir.mkdir(parents=True, exist_ok=True)\n\n    # File name inside the directory\n    out_file = out_dir / \"figures_metadata.json\"\n\n    # Save figures metadata to output\n    figures_metadata = {\n        \"experiment_name\": experiment_name,\n        \"experiment_id\": dc.experiment_id,\n        \"saved_figures\": list(dc.figures.keys()),\n        \"timestamp\": datetime.now(timezone.utc).isoformat(),\n    }\n\n    with out_file.open(\"w\", encoding=\"utf-8\") as f:\n        json.dump(figures_metadata, f, indent=2, ensure_ascii=False)\n\n    print(f\"Saved figures metadata to {out_file}\")\n    print(\n        f\"Generated {len(dc.figures)} figures for thresholds: {list(dc.figures.keys())}\"\n    )\n\n"
          ],
          "image": "python:3.11"
        }
      },
      "exec-directional-change-detector-2": {
        "container": {
          "args": [
            "--executor_input",
            "{{$}}",
            "--function_to_execute",
            "directional_change_detector"
          ],
          "command": [
            "sh",
            "-c",
            "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location '--upgrade' 'pip' 'setuptools' 'wheel' 'cython<3.0.0' '--no-build-isolation' 'pyyaml==5.4.1' 'IPython' 'matplotlib' 'numpy' 'pandas' 'seaborn' 'tensorflow' 'google-cloud-storage' 'plotly' 'google-cloud-bigquery' 'uuid' 'typing' 'datetime' 'pyarrow'  &&  python3 -m pip install --quiet --no-warn-script-location 'kfp==2.15.2' '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"' && \"$0\" \"$@\"\n",
            "sh",
            "-ec",
            "program_path=$(mktemp -d)\n\nprintf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\n_KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n",
            "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import *\n\ndef directional_change_detector(\n    df: Input[Dataset],\n    # thresholds: List[float],\n    thresholds: float,\n    # config_gcs_uri: str,\n    # experiment_name: List[str],\n    # experiment_name: str,\n    price_col: str,\n    time_col: str,\n    project_id: str,\n    dataset_id: str,\n    # table_id: List[str],\n    # table_id: str,\n    load_to_bq: bool,\n    fast_plot: bool,\n    max_event_markers: int,\n    directional_change_events: Output[Dataset],\n    figures_output: Output[Dataset],\n):\n\n    # In the following code, we are recording both upward and downward trends.\n    # Note that PDCC2_Up and PDCC_Down refer to the directional change confirmation\n    # points for the upward and downward trends, respectively.\n    # OSV_Up and OSV_Down represent the overshoot values for the upward and downward\n    # trends. Also note that \"Upward Update\" marks the end of an upward trend,\n    # while \"Downward Update\" indicates the end of a downward trend.\n\n    import uuid\n    from typing import Tuple, Union, Optional\n    from datetime import datetime, timezone\n    import pandas as pd\n    import numpy as np\n    import plotly.graph_objects as go\n    import logging\n    from google.cloud import bigquery\n    from google.cloud.exceptions import NotFound\n    from google.cloud import storage\n    import json\n    import os\n    from pathlib import Path\n    class DirectionalChange:\n        \"\"\"\n        Class to compute and plot Directional Change (DC) events for given thresholds,\n        and load summary events into BigQuery with experiment tracking.\n\n        This class preserves the original algorithmic logic written\n        by Dr V L Raju Chinthalapati, University of London. It only\n        organizes the code and adds configurability for running multiple thresholds\n        (downward and upward) in one go, plus experiment tagging in BigQuery.\n        \"\"\"\n\n        def __init__(\n            self,\n            df: pd.DataFrame,\n            thresholds: list,\n            *,\n            price_col: str = \"PRICE\",\n            time_col: str = \"trade_ts\",\n            experiment_name: str,\n            project_id: str = \"derivatives-417104\",\n            dataset_id: str = \"coindesk\",\n            table_id: str = \"dc_events\",\n            load_to_bq: bool = True,\n            fast_plot: bool = False,\n            max_event_markers: Optional[int] = 500,\n            figures_output_path: Optional[str] = None,\n        ) -> None:\n            \"\"\"\n            Parameters\n            ----------\n            df : DataFrame\n                Must contain price and time columns\n                (configurable via `price_col` and `time_col`).\n            thresholds : sequence of floats or (down, up) pairs\n                If an element is a single float, it is used for both downward and upward\n                thresholds. If an element is a (down, up) pair, each direction uses its\n                own value.\n            price_col : str\n                Column name for prices (default 'PRICE').\n            time_col : str\n                Column name for timestamps (default 'load_time_toronto').\n            experiment_name : str\n                Human-readable label for this run; stored with the events.\n            project_id, dataset_id, table_id : str\n                BigQuery destination identifiers.\n            load_to_bq : bool\n                If True, load summary rows into BigQuery.\n                If True, load summary rows into BigQuery.\n            \"\"\"\n            # Store inputs\n            self.df_original = df.copy()\n            self.price_col = price_col\n            self.time_col = time_col\n            # Basic validation\n            missing = [\n                c\n                for c in [self.price_col, self.time_col]\n                if c not in self.df_original.columns\n            ]\n            if missing:\n                raise ValueError(\n                    f\"DirectionalChange: missing required column(s): {missing}\"\n                )\n            # dc_thresholds = [0.001, 0.005, 0.010, 0.015]\n            # It means that by default up and down thresholds are the same\n            # threshold_sets = [(0.001,0.001), (0.005,0.005), (0.010,0.010), (0.015,0.015)]\n            # Means u can have up and down thresholds separately per DC processing\n            self.threshold_sets: list[Tuple[float, float]] = [\n                (t, t)\n                if not isinstance(t, (tuple, list))\n                else (float(t[0]), float(t[1]))\n                for t in thresholds\n            ]\n            self.experiment_name = experiment_name\n            self.experiment_id = str(uuid.uuid4())  # auto-generated experiment ID\n\n            # BigQuery parameters\n            self.project_id = project_id\n            self.dataset_id = dataset_id\n            self.table_id = table_id\n            self.load_to_bq = load_to_bq\n            self.client = bigquery.Client(project=self.project_id)\n\n            # Storage for per-threshold outputs\n            # key = (down, up) tuple -> dict of DC events\n            # self.events is a dictionary\n            # whose keys are (down_threshold, up_threshold) pairs (like (0.01, 0.01)),\n            # and whose values are generic dictionaries containing event data.\n            self.events: dict[Tuple[float, float], dict] = {}\n            # self.figures is a dictionary\n            # whose keys are the same (down_threshold, up_threshold) pairs,\n            # and whose values are Plotly Figure objects corresponding to those thresholds.\n            self.figures: dict[Tuple[float, float], go.Figure] = {}\n\n            # Plot parameter options\n            self.fast_plot = fast_plot\n            self.max_event_markers = max_event_markers\n\n            # Figure output settings\n            self.figures_output_path = figures_output_path\n\n            # Ensure destination table exists with the required schema\n            if self.load_to_bq:\n                self._ensure_table()\n\n        # ---------------------------------------------------------------------\n        # Public API\n        # ---------------------------------------------------------------------\n        def run_all(self) -> None:\n            \"\"\"Run DC detection for all configured thresholds.\"\"\"\n            for down_up in self.threshold_sets:\n                self._process_threshold(down_up)\n\n            # Save all figures if output path is specified\n            if self.figures_output_path:\n                self._save_all_figures()\n\n        def get_events(self, threshold: Union[float, Tuple[float, float]]):\n            \"\"\"Retrieve raw events data and ancillary outputs for a threshold.\n\n            Accepts a single float (interpreted as (t, t)) or a (down, up) pair.\n            Returns a dict with keys: 'downward', 'upward', 'max_OSV', 'max_OSV2', 'df'.\n            \"\"\"\n            if not isinstance(threshold, (tuple, list)):\n                key = (threshold, threshold)\n            else:\n                key = (float(threshold[0]), float(threshold[1]))\n            return self.events.get(key)\n\n        def get_figure(\n            self, threshold: Union[float, Tuple[float, float]]\n        ) -> Optional[go.Figure]:\n            \"\"\"Retrieve the Plotly figure for a given threshold (float or pair).\"\"\"\n            if not isinstance(threshold, (tuple, list)):\n                key = (threshold, threshold)\n            else:\n                key = (float(threshold[0]), float(threshold[1]))\n            return self.figures.get(key)\n\n        def query_summary(\n            self,\n            *,\n            threshold: Optional[Union[float, Tuple[float, float]]] = None,\n            experiment_only: bool = True,\n            distinct: bool = True,\n        ) -> pd.DataFrame:\n            \"\"\"Query previously loaded DC summaries from BigQuery.\n\n            Parameters\n            ----------\n            threshold : float | (down, up) | None\n                If provided, filters rows whose downward==upward==threshold (float)\n                or separately by down/up when a pair is given. If None, no threshold\n                filter.\n            experiment_only : bool\n                If True, restrict to this object's experiment_id.\n            distinct : bool\n                If True, SELECT DISTINCT; else SELECT *.\n            \"\"\"\n            select_kw = \"DISTINCT\" if distinct else \"*\"\n            table_fqn = f\"`{self.project_id}.{self.dataset_id}.{self.table_id}`\"\n\n            where = []\n            if experiment_only:\n                where.append(f\"experiment_id = '{self.experiment_id}'\")\n\n            if threshold is not None:\n                if isinstance(threshold, (tuple, list)):\n                    down, up = float(threshold[0]), float(threshold[1])\n                    where.append(f\"threshold_down = {down}\")\n                    where.append(f\"threshold_up = {up}\")\n                else:\n                    t = float(threshold)\n                    where.append(f\"threshold_down = {t}\")\n                    where.append(f\"threshold_up = {t}\")\n\n            where_sql = (\" WHERE \" + \" AND \".join(where)) if where else \"\"\n            query = f\"\"\"\n            SELECT {select_kw} *\n            FROM {table_fqn}\n            {where_sql}\n            ORDER BY start_time, end_time ASC\n            \"\"\"\n            return self.client.query(query).to_dataframe()\n\n        # ---------------------------------------------------------------------\n        # Internals\n        # ---------------------------------------------------------------------\n        def _ensure_table(self) -> None:\n            \"\"\"Create destination table if it doesn't exist (idempotent).\"\"\"\n            table_ref = self.client.dataset(self.dataset_id).table(self.table_id)\n            try:\n                self.client.get_table(table_ref)\n                return\n            except NotFound:\n                pass\n            except Exception as exc:\n                message = str(exc).lower()\n                if \"notfound\" in message or \"not found\" in message:\n                    pass\n                else:\n                    raise\n\n            schema = [\n                bigquery.SchemaField(\"start_time\", \"TIMESTAMP\", mode=\"REQUIRED\"),\n                bigquery.SchemaField(\"end_time\", \"TIMESTAMP\", mode=\"REQUIRED\"),\n                bigquery.SchemaField(\"start_price\", \"FLOAT64\", mode=\"REQUIRED\"),\n                bigquery.SchemaField(\"end_price\", \"FLOAT64\", mode=\"REQUIRED\"),\n                bigquery.SchemaField(\"event_type\", \"STRING\", mode=\"REQUIRED\"),\n                bigquery.SchemaField(\"threshold_down\", \"FLOAT64\", mode=\"REQUIRED\"),\n                bigquery.SchemaField(\"threshold_up\", \"FLOAT64\", mode=\"REQUIRED\"),\n                bigquery.SchemaField(\"experiment_name\", \"STRING\"),\n                bigquery.SchemaField(\"experiment_id\", \"STRING\"),\n            ]\n            table = bigquery.Table(table_ref, schema=schema)\n            self.client.create_table(table)\n            print(\n                f\"Created table {table_ref.project}.\"\n                f\"{table_ref.dataset_id}.\"\n                f\"{table_ref.table_id}\"\n            )\n\n        def _process_threshold(self, down_up: Tuple[float, float]) -> None:\n            \"\"\"\n            Process a single (down, up) threshold pair: detect DC events, store,\n            plot, and optionally load a summary to BigQuery.\n            \"\"\"\n            threshold_down, threshold_up = float(down_up[0]), float(down_up[1])\n\n            # Copy dataframe and extract series\n            # TODO: investigate if we can use the original dataframe without copying....again!\n            # We copied already in the constructor\n            df = self.df_original.copy()\n            ask_prices = df[self.price_col]\n            dates = df[self.time_col]\n\n            # Prepare columns\n            # TODO: DO WE NEED THESE COLUMNS? We write an events table not a ticks table\n            df[\"event\"] = pd.Series(pd.NA, index=df.index, dtype=\"object\")\n            df[\"event_type\"] = pd.Series(pd.NA, index=df.index, dtype=\"object\")\n            # Date placeholders (timestamps)\n            PDCC_date = pd.NaT\n            max_date = pd.NaT\n            osv_down_date_today = pd.NaT\n            osv_up_date_today = pd.NaT\n            PDCC2_date = pd.NaT\n            min_date = pd.NaT\n\n            # Initialize tracking variables for downward trends\n            maxPrice = 0.5  # highest price seen before confirming a down-trend\n            minPrice = 10_000_000  # placeholder large number; reset once trend starts\n            PDCC = 0  # price at which down-trend is confirmed\n            OSV = 0  # instantaneous overshoot during a down-trend\n            max_OSV = 0  # maximum overshoot seen in a single down-trend\n            filter_down = 0  # flag: 0=not in down-trend, 1=in down-trend\n            filter_up = 0  # flag: prevents overlap with up-trend logic\n\n            # Initialize tracking variables for upward trends\n            maxPrice2 = 0.5  # lowest price seen before confirming an up-trend\n            minPrice2 = (\n                10_000_000  # placeholder large number; reset once up-trend starts\n            )\n            PDCC2 = 0  # price at which up-trend is confirmed\n            OSV2 = 0  # instantaneous overshoot during an up-trend\n            max_OSV2 = 0  # maximum overshoot seen in a single up-trend\n\n            # Lists to record each event for later plotting\n\n            # Initialize containers for detected DC events.\n            # Each event is stored as a tuple in the form:\n            # (start_time, end_time, start_price, end_price, event_type)\n            # Using lists of tuples keeps the inner loop lightweight and fast,\n            # avoiding the overhead of appending rows to a DataFrame during iteration.\n            # These lists are later converted to a pandas DataFrame for BigQuery loading.\n            downward_events: list[tuple] = []\n            upward_events: list[tuple] = []\n\n            # Main loop over each price/time\n            for askPrice, date_today in zip(ask_prices, dates):\n\n                # --- Section A: Detect start of a downward trend ---\n                if filter_down == 0:\n                    difference = maxPrice - askPrice\n                    # If price has fallen at least Threshold% from the previous max\n                    if maxPrice > askPrice and difference >= maxPrice * threshold_down:\n                        # Confirm PDCC_Down event\n                        minPrice = askPrice\n                        PDCC = askPrice\n                        filter_down = 1  # enter down-trend mode\n                        max_OSV = 0  # reset overshoot counter\n                        filter_up = 0  # ensure up-trend logic is off\n\n                        # Record the start of the down-trend\n                        downward_events.append(\n                            (max_date, date_today, maxPrice, askPrice, \"PDCC_Down\")\n                        )\n                        PDCC_date = date_today\n                        df.loc[df[self.time_col] == date_today, \"event\"] = \"PDCC_Down\"\n                        df.loc[\n                            df[self.time_col] == date_today, \"event_type\"\n                        ] = \"Downward Start\"\n\n                        # Also mark the last up-trend\u2019s overshoot update\n                        upward_events.append(\n                            (PDCC2_date, osv_up_date_today, PDCC2, maxPrice2, \"OSV_Up\")\n                        )\n                        df.loc[\n                            df[self.time_col] == osv_up_date_today, \"event\"\n                        ] = \"OSV_Up\"\n                        df.loc[\n                            df[self.time_col] == osv_up_date_today, \"event_type\"\n                        ] = \"Upward Update\"\n\n                        # Reset up-trend trackers\n                        minPrice2 = 10_000_000\n                        maxPrice2 = 0.5\n\n                    # If price goes above the previous max, just update maxPrice\n                    if askPrice > maxPrice:\n                        maxPrice = askPrice\n                        max_date = date_today\n\n                # --- Section B: While inside a downward trend, measure overshoot ---\n                if filter_down == 1:\n                    # If price has risen above the local low, compute overshoot\n                    if minPrice < askPrice:\n                        OSV = (PDCC - askPrice) / (PDCC * threshold_down)\n                        max_OSV = max(max_OSV, OSV)\n                        # Clamp negative overshoots to zero\n                        if OSV < 0:\n                            OSV = 0\n                            # If an upward signal is already active, exit down-trend\n                            if filter_up == 1:\n                                filter_down = 0\n                                minPrice = 10_000_000\n                                maxPrice = 0.5\n\n                    # Always update the running low and overshoot marker date\n                    if minPrice >= askPrice or filter_up == 1:\n                        minPrice = askPrice\n                        osv_down_date_today = date_today\n                        OSV = (PDCC - askPrice) / (PDCC * threshold_down)\n                        max_OSV = max(max_OSV, OSV)\n\n                # --- Section C: Detect start of an upward trend ---\n                if filter_up == 0:\n                    difference2 = askPrice - minPrice2\n                    # If price has risen at least Threshold2% from the previous min\n                    if minPrice2 < askPrice and difference2 >= minPrice2 * threshold_up:\n                        # Confirm PDCC2_UP event\n                        maxPrice2 = askPrice\n                        PDCC2 = askPrice\n                        filter_up = 1  # enter up-trend mode\n                        max_OSV2 = 0  # reset overshoot counter\n\n                        # Record the start of the up-trend\n                        upward_events.append(\n                            (min_date, date_today, minPrice2, PDCC2, \"PDCC2_UP\")\n                        )\n                        PDCC2_date = date_today\n                        df.loc[df[self.time_col] == date_today, \"event\"] = \"PDCC2_UP\"\n                        df.loc[\n                            df[self.time_col] == date_today, \"event_type\"\n                        ] = \"Upward Start\"\n\n                        # Also mark the last down-trend\u2019s overshoot update\n                        downward_events.append(\n                            (PDCC_date, osv_down_date_today, PDCC, minPrice, \"OSV_Down\")\n                        )\n                        df.loc[\n                            df[self.time_col] == osv_down_date_today, \"event\"\n                        ] = \"OSV_Down\"\n                        df.loc[\n                            df[self.time_col] == osv_down_date_today, \"event_type\"\n                        ] = \"Downward Update\"\n\n                        # Reset down-trend trackers\n                        filter_down = 0\n                        minPrice = 10_000_000\n                        maxPrice = 0.5\n\n                    # If price dips below previous low, just update minPrice2\n                    if askPrice < minPrice2:\n                        minPrice2 = askPrice\n                        min_date = date_today\n\n                # --- Section D: While inside an upward trend, measure overshoot ---\n                if filter_up == 1:\n                    # If price has fallen back below the running max, compute overshoot\n                    if askPrice < maxPrice2:\n                        OSV2 = (askPrice - PDCC2) / (PDCC2 * threshold_up)\n                        max_OSV2 = max(max_OSV2, OSV2)\n                        # Clamp negative overshoots to zero\n                        if OSV2 < 0:\n                            OSV2 = 0\n                            # If a down-signal is already active, exit up-trend\n                            if filter_down == 1:\n                                filter_up = 0\n                                minPrice2 = 10_000_000\n                                maxPrice2 = 0.5\n\n                    # Always update the running high and overshoot marker date\n                    if askPrice >= maxPrice2 or filter_down == 1:\n                        maxPrice2 = askPrice\n                        osv_up_date_today = date_today\n                        OSV2 = (askPrice - PDCC2) / (PDCC2 * threshold_up)\n                        max_OSV2 = max(max_OSV2, OSV2)\n\n            # Final results: the largest overshoot seen in any down- or up-trend\n            print(\n                f\"(down={threshold_down}, up={threshold_up}) \"\n                f\"Final maximum OSV for downward trends: {max_OSV}\"\n            )\n            print(\n                f\"(down={threshold_down}, up={threshold_up}) \"\n                f\"Final maximum OSV2 for upward trends:   {max_OSV2}\"\n            )\n\n            # Store events for this threshold pair\n            key = (threshold_down, threshold_up)\n            self.events[key] = {\n                \"downward\": downward_events,\n                \"upward\": upward_events,\n                \"max_OSV\": max_OSV,\n                \"max_OSV2\": max_OSV2,\n                \"df\": df,\n            }\n\n            # Build a Plotly figure marking all PDCC and OSV events\n            fig = self._build_plot(\n                df=df,\n                downward_events=downward_events,\n                upward_events=upward_events,\n                threshold_pair=key,\n            )\n            self.figures[key] = fig\n\n            # Load summary to BigQuery\n            if self.load_to_bq:\n                self._load_dc_events(\n                    downward_events + upward_events, threshold_pair=key\n                )\n\n        def _build_plot(\n            self,\n            *,\n            df: pd.DataFrame,\n            downward_events: list[tuple],\n            upward_events: list[tuple],\n            threshold_pair: Tuple[float, float],\n        ) -> go.Figure:\n            \"\"\"\n            Build a Plotly figure marking PDCC and OSV events on prices.\n            \"\"\"\n            dates = df[self.time_col]\n            prices = df[self.price_col]\n            th_down, th_up = threshold_pair\n            fig = go.Figure()\n            # Use WebGL for faster rendering when requested\n            price_trace_cls = go.Scattergl if self.fast_plot else go.Scatter\n            fig.add_trace(\n                price_trace_cls(x=dates, y=prices, mode=\"lines\", name=\"Ask Prices\")\n            )\n\n            # Plot downward-trend starts and updates\n            downward_start_added = False\n            downward_update_added = False\n            down_iter = (\n                downward_events[-self.max_event_markers :]\n                if (self.fast_plot and self.max_event_markers)\n                else downward_events\n            )\n            for prev, today, price0, price1, event in down_iter:\n                if event == \"PDCC_Down\":\n                    fig.add_trace(\n                        go.Scatter(\n                            x=[today],\n                            y=[price1],\n                            mode=\"markers\",\n                            marker=dict(color=\"red\", symbol=\"triangle-down\"),\n                            name=\"Downward Start\",\n                            showlegend=not downward_start_added,\n                        )\n                    )\n                    downward_start_added = True\n                elif event == \"OSV_Down\":\n                    fig.add_trace(\n                        go.Scatter(\n                            x=[today],\n                            y=[price1],\n                            mode=\"markers\",\n                            marker=dict(color=\"orange\", symbol=\"triangle-down\"),\n                            name=\"Downward Update\",\n                            showlegend=not downward_update_added,\n                        )\n                    )\n                    downward_update_added = True\n\n                if pd.notna(prev) and pd.notna(today):\n                    fig.add_shape(\n                        type=\"line\",\n                        x0=prev,\n                        y0=price0,\n                        x1=today,\n                        y1=price1,\n                        line=dict(\n                            color=\"red\" if event == \"PDCC_Down\" else \"black\",\n                            width=2,\n                            dash=\"dashdot\",\n                        ),\n                        opacity=0.7,\n                    )\n\n            # Plot upward-trend starts and updates on the *same* figure\n            upward_start_added = False\n            upward_update_added = False\n            up_iter = (\n                upward_events[-self.max_event_markers :]\n                if (self.fast_plot and self.max_event_markers)\n                else upward_events\n            )\n            for prev, today, price0, price1, event in up_iter:\n                if event == \"PDCC2_UP\":\n                    fig.add_trace(\n                        go.Scatter(\n                            x=[today],\n                            y=[price1],\n                            mode=\"markers\",\n                            marker=dict(color=\"green\", symbol=\"triangle-up\"),\n                            name=\"Upward Start\",\n                            showlegend=not upward_start_added,\n                        )\n                    )\n                    upward_start_added = True\n                elif event == \"OSV_Up\":\n                    fig.add_trace(\n                        go.Scatter(\n                            x=[today],\n                            y=[price1],\n                            mode=\"markers\",\n                            marker=dict(color=\"magenta\", symbol=\"triangle-up\"),\n                            name=\"Upward Update\",\n                            showlegend=not upward_update_added,\n                        )\n                    )\n                    upward_update_added = True\n\n                if pd.notna(prev) and pd.notna(today):\n                    fig.add_shape(\n                        type=\"line\",\n                        x0=prev,\n                        y0=price0,\n                        x1=today,\n                        y1=price1,\n                        line=dict(\n                            color=\"green\" if event == \"PDCC2_UP\" else \"magenta\",\n                            width=2,\n                            dash=\"dashdot\",\n                        ),\n                        opacity=0.7,\n                    )\n\n            fig.update_layout(\n                title=f\"DC Events (Thresholds: down={th_down}, up={th_up})\",\n                xaxis_title=\"Date\",\n                yaxis_title=\"Price\",\n                legend_title=\"Events\",\n                legend=dict(itemsizing=\"constant\"),\n            )\n            return fig\n\n        def _load_dc_events(\n            self, events: list[tuple], *, threshold_pair: Tuple[float, float]\n        ) -> Optional[pd.DataFrame]:\n            \"\"\"\n            Loads DC summary events to BigQuery with experiment metadata.\n\n            - Remove the default null start/end dates.\n            - Create DC summary events DataFrame and load it to BQ.\n            \"\"\"\n            if len(events) <= 1:\n                print(\"No DC events to load\")\n                return None\n\n            # Remove the default null start/end dates\n            clean_events = [ev for ev in events if pd.notna(ev[0]) and pd.notna(ev[1])]\n            if not clean_events:\n                print(\"No DC events to load\")\n                return None\n\n            # Create DC summary events dataframe\n            df_ev = pd.DataFrame(\n                clean_events,\n                columns=[\n                    \"start_time\",\n                    \"end_time\",\n                    \"start_price\",\n                    \"end_price\",\n                    \"event_type\",\n                ],\n            )\n            df_ev[\"threshold_down\"], df_ev[\"threshold_up\"] = threshold_pair\n            df_ev[\"experiment_name\"] = self.experiment_name\n            df_ev[\"experiment_id\"] = self.experiment_id\n\n            # # Ensure pandas Timestamps become UTC tz-aware\n            # df_ev[\"start_time\"] = pd.to_datetime(df_ev[\"start_time\"], utc=True, errors=\"coerce\")\n            # df_ev[\"end_time\"] = pd.to_datetime(df_ev[\"end_time\"], utc=True, errors=\"coerce\")\n\n            # Ensure timestamps are timezone-aware but do NOT drop any rows\n            df_ev[\"start_time\"] = pd.to_datetime(\n                df_ev[\"start_time\"], utc=True, format=\"mixed\", errors=\"raise\"\n            )\n            df_ev[\"end_time\"] = pd.to_datetime(\n                df_ev[\"end_time\"], utc=True, format=\"mixed\", errors=\"raise\"\n            )\n\n            table_ref = self.client.dataset(self.dataset_id).table(self.table_id)\n            job = self.client.load_table_from_dataframe(df_ev, table_ref)\n            job.result()  # wait for load\n            print(\"Loaded DC events to BigQuery.\")\n            return df_ev\n\n        def _save_figure_to_gcs(\n            self, fig: go.Figure, threshold_pair: Tuple[float, float]\n        ) -> str:\n            \"\"\"Save a Plotly figure to GCS as HTML and return the GCS path.\"\"\"\n            if not self.figures_output_path:\n                return \"\"\n\n            # Create filename based on threshold\n            down_th, up_th = threshold_pair\n            filename = f\"dc_events_down_{down_th}_up_{up_th}.html\"\n\n            # Parse GCS path\n            if self.figures_output_path.startswith(\"gs://\"):\n                bucket_name = self.figures_output_path[5:].split(\"/\")[0]\n                prefix = \"/\".join(self.figures_output_path[5:].split(\"/\")[1:])\n                if prefix and not prefix.endswith(\"/\"):\n                    prefix += \"/\"\n                gcs_path = f\"{prefix}{filename}\"\n            else:\n                # Assume it's a local path for testing\n                gcs_path = os.path.join(self.figures_output_path, filename)\n                os.makedirs(os.path.dirname(gcs_path), exist_ok=True)\n                fig.write_html(gcs_path)\n                return gcs_path\n\n            # Save to GCS\n            client = storage.Client(project=self.project_id)\n            bucket = client.bucket(bucket_name)\n            blob = bucket.blob(gcs_path)\n\n            # Convert figure to HTML string\n            html_content = fig.to_html(include_plotlyjs=\"cdn\")\n            blob.upload_from_string(html_content, content_type=\"text/html\")\n\n            full_gcs_path = f\"gs://{bucket_name}/{gcs_path}\"\n            print(f\"Saved figure to: {full_gcs_path}\")\n            return full_gcs_path\n\n        def _save_all_figures(self) -> None:\n            \"\"\"Save all generated figures to GCS.\"\"\"\n            if not self.figures_output_path:\n                return\n\n            saved_paths = []\n            for threshold_pair, fig in self.figures.items():\n                gcs_path = self._save_figure_to_gcs(fig, threshold_pair)\n                if gcs_path:\n                    saved_paths.append(gcs_path)\n\n            # Save metadata about saved figures\n            if saved_paths:\n                metadata = {\n                    \"experiment_name\": self.experiment_name,\n                    \"experiment_id\": self.experiment_id,\n                    \"saved_figures\": saved_paths,\n                    \"timestamp\": datetime.now(timezone.utc).isoformat(),\n                }\n\n                # Save metadata to GCS\n                if self.figures_output_path.startswith(\"gs://\"):\n                    bucket_name = self.figures_output_path[5:].split(\"/\")[0]\n                    prefix = \"/\".join(self.figures_output_path[5:].split(\"/\")[1:])\n                    if prefix and not prefix.endswith(\"/\"):\n                        prefix += \"/\"\n                    metadata_path = f\"{prefix}figures_metadata.json\"\n\n                    client = storage.Client(project=self.project_id)\n                    bucket = client.bucket(bucket_name)\n                    blob = bucket.blob(metadata_path)\n                    blob.upload_from_string(\n                        json.dumps(metadata, indent=2), content_type=\"application/json\"\n                    )\n                    print(f\"Saved metadata to: gs://{bucket_name}/{metadata_path}\")\n\n    # Start here\n    # Create df from input dataset\n    df = pd.read_csv(df.path)\n\n    # Define threshold string e.g. 0p01\n    thr_str = str(thresholds).replace('.', 'p')\n    # Define experiment name e.g. exp_dc_detection_0p01_20251010173542\n    experiment_name = f\"exp_dc_detection_{thr_str}_{datetime.now().strftime('%Y%m%d%H%M%S')}\"\n    # Define BQ table id e.g. dc_events_threshold_0p01\n    table_id = f\"dc_events_threshold_{thr_str}\"\n\n    # Create a DirectionalChange object\n    dc = DirectionalChange(\n        df=df,  # must have columns PRICE and load_time_toronto\n        thresholds=[thresholds],  # used for both down & up\n        experiment_name=experiment_name,\n        price_col=price_col,\n        time_col=time_col,\n        project_id=project_id,\n        dataset_id=dataset_id,\n        table_id=table_id,  # optional, defaults to \"dc_events\"\n        load_to_bq=load_to_bq,  # set False to skip BQ loads\n        fast_plot=fast_plot,\n        max_event_markers=max_event_markers,\n        figures_output_path=figures_output.path,\n    )\n    dc.run_all()\n\n    # Save events data to output\n    events_data = []\n    for threshold_pair in dc.threshold_sets:\n        events = dc.get_events(threshold_pair)\n        if events:\n            # Convert events to DataFrame format for output\n            down_events = events[\"downward\"]\n            up_events = events[\"upward\"]\n\n            for event in down_events + up_events:\n                # Filter out events with missing start/end times.\n                # These are typically the first PDCC events where start time is NaN.\n                if len(event) >= 5 and pd.notna(event[0]) and pd.notna(event[1]):\n                    events_data.append(\n                        {\n                            \"start_time\": event[0],\n                            \"end_time\": event[1],\n                            \"start_price\": event[2],\n                            \"end_price\": event[3],\n                            \"event_type\": event[4],\n                            \"threshold_down\": threshold_pair[0],\n                            \"threshold_up\": threshold_pair[1],\n                            \"experiment_name\": experiment_name,\n                            \"experiment_id\": dc.experiment_id,\n                        }\n                    )\n    # Save events data to output\n    if events_data:\n        events_df = pd.DataFrame(events_data)\n        events_df.to_csv(directional_change_events.path, index=False)\n        print(f\"Saved {len(events_data)} events to {directional_change_events.path}\")\n\n    # Figures_output.path is a directory like: /gcs/.../figures_output\n    out_dir = Path(figures_output.path)\n    out_dir.mkdir(parents=True, exist_ok=True)\n\n    # File name inside the directory\n    out_file = out_dir / \"figures_metadata.json\"\n\n    # Save figures metadata to output\n    figures_metadata = {\n        \"experiment_name\": experiment_name,\n        \"experiment_id\": dc.experiment_id,\n        \"saved_figures\": list(dc.figures.keys()),\n        \"timestamp\": datetime.now(timezone.utc).isoformat(),\n    }\n\n    with out_file.open(\"w\", encoding=\"utf-8\") as f:\n        json.dump(figures_metadata, f, indent=2, ensure_ascii=False)\n\n    print(f\"Saved figures metadata to {out_file}\")\n    print(\n        f\"Generated {len(dc.figures)} figures for thresholds: {list(dc.figures.keys())}\"\n    )\n\n"
          ],
          "image": "python:3.11"
        }
      },
      "exec-extract-bq-to-dataset": {
        "container": {
          "args": [
            "--executor_input",
            "{{$}}",
            "--function_to_execute",
            "extract_bq_to_dataset"
          ],
          "command": [
            "sh",
            "-c",
            "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location 'google-cloud-bigquery==2.30.0'  &&  python3 -m pip install --quiet --no-warn-script-location 'kfp==2.15.2' '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"' && \"$0\" \"$@\"\n",
            "sh",
            "-ec",
            "program_path=$(mktemp -d)\n\nprintf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\n_KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n",
            "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import *\n\ndef extract_bq_to_dataset(\n    bq_client_project_id: str,\n    source_project_id: str,\n    dataset_id: str,\n    table_name: str,\n    dataset: Output[Dataset],\n    destination_gcs_uri: str = None,\n    dataset_location: str = \"US\",\n    extract_job_config: dict = None,\n    skip_if_exists: bool = True,\n):\n    \"\"\"\n    Extract BQ table in GCS.\n    Args:\n        bq_client_project_id (str): project id that will be used by the bq client\n        source_project_id (str): project id from where BQ table will be extracted\n        dataset_id (str): dataset id from where BQ table will be extracted\n        table_name (str): table name (without project id and dataset id)\n        dataset (Output[Dataset]): output dataset artifact generated by the operation,\n            this parameter will be passed automatically by the orchestrator\n        dataset_location (str): bq dataset location. Defaults to \"EU\".\n        extract_job_config (dict): dict containing optional parameters\n            required by the bq extract operation. Defaults to None.\n            See available parameters here\n            https://googleapis.dev/python/bigquery/latest/generated/google.cloud.bigquery.job.ExtractJobConfig.html # noqa\n        destination_gcs_uri (str): GCS URI to use for saving query results (optional).\n\n    Returns:\n        Outputs (NamedTuple (str, list)): Output dataset directory and its  GCS uri.\n    \"\"\"\n\n    import logging\n    from pathlib import Path\n    from google.cloud.exceptions import GoogleCloudError\n    from google.cloud import bigquery\n\n    # set uri of output dataset if destination_gcs_uri is provided\n    if destination_gcs_uri:\n        dataset.uri = destination_gcs_uri\n\n    logging.info(f\"Checking if destination exists: {dataset.path}\")\n    if Path(dataset.path).exists() and skip_if_exists:\n        logging.info(\"Destination already exists, skipping table extraction!\")\n        return\n\n    full_table_id = f\"{source_project_id}.{dataset_id}.{table_name}\"\n    table = bigquery.table.Table(table_ref=full_table_id)\n\n    if extract_job_config is None:\n        extract_job_config = {}\n    job_config = bigquery.job.ExtractJobConfig(**extract_job_config)\n\n    logging.info(f\"Extract table {table} to {dataset.uri}\")\n    client = bigquery.client.Client(\n        project=bq_client_project_id, location=dataset_location\n    )\n    extract_job = client.extract_table(\n        table,\n        dataset.uri,\n        job_config=job_config,\n    )\n\n    try:\n        result = extract_job.result()\n        logging.info(\"Table extracted, result: {}\".format(result))\n    except GoogleCloudError as e:\n        logging.error(e)\n        logging.error(extract_job.error_result)\n        logging.error(extract_job.errors)\n        raise e\n\n"
          ],
          "image": "python:3.10"
        }
      },
      "exec-feature-engineering": {
        "container": {
          "args": [
            "--executor_input",
            "{{$}}",
            "--function_to_execute",
            "feature_engineering"
          ],
          "command": [
            "sh",
            "-c",
            "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location 'pandas' 'numpy' 'pyarrow' 'google-cloud-bigquery'  &&  python3 -m pip install --quiet --no-warn-script-location 'kfp==2.15.2' '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"' && \"$0\" \"$@\"\n",
            "sh",
            "-ec",
            "program_path=$(mktemp -d)\n\nprintf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\n_KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n",
            "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import *\n\ndef feature_engineering(\n    dc_summary: Input[Dataset],\n    raw_ticks: Input[Dataset],\n    threshold: float,\n    dc_ticks: Output[Dataset],\n    metrics: Output[Metrics],\n    columns_json: str = \"\",\n):\n    \"\"\"Component wrapper: calls FeatureEngineer().run()\"\"\"\n    import os\n    import pandas as pd\n    import numpy as np\n    import json\n    import logging\n    from google.cloud import bigquery\n\n\n    class FeatureEngineer:\n        \"\"\"\n        Stateful DC Feature Engineering for full-tick coverage.\n\n        This class augments every tick with Directional Change (DC) regime features.\n        It tracks the last known PDCC event (upward or downward) and computes\n        instantaneous overshoot magnitudes for all ticks within that regime.\n\n        It is designed to be fully threshold-isolated: each instance processes\n        a single threshold level independently.\n        \"\"\"\n        def __init__(self, ticks_path, dc_path, threshold, columns_json, metrics, output_dataset):\n            \"\"\"\n            Parameters\n            ----------\n            ticks_path : str\n                Path to CSV of raw tick data.\n            dc_path : str\n                Path to CSV of DC summary data (output from DC detection component).\n            threshold : float\n                The DC threshold currently being processed (up/down symmetry assumed).\n            columns_json : str\n                Optional JSON mapping of column names to adapt to other schemas.\n            metrics : kfp.v2.dsl.Metrics or None\n                Vertex AI metrics object for experiment logging.\n            \"\"\"\n            self.ticks_path = ticks_path\n            self.dc_path = dc_path\n            self.threshold = float(threshold)\n            self.metrics = metrics\n            self.output_dataset = output_dataset\n            # Schema abstraction layer to make FE logic generic\n            default_cols = {\n                \"ticks_time\": \"trade_ts\",\n                \"ticks_price\": \"PRICE\",\n                \"ticks_event\": \"event\",\n\n                \"dc_start_time\": \"start_time\",\n                \"dc_end_time\": \"end_time\",\n                \"dc_start_price\": \"start_price\",\n                \"dc_end_price\": \"end_price\",\n                \"dc_threshold\": \"threshold_up\",\n                \"dc_event_type\": \"event_type\",\n\n                # engineered outputs\n                \"col_pdcc_up\": \"PDCC2_UP\",\n                \"col_pdcc_down\": \"PDCC_Down\",\n                \"col_osv_up\": \"OSV_Up\",\n                \"col_osv_down\": \"OSV_Down\",\n                # \"col_pdcc_flag\": \"PDCC_event_ind\",\n                # \"col_osv_up_flag\": \"OSV_Up_flag\",\n                # \"col_osv_down_flag\": \"OSV_Down_flag\",\n            }\n            overrides = json.loads(columns_json) if columns_json else {}\n            self.C = {**default_cols, **overrides}\n\n        # ---------- Helpers ----------\n        @staticmethod\n        def _to_utc_naive(series: pd.Series) -> pd.Series:\n            \"\"\"\n            Convert timestamps to UTC and strip timezone info.\n            Handles strings with trailing ' UTC' or mixed formats robustly.\n            Raises on invalid datetime formats to enforce data integrity.\n            \"\"\"\n            try:\n                # Normalize potential ' UTC' suffixes and mixed formats\n                series = series.astype(str).str.replace(\" UTC\", \"\", regex=False)\n                s = pd.to_datetime(series, errors=\"raise\", utc=True, format='mixed')\n                return s.dt.tz_convert(\"UTC\").dt.tz_localize(None)\n            except Exception as e:\n                raise ValueError(f\"[FeatureEngineer] Timestamp conversion failed: {e}\")\n\n        def _load_inputs(self):\n            ticks_df = pd.read_csv(self.ticks_path)\n            dc_df = pd.read_csv(self.dc_path)\n            logging.info(f\"[FE] Loaded ticks={len(ticks_df)}, dc_summary={len(dc_df)}\")\n            return ticks_df, dc_df\n\n        def _prepare_dc_anchors(self, dc_df):\n            \"\"\"\n            Extract PDCC events (PDCC2_UP, PDCC_Down) as regime change anchors.\n\n            Purpose:\n            --------\n            We need to know timestamps where the market switched regimes (DC confirmation events)\n            (e.g., from downward \u2192 upward, or vice versa). Those events define the start\n            of a new regime whose overshoot and features we'll track until the next PDCC event.\n\n            This function extracts and cleans that \"anchor timeline\".\n\n            Returns\n            -------\n            anchors : DataFrame\n                Columns include:\n                - C[\"dc_end_time\"]: timestamp of event\n                - C[\"dc_end_price\"]: price at event\n                - C[\"dc_event_type\"]: event label (\"PDCC2_UP\" or \"PDCC_Down\")\n                - regime: derived label (\"up\" or \"down\")\n            \"\"\"\n            C = self.C\n            # Work on a copy so we never mutate the input dataframe\n            dc_f = dc_df.copy()\n            # Filter DC summary rows to only the current threshold\n            # The DC summary table MAY contain multiple thresholds (0.001, 0.005, etc.).\n            # We only want anchors that correspond to *this* component's active threshold.\n            if C[\"dc_threshold\"] in dc_f.columns:\n                dc_f = dc_f[np.isclose(\n                    dc_f[C[\"dc_threshold\"]].astype(float), # cast to float\n                    self.threshold,  # this is the component's active threshold\n                    rtol=0, atol=1e-12 # exact match within tiny tolerance\n                )]\n            # Keep only PDCC events (regime change anchors)\n            # We don't want OSV (overshoot) rows here; those don\u2019t start a new regime.\n            # PDCC2_UP \u2192 marks the beginning of an upward regime\n            # PDCC_Down \u2192 marks the beginning of a downward regime\n            want = dc_f[C[\"dc_event_type\"]].isin([\"PDCC2_UP\", \"PDCC_Down\"])\n            # --- Select and rename relevant columns ---\n            # Rename them to uniform names so downstream logic doesn\u2019t depend on dataset schema.\n            # 'end_time' becomes 'event_time' \u2192 the timestamp of the PDCC event\n            # 'end_price' becomes 'anchor_price' \u2192 the price level at that event\n            anchors = dc_f.loc[want, [\n                C[\"dc_end_time\"], C[\"dc_end_price\"], C[\"dc_event_type\"]\n            ]]\n\n            # Ensure end_time is parsed and timezone-normalized\n            anchors[C[\"dc_end_time\"]] = self._to_utc_naive(anchors[C[\"dc_end_time\"]])\n\n            # Sort anchors chronologically by end_time (required for merge_asof)\n            anchors = (\n                anchors\n                .sort_values(C[\"dc_end_time\"])\n                .reset_index(drop=True)\n            )\n\n            # Label regime direction\n            anchors[\"regime\"] = np.where(\n                anchors[C[\"dc_event_type\"]].eq(\"PDCC2_UP\"), \"up\", \"down\"\n            )\n\n            # Warn if there are duplicates (i.e. multiple PDCC events at the same time)\n            num_dupes = anchors.duplicated(subset=[C[\"dc_end_time\"]]).sum()\n            if num_dupes > 0:\n                logging.warning(f\"[FE] Found {num_dupes} anchors sharing the same end_time.\")\n\n            return anchors\n\n        def _merge_state(self, ticks_df, anchors):\n            \"\"\"Attach latest PDCC regime to each tick.\n\n            Align each tick with the latest PDCC anchor (regime state).\n\n            - For every tick timestamp, find the most recent PDCC event that\n            occurred before or at that tick.\n            - This assigns a regime ('up' or 'down') and anchor price per tick.\n\n            Returns\n            -------\n            merged : DataFrame\n                Columns include:\n                - C[\"ticks_time\"]: timestamp of tick\n                - C[\"anchor_price\"]: price at anchor event\n                - regime: derived label (\"up\" or \"down\")\n            \"\"\"\n            C = self.C\n\n            # Normalize tick timestamps and sort for chronological merge\n            ticks_df[C[\"ticks_time\"]] = self._to_utc_naive(ticks_df[C[\"ticks_time\"]])\n            ticks_df = ticks_df.sort_values(C[\"ticks_time\"]).reset_index(drop=True)\n\n            # Handle edge case: no anchors for this threshold\n            if anchors.empty:\n                raise ValueError(\n                    f\"[FE] No PDCC anchors found for threshold={self.threshold}. \"\n                    \"Cannot compute directional-change features without at least one PDCC event.\"\n                )\n\n            # Merge each tick with the last PDCC anchor before it (backward match)\n\n            # merge_asof performs a time-aware \u201clast known value\u201d join\n            # it finds, for each left-side timestamp, the latest right-side row whose \n            # timestamp is less than or equal to the left one.\n\n            # i.e For each tick, what was the most recent PDCC event \n            # that happened before that tick?\n            merged = pd.merge_asof(\n                left=ticks_df,\n                right=anchors[[C[\"dc_end_time\"], C[\"dc_end_price\"], \"regime\"]],\n                left_on=C[\"ticks_time\"], # trade_ts\n                right_on=C[\"dc_end_time\"], # PDCC event_time\n                direction=\"backward\", # Match latest event <= tick time\n            )\n            return merged\n\n        @staticmethod\n        def _compute_features(df, C):\n            \"\"\"\n            Generate continuous Directional Change (DC) features for every tick.\n\n            Each tick inherits its last known PDCC regime (up/down/none) and computes\n            instantaneous overshoot magnitudes relative to the corresponding regime's\n            anchor price.\n\n            - PDCC2_UP / PDCC_Down fire only on the actual PDCC confirmation tick.\n            - regime_up / regime_down added as persistent regime state trackers.\n\n            For each tick:\n            - Determine current PDCC regime (\"up\", \"down\", or \"none\")\n            - Compute % overshoot from anchor price based on regime:\n                    up   \u2192 (PRICE - anchor_price) / anchor_price * 100\n                    down \u2192 (anchor_price - PRICE) / anchor_price * 100\n            - Add binary regime indicators and overshoot flags\n\n            Returns\n            -------\n            df : pandas.DataFrame\n                Input ticks annotated with the following engineered columns:\n\n                - C[\"col_pdcc_flag\"]: 1 if tick belongs to any PDCC regime, else 0\n                - C[\"col_pdcc_up\"]: 1 if in upward regime, else 0\n                - C[\"col_pdcc_down\"]: 1 if in downward regime, else 0\n                - C[\"col_osv_up\"]: upward overshoot magnitude (%)\n                - C[\"col_osv_down\"]: downward overshoot magnitude (%)\n                - C[\"col_osv_up_flag\"]: 1 if overshoot magnitude > 0 in upward regime\n                - C[\"col_osv_down_flag\"]: 1 if overshoot magnitude > 0 in downward regime\n\n            Notes\n            -----\n            - Overshoots are set to 0 outside active regimes.\n            - NaN or infinite values are replaced with 0 for stability.\n            - This method assumes df already includes columns:\n                PRICE, anchor_price (C[\"dc_end_price\"]), and regime.\n            \"\"\"\n\n            # Normalize regime column\n            reg = df[\"regime\"].fillna(\"none\")\n\n            # Continuous regime indicators\n            regime_up = (reg == \"up\").astype(int)\n            regime_down = (reg == \"down\").astype(int)\n\n            # PDCC event confirmation flags \n            # Mark only the tick that aligns exactly with the PDCC event itself\n            tick_time = df[C[\"ticks_time\"]]\n            anchor_time = df[C[\"dc_end_time\"]]\n            pdcc2_up = ((reg == \"up\") & (tick_time == anchor_time)).astype(int)\n            pdcc_down = ((reg == \"down\") & (tick_time == anchor_time)).astype(int)\n\n            # Signed OSV magnitudes\n            price = df[C[\"ticks_price\"]].astype(float)\n            # Anchors will be NaN for ticks before the first PDCC event.\n            # Guard division to avoid propagating NaNs or Infs in those rows.\n            anchor = df[C[\"dc_end_price\"]].astype(float)\n            safe_anchor = np.where(np.isnan(anchor) | (anchor == 0.0), np.nan, anchor)\n\n\n            # Continuous signed OSV magnitudes (actual magnitudes, even during retracements)\n            osv_up = np.where(\n                regime_up.astype(bool),\n                (price - safe_anchor) / safe_anchor * 100.0,\n                0.0,   # outside regime\n            )\n\n            osv_down = np.where(\n                regime_down.astype(bool),\n                (safe_anchor - price) / safe_anchor * 100.0,\n                0.0,   # outside regime\n            )\n\n            # Clamped OSV magnitudes (floor at 0)\n            osv_up_v2 = np.where(\n                regime_up.astype(bool),\n                np.maximum(0.0, (price - safe_anchor) / safe_anchor * 100.0),\n                0.0,\n            )\n            osv_down_v2 = np.where(\n                regime_down.astype(bool),\n                np.maximum(0.0, (safe_anchor - price) / safe_anchor * 100.0),\n                0.0,\n            )\n\n            # Replace NaNs from safe_anchor with 0 only outside regime; inside regime we want continuity.\n            # (If anchor was NaN inside regime, that indicates an upstream join issue and should be rare.)\n            # Pragmatically fill any residual NaNs with 0:\n            osv_up, osv_down, osv_up_v2, osv_down_v2 = [\n                np.nan_to_num(x, nan=0.0, posinf=0.0, neginf=0.0)\n                for x in (osv_up, osv_down, osv_up_v2, osv_down_v2)\n            ]\n\n\n            # Detect NaN or Inf before cleaning\n            nan_up, nan_dn = np.isnan(osv_up).sum(), np.isnan(osv_down).sum()\n            inf_up, inf_dn = np.isinf(osv_up).sum(), np.isinf(osv_down).sum()\n            if any([nan_up, nan_dn, inf_up, inf_dn]):\n                logging.warning(f\"[FE] NaNs: up={nan_up}, down={nan_dn}; infs: up={inf_up}, down={inf_dn}\")\n\n            # Write engineered columns\n            df[C[\"col_pdcc_up\"]] = pdcc2_up\n            df[C[\"col_pdcc_down\"]] = pdcc_down\n\n            # Persistent regime trackers\n            df[\"regime_up\"] = regime_up\n            df[\"regime_down\"] = regime_down\n\n            # Continuous signed OSV magnitudes\n            df[C[\"col_osv_up\"]] = osv_up\n            df[C[\"col_osv_down\"]] = osv_down\n\n            # Clamped OSV magnitudes\n            df[f\"{C['col_osv_up']}_clamped\"] = osv_up_v2\n            df[f\"{C['col_osv_down']}_clamped\"] = osv_down_v2\n\n            return df\n\n        @staticmethod\n        def _compute_regime_stats(df, C):\n            \"\"\"\n            Compute per-regime duration statistics (in ticks).\n\n            For each regime between consecutive PDCC events,\n            count how many ticks belong to that regime and report\n            mean/std per regime type (up vs down).\n            \"\"\"\n            # Sort chronologically to ensure consistent order\n            df = df.sort_values(C[\"ticks_time\"]).reset_index(drop=True)\n\n            # Mark PDCC boundaries and regime type at each start\n            pdcc_idx = df.index[(df[C[\"col_pdcc_up\"]] == 1) | (df[C[\"col_pdcc_down\"]] == 1)]\n            regime_type = np.where(\n                df.loc[pdcc_idx, C[\"col_pdcc_up\"]] == 1, \"up\", \"down\"\n            )\n\n            # If no PDCC events, return zeros\n            if len(pdcc_idx) == 0:\n                return {\n                    \"avg_ticks_up_regime\": 0.0,\n                    \"avg_ticks_down_regime\": 0.0,\n                    \"std_ticks_up_regime\": 0.0,\n                    \"std_ticks_down_regime\": 0.0,\n                    \"num_up_regimes\": 0,\n                    \"num_down_regimes\": 0,\n                }\n\n            # Append the last index to close final segment\n            pdcc_idx = list(pdcc_idx) + [len(df)]\n            lengths = np.diff(pdcc_idx)\n\n            # Match each regime length with its regime type\n            regimes = pd.DataFrame({\n                \"type\": regime_type,\n                \"length\": lengths,\n            })\n\n            # Compute per-type stats\n            stats = {}\n            for t in [\"up\", \"down\"]:\n                lengths_t = regimes.loc[regimes[\"type\"] == t, \"length\"]\n                stats[f\"avg_ticks_{t}_regime\"] = float(lengths_t.mean()) if len(lengths_t) else 0.0\n                stats[f\"std_ticks_{t}_regime\"] = float(lengths_t.std()) if len(lengths_t) else 0.0\n                stats[f\"num_{t}_regimes\"] = int(len(lengths_t))\n\n            return stats\n\n        def _collect_metrics(self, ticks_df, dc_df, anchors, engineered):\n            \"\"\"Collect record counts, coverage, and NaN sanity checks.\"\"\"\n            C = self.C\n            fe_metrics = {\n                \"num_ticks_input\": len(ticks_df),\n                \"num_dc_events_input\": len(dc_df),\n                \"num_pdcc_anchors\": len(anchors),\n                \"num_ticks_output\": len(engineered),\n                \"tick_count_match\": int(len(engineered) == len(ticks_df)),\n            }\n\n            # Count regime coverage\n            fe_metrics[\"regime_up_ticks\"] = int(engineered[\"regime_up\"].sum())\n            fe_metrics[\"regime_down_ticks\"] = int(engineered[\"regime_down\"].sum())\n\n            # Compute regime duration statistics\n            regime_stats = self._compute_regime_stats(engineered, C)\n            fe_metrics.update(regime_stats)\n\n            # Check for NaNs in engineered DC columns\n            engineered_cols = [\n                C[\"col_pdcc_up\"], C[\"col_pdcc_down\"],\n                C[\"col_osv_up\"], C[\"col_osv_down\"],\n                f\"{C['col_osv_up']}_clamped\", f\"{C['col_osv_down']}_clamped\",\n            ]\n            nan_stats = engineered[engineered_cols].isna().sum().to_dict()\n            for k, v in nan_stats.items():\n                fe_metrics[f\"nan_count_{k}\"] = int(v)\n\n            # Log none regime count\n            none_count = int(engineered[\"regime\"].isna().sum())\n            fe_metrics[\"none_regime_count\"] = none_count\n\n            # Log metrics to Cloud Logging\n            logging.info(f\"[FE metrics] {fe_metrics}\")\n\n            # Log metrics to Vertex AI metrics\n            if self.metrics is not None:\n                for k, v in fe_metrics.items():\n                    if isinstance(v, (int, float)):\n                        self.metrics.log_metric(k, float(v))\n            return\n\n        def _load_to_bq(\n            self,\n            df_ticks: pd.DataFrame,\n            project_id: str,\n            dataset_id: str,\n            table_id: str,\n        ):\n            \"\"\"\n            Load feature-engineered DC tick dataset to BigQuery with metadata.\n\n            This writes every augmented tick (PRICE, PDCC flags, OSV magnitudes, etc.)\n            to BigQuery under a given experiment and threshold context.\n\n            Parameters\n            ----------\n            df_ticks : pd.DataFrame\n                Full tick-level feature-engineered dataset.\n            threshold : float\n                The DC threshold value used for this processing run.\n            project_id : str\n                GCP project ID where the table resides.\n            dataset_id : str\n                BigQuery dataset ID (e.g. 'coindesk').\n            table_id : str\n                BigQuery table ID (e.g. 'feature_engineered_ticks').\n\n            Returns\n            -------\n            df_ticks : pd.DataFrame\n                The same DataFrame, returned after successful load.\n            \"\"\"\n\n            if df_ticks.empty:\n                logging.warning(\"[FE->BQ] No rows to load (empty DataFrame).\")\n                return None\n\n            # Ensure timestamps are parsed and UTC-normalized\n            C = self.C\n            try:\n                df_ticks[C[\"ticks_time\"]] = pd.to_datetime(\n                    df_ticks[C[\"ticks_time\"]], utc=True, format=\"mixed\", errors=\"raise\"\n                )\n            except Exception as e:\n                raise ValueError(f\"[FE->BQ] Timestamp normalization failed: {e}\")    \n\n            # Initialize BigQuery client\n            client = bigquery.Client(project=project_id)\n            table_ref = client.dataset(dataset_id).table(table_id)\n\n            # Define BigQuery job config\n            job_config = bigquery.LoadJobConfig(\n                write_disposition=\"WRITE_TRUNCATE\",\n                # source_format=bigquery.SourceFormat.PARQUET,\n                time_partitioning=bigquery.TimePartitioning(field=self.C[\"ticks_time\"]),\n                # clustering_fields=[\n                #     C[\"ticks_price\"],\n                #     C[\"col_pdcc_up\"], C[\"col_pdcc_down\"],\n                #     C[\"col_osv_up\"], f\"{C['col_osv_up']}_clamped\",\n                #     C[\"col_osv_down\"], f\"{C['col_osv_down']}_clamped\",\n                # ],\n            )\n\n            # Load DataFrame to BigQuery\n            try:\n                job = client.load_table_from_dataframe(df_ticks, table_ref, job_config=job_config)\n                job.result()  # Wait for the job to finish\n                logging.info(f\"[FE->BQ] Loaded {len(df_ticks)} ticks into {dataset_id}.{table_id}\")\n            except Exception as e:\n                raise RuntimeError(f\"[FE->BQ] Failed to load feature-engineered ticks: {e}\")\n\n            return df_ticks\n\n        def run(self):\n            \"\"\"\n            Run the complete feature engineering workflow:\n                1. Load data\n                2. Prepare PDCC anchors\n                3. Merge anchors with ticks\n                4. Compute DC features\n                5. Collect metrics\n                6. Save engineered output\n            \"\"\"\n            ticks_df, dc_df = self._load_inputs()\n            anchors = self._prepare_dc_anchors(dc_df)\n            merged = self._merge_state(ticks_df, anchors)\n\n            engineered = self._compute_features(merged, self.C)\n            self._collect_metrics(ticks_df, dc_df, anchors, engineered)\n\n            thr_str = str(self.threshold).replace('.', 'p')\n\n            os.makedirs(self.output_dataset.path, exist_ok=True)\n            out_file = os.path.join(self.output_dataset.path, f\"feat_eng_{thr_str}.csv\")\n            engineered.to_csv(out_file, index=False, encoding=\"utf-8\")\n            logging.info(f\"[FE] Saved {len(engineered)} rows to {out_file}\")\n\n\n            self._load_to_bq(\n                df_ticks=engineered,\n                project_id=\"derivatives-417104\",\n                dataset_id=\"coindesk\",\n                table_id=f\"feat_eng_ticks_{thr_str}\",\n            )\n            logging.info(f\"[FE] Loaded {len(engineered)} ticks to BigQuery\")\n\n            return engineered\n\n    fe = FeatureEngineer(\n        ticks_path=raw_ticks.path,\n        dc_path=dc_summary.path,\n        threshold=threshold,\n        columns_json=columns_json,\n        metrics=metrics,\n        output_dataset=dc_ticks,\n    )\n    fe.run()\n\n"
          ],
          "image": "python:3.11"
        }
      },
      "exec-feature-engineering-2": {
        "container": {
          "args": [
            "--executor_input",
            "{{$}}",
            "--function_to_execute",
            "feature_engineering"
          ],
          "command": [
            "sh",
            "-c",
            "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location 'pandas' 'numpy' 'pyarrow' 'google-cloud-bigquery'  &&  python3 -m pip install --quiet --no-warn-script-location 'kfp==2.15.2' '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"' && \"$0\" \"$@\"\n",
            "sh",
            "-ec",
            "program_path=$(mktemp -d)\n\nprintf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\n_KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n",
            "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import *\n\ndef feature_engineering(\n    dc_summary: Input[Dataset],\n    raw_ticks: Input[Dataset],\n    threshold: float,\n    dc_ticks: Output[Dataset],\n    metrics: Output[Metrics],\n    columns_json: str = \"\",\n):\n    \"\"\"Component wrapper: calls FeatureEngineer().run()\"\"\"\n    import os\n    import pandas as pd\n    import numpy as np\n    import json\n    import logging\n    from google.cloud import bigquery\n\n\n    class FeatureEngineer:\n        \"\"\"\n        Stateful DC Feature Engineering for full-tick coverage.\n\n        This class augments every tick with Directional Change (DC) regime features.\n        It tracks the last known PDCC event (upward or downward) and computes\n        instantaneous overshoot magnitudes for all ticks within that regime.\n\n        It is designed to be fully threshold-isolated: each instance processes\n        a single threshold level independently.\n        \"\"\"\n        def __init__(self, ticks_path, dc_path, threshold, columns_json, metrics, output_dataset):\n            \"\"\"\n            Parameters\n            ----------\n            ticks_path : str\n                Path to CSV of raw tick data.\n            dc_path : str\n                Path to CSV of DC summary data (output from DC detection component).\n            threshold : float\n                The DC threshold currently being processed (up/down symmetry assumed).\n            columns_json : str\n                Optional JSON mapping of column names to adapt to other schemas.\n            metrics : kfp.v2.dsl.Metrics or None\n                Vertex AI metrics object for experiment logging.\n            \"\"\"\n            self.ticks_path = ticks_path\n            self.dc_path = dc_path\n            self.threshold = float(threshold)\n            self.metrics = metrics\n            self.output_dataset = output_dataset\n            # Schema abstraction layer to make FE logic generic\n            default_cols = {\n                \"ticks_time\": \"trade_ts\",\n                \"ticks_price\": \"PRICE\",\n                \"ticks_event\": \"event\",\n\n                \"dc_start_time\": \"start_time\",\n                \"dc_end_time\": \"end_time\",\n                \"dc_start_price\": \"start_price\",\n                \"dc_end_price\": \"end_price\",\n                \"dc_threshold\": \"threshold_up\",\n                \"dc_event_type\": \"event_type\",\n\n                # engineered outputs\n                \"col_pdcc_up\": \"PDCC2_UP\",\n                \"col_pdcc_down\": \"PDCC_Down\",\n                \"col_osv_up\": \"OSV_Up\",\n                \"col_osv_down\": \"OSV_Down\",\n                # \"col_pdcc_flag\": \"PDCC_event_ind\",\n                # \"col_osv_up_flag\": \"OSV_Up_flag\",\n                # \"col_osv_down_flag\": \"OSV_Down_flag\",\n            }\n            overrides = json.loads(columns_json) if columns_json else {}\n            self.C = {**default_cols, **overrides}\n\n        # ---------- Helpers ----------\n        @staticmethod\n        def _to_utc_naive(series: pd.Series) -> pd.Series:\n            \"\"\"\n            Convert timestamps to UTC and strip timezone info.\n            Handles strings with trailing ' UTC' or mixed formats robustly.\n            Raises on invalid datetime formats to enforce data integrity.\n            \"\"\"\n            try:\n                # Normalize potential ' UTC' suffixes and mixed formats\n                series = series.astype(str).str.replace(\" UTC\", \"\", regex=False)\n                s = pd.to_datetime(series, errors=\"raise\", utc=True, format='mixed')\n                return s.dt.tz_convert(\"UTC\").dt.tz_localize(None)\n            except Exception as e:\n                raise ValueError(f\"[FeatureEngineer] Timestamp conversion failed: {e}\")\n\n        def _load_inputs(self):\n            ticks_df = pd.read_csv(self.ticks_path)\n            dc_df = pd.read_csv(self.dc_path)\n            logging.info(f\"[FE] Loaded ticks={len(ticks_df)}, dc_summary={len(dc_df)}\")\n            return ticks_df, dc_df\n\n        def _prepare_dc_anchors(self, dc_df):\n            \"\"\"\n            Extract PDCC events (PDCC2_UP, PDCC_Down) as regime change anchors.\n\n            Purpose:\n            --------\n            We need to know timestamps where the market switched regimes (DC confirmation events)\n            (e.g., from downward \u2192 upward, or vice versa). Those events define the start\n            of a new regime whose overshoot and features we'll track until the next PDCC event.\n\n            This function extracts and cleans that \"anchor timeline\".\n\n            Returns\n            -------\n            anchors : DataFrame\n                Columns include:\n                - C[\"dc_end_time\"]: timestamp of event\n                - C[\"dc_end_price\"]: price at event\n                - C[\"dc_event_type\"]: event label (\"PDCC2_UP\" or \"PDCC_Down\")\n                - regime: derived label (\"up\" or \"down\")\n            \"\"\"\n            C = self.C\n            # Work on a copy so we never mutate the input dataframe\n            dc_f = dc_df.copy()\n            # Filter DC summary rows to only the current threshold\n            # The DC summary table MAY contain multiple thresholds (0.001, 0.005, etc.).\n            # We only want anchors that correspond to *this* component's active threshold.\n            if C[\"dc_threshold\"] in dc_f.columns:\n                dc_f = dc_f[np.isclose(\n                    dc_f[C[\"dc_threshold\"]].astype(float), # cast to float\n                    self.threshold,  # this is the component's active threshold\n                    rtol=0, atol=1e-12 # exact match within tiny tolerance\n                )]\n            # Keep only PDCC events (regime change anchors)\n            # We don't want OSV (overshoot) rows here; those don\u2019t start a new regime.\n            # PDCC2_UP \u2192 marks the beginning of an upward regime\n            # PDCC_Down \u2192 marks the beginning of a downward regime\n            want = dc_f[C[\"dc_event_type\"]].isin([\"PDCC2_UP\", \"PDCC_Down\"])\n            # --- Select and rename relevant columns ---\n            # Rename them to uniform names so downstream logic doesn\u2019t depend on dataset schema.\n            # 'end_time' becomes 'event_time' \u2192 the timestamp of the PDCC event\n            # 'end_price' becomes 'anchor_price' \u2192 the price level at that event\n            anchors = dc_f.loc[want, [\n                C[\"dc_end_time\"], C[\"dc_end_price\"], C[\"dc_event_type\"]\n            ]]\n\n            # Ensure end_time is parsed and timezone-normalized\n            anchors[C[\"dc_end_time\"]] = self._to_utc_naive(anchors[C[\"dc_end_time\"]])\n\n            # Sort anchors chronologically by end_time (required for merge_asof)\n            anchors = (\n                anchors\n                .sort_values(C[\"dc_end_time\"])\n                .reset_index(drop=True)\n            )\n\n            # Label regime direction\n            anchors[\"regime\"] = np.where(\n                anchors[C[\"dc_event_type\"]].eq(\"PDCC2_UP\"), \"up\", \"down\"\n            )\n\n            # Warn if there are duplicates (i.e. multiple PDCC events at the same time)\n            num_dupes = anchors.duplicated(subset=[C[\"dc_end_time\"]]).sum()\n            if num_dupes > 0:\n                logging.warning(f\"[FE] Found {num_dupes} anchors sharing the same end_time.\")\n\n            return anchors\n\n        def _merge_state(self, ticks_df, anchors):\n            \"\"\"Attach latest PDCC regime to each tick.\n\n            Align each tick with the latest PDCC anchor (regime state).\n\n            - For every tick timestamp, find the most recent PDCC event that\n            occurred before or at that tick.\n            - This assigns a regime ('up' or 'down') and anchor price per tick.\n\n            Returns\n            -------\n            merged : DataFrame\n                Columns include:\n                - C[\"ticks_time\"]: timestamp of tick\n                - C[\"anchor_price\"]: price at anchor event\n                - regime: derived label (\"up\" or \"down\")\n            \"\"\"\n            C = self.C\n\n            # Normalize tick timestamps and sort for chronological merge\n            ticks_df[C[\"ticks_time\"]] = self._to_utc_naive(ticks_df[C[\"ticks_time\"]])\n            ticks_df = ticks_df.sort_values(C[\"ticks_time\"]).reset_index(drop=True)\n\n            # Handle edge case: no anchors for this threshold\n            if anchors.empty:\n                raise ValueError(\n                    f\"[FE] No PDCC anchors found for threshold={self.threshold}. \"\n                    \"Cannot compute directional-change features without at least one PDCC event.\"\n                )\n\n            # Merge each tick with the last PDCC anchor before it (backward match)\n\n            # merge_asof performs a time-aware \u201clast known value\u201d join\n            # it finds, for each left-side timestamp, the latest right-side row whose \n            # timestamp is less than or equal to the left one.\n\n            # i.e For each tick, what was the most recent PDCC event \n            # that happened before that tick?\n            merged = pd.merge_asof(\n                left=ticks_df,\n                right=anchors[[C[\"dc_end_time\"], C[\"dc_end_price\"], \"regime\"]],\n                left_on=C[\"ticks_time\"], # trade_ts\n                right_on=C[\"dc_end_time\"], # PDCC event_time\n                direction=\"backward\", # Match latest event <= tick time\n            )\n            return merged\n\n        @staticmethod\n        def _compute_features(df, C):\n            \"\"\"\n            Generate continuous Directional Change (DC) features for every tick.\n\n            Each tick inherits its last known PDCC regime (up/down/none) and computes\n            instantaneous overshoot magnitudes relative to the corresponding regime's\n            anchor price.\n\n            - PDCC2_UP / PDCC_Down fire only on the actual PDCC confirmation tick.\n            - regime_up / regime_down added as persistent regime state trackers.\n\n            For each tick:\n            - Determine current PDCC regime (\"up\", \"down\", or \"none\")\n            - Compute % overshoot from anchor price based on regime:\n                    up   \u2192 (PRICE - anchor_price) / anchor_price * 100\n                    down \u2192 (anchor_price - PRICE) / anchor_price * 100\n            - Add binary regime indicators and overshoot flags\n\n            Returns\n            -------\n            df : pandas.DataFrame\n                Input ticks annotated with the following engineered columns:\n\n                - C[\"col_pdcc_flag\"]: 1 if tick belongs to any PDCC regime, else 0\n                - C[\"col_pdcc_up\"]: 1 if in upward regime, else 0\n                - C[\"col_pdcc_down\"]: 1 if in downward regime, else 0\n                - C[\"col_osv_up\"]: upward overshoot magnitude (%)\n                - C[\"col_osv_down\"]: downward overshoot magnitude (%)\n                - C[\"col_osv_up_flag\"]: 1 if overshoot magnitude > 0 in upward regime\n                - C[\"col_osv_down_flag\"]: 1 if overshoot magnitude > 0 in downward regime\n\n            Notes\n            -----\n            - Overshoots are set to 0 outside active regimes.\n            - NaN or infinite values are replaced with 0 for stability.\n            - This method assumes df already includes columns:\n                PRICE, anchor_price (C[\"dc_end_price\"]), and regime.\n            \"\"\"\n\n            # Normalize regime column\n            reg = df[\"regime\"].fillna(\"none\")\n\n            # Continuous regime indicators\n            regime_up = (reg == \"up\").astype(int)\n            regime_down = (reg == \"down\").astype(int)\n\n            # PDCC event confirmation flags \n            # Mark only the tick that aligns exactly with the PDCC event itself\n            tick_time = df[C[\"ticks_time\"]]\n            anchor_time = df[C[\"dc_end_time\"]]\n            pdcc2_up = ((reg == \"up\") & (tick_time == anchor_time)).astype(int)\n            pdcc_down = ((reg == \"down\") & (tick_time == anchor_time)).astype(int)\n\n            # Signed OSV magnitudes\n            price = df[C[\"ticks_price\"]].astype(float)\n            # Anchors will be NaN for ticks before the first PDCC event.\n            # Guard division to avoid propagating NaNs or Infs in those rows.\n            anchor = df[C[\"dc_end_price\"]].astype(float)\n            safe_anchor = np.where(np.isnan(anchor) | (anchor == 0.0), np.nan, anchor)\n\n\n            # Continuous signed OSV magnitudes (actual magnitudes, even during retracements)\n            osv_up = np.where(\n                regime_up.astype(bool),\n                (price - safe_anchor) / safe_anchor * 100.0,\n                0.0,   # outside regime\n            )\n\n            osv_down = np.where(\n                regime_down.astype(bool),\n                (safe_anchor - price) / safe_anchor * 100.0,\n                0.0,   # outside regime\n            )\n\n            # Clamped OSV magnitudes (floor at 0)\n            osv_up_v2 = np.where(\n                regime_up.astype(bool),\n                np.maximum(0.0, (price - safe_anchor) / safe_anchor * 100.0),\n                0.0,\n            )\n            osv_down_v2 = np.where(\n                regime_down.astype(bool),\n                np.maximum(0.0, (safe_anchor - price) / safe_anchor * 100.0),\n                0.0,\n            )\n\n            # Replace NaNs from safe_anchor with 0 only outside regime; inside regime we want continuity.\n            # (If anchor was NaN inside regime, that indicates an upstream join issue and should be rare.)\n            # Pragmatically fill any residual NaNs with 0:\n            osv_up, osv_down, osv_up_v2, osv_down_v2 = [\n                np.nan_to_num(x, nan=0.0, posinf=0.0, neginf=0.0)\n                for x in (osv_up, osv_down, osv_up_v2, osv_down_v2)\n            ]\n\n\n            # Detect NaN or Inf before cleaning\n            nan_up, nan_dn = np.isnan(osv_up).sum(), np.isnan(osv_down).sum()\n            inf_up, inf_dn = np.isinf(osv_up).sum(), np.isinf(osv_down).sum()\n            if any([nan_up, nan_dn, inf_up, inf_dn]):\n                logging.warning(f\"[FE] NaNs: up={nan_up}, down={nan_dn}; infs: up={inf_up}, down={inf_dn}\")\n\n            # Write engineered columns\n            df[C[\"col_pdcc_up\"]] = pdcc2_up\n            df[C[\"col_pdcc_down\"]] = pdcc_down\n\n            # Persistent regime trackers\n            df[\"regime_up\"] = regime_up\n            df[\"regime_down\"] = regime_down\n\n            # Continuous signed OSV magnitudes\n            df[C[\"col_osv_up\"]] = osv_up\n            df[C[\"col_osv_down\"]] = osv_down\n\n            # Clamped OSV magnitudes\n            df[f\"{C['col_osv_up']}_clamped\"] = osv_up_v2\n            df[f\"{C['col_osv_down']}_clamped\"] = osv_down_v2\n\n            return df\n\n        @staticmethod\n        def _compute_regime_stats(df, C):\n            \"\"\"\n            Compute per-regime duration statistics (in ticks).\n\n            For each regime between consecutive PDCC events,\n            count how many ticks belong to that regime and report\n            mean/std per regime type (up vs down).\n            \"\"\"\n            # Sort chronologically to ensure consistent order\n            df = df.sort_values(C[\"ticks_time\"]).reset_index(drop=True)\n\n            # Mark PDCC boundaries and regime type at each start\n            pdcc_idx = df.index[(df[C[\"col_pdcc_up\"]] == 1) | (df[C[\"col_pdcc_down\"]] == 1)]\n            regime_type = np.where(\n                df.loc[pdcc_idx, C[\"col_pdcc_up\"]] == 1, \"up\", \"down\"\n            )\n\n            # If no PDCC events, return zeros\n            if len(pdcc_idx) == 0:\n                return {\n                    \"avg_ticks_up_regime\": 0.0,\n                    \"avg_ticks_down_regime\": 0.0,\n                    \"std_ticks_up_regime\": 0.0,\n                    \"std_ticks_down_regime\": 0.0,\n                    \"num_up_regimes\": 0,\n                    \"num_down_regimes\": 0,\n                }\n\n            # Append the last index to close final segment\n            pdcc_idx = list(pdcc_idx) + [len(df)]\n            lengths = np.diff(pdcc_idx)\n\n            # Match each regime length with its regime type\n            regimes = pd.DataFrame({\n                \"type\": regime_type,\n                \"length\": lengths,\n            })\n\n            # Compute per-type stats\n            stats = {}\n            for t in [\"up\", \"down\"]:\n                lengths_t = regimes.loc[regimes[\"type\"] == t, \"length\"]\n                stats[f\"avg_ticks_{t}_regime\"] = float(lengths_t.mean()) if len(lengths_t) else 0.0\n                stats[f\"std_ticks_{t}_regime\"] = float(lengths_t.std()) if len(lengths_t) else 0.0\n                stats[f\"num_{t}_regimes\"] = int(len(lengths_t))\n\n            return stats\n\n        def _collect_metrics(self, ticks_df, dc_df, anchors, engineered):\n            \"\"\"Collect record counts, coverage, and NaN sanity checks.\"\"\"\n            C = self.C\n            fe_metrics = {\n                \"num_ticks_input\": len(ticks_df),\n                \"num_dc_events_input\": len(dc_df),\n                \"num_pdcc_anchors\": len(anchors),\n                \"num_ticks_output\": len(engineered),\n                \"tick_count_match\": int(len(engineered) == len(ticks_df)),\n            }\n\n            # Count regime coverage\n            fe_metrics[\"regime_up_ticks\"] = int(engineered[\"regime_up\"].sum())\n            fe_metrics[\"regime_down_ticks\"] = int(engineered[\"regime_down\"].sum())\n\n            # Compute regime duration statistics\n            regime_stats = self._compute_regime_stats(engineered, C)\n            fe_metrics.update(regime_stats)\n\n            # Check for NaNs in engineered DC columns\n            engineered_cols = [\n                C[\"col_pdcc_up\"], C[\"col_pdcc_down\"],\n                C[\"col_osv_up\"], C[\"col_osv_down\"],\n                f\"{C['col_osv_up']}_clamped\", f\"{C['col_osv_down']}_clamped\",\n            ]\n            nan_stats = engineered[engineered_cols].isna().sum().to_dict()\n            for k, v in nan_stats.items():\n                fe_metrics[f\"nan_count_{k}\"] = int(v)\n\n            # Log none regime count\n            none_count = int(engineered[\"regime\"].isna().sum())\n            fe_metrics[\"none_regime_count\"] = none_count\n\n            # Log metrics to Cloud Logging\n            logging.info(f\"[FE metrics] {fe_metrics}\")\n\n            # Log metrics to Vertex AI metrics\n            if self.metrics is not None:\n                for k, v in fe_metrics.items():\n                    if isinstance(v, (int, float)):\n                        self.metrics.log_metric(k, float(v))\n            return\n\n        def _load_to_bq(\n            self,\n            df_ticks: pd.DataFrame,\n            project_id: str,\n            dataset_id: str,\n            table_id: str,\n        ):\n            \"\"\"\n            Load feature-engineered DC tick dataset to BigQuery with metadata.\n\n            This writes every augmented tick (PRICE, PDCC flags, OSV magnitudes, etc.)\n            to BigQuery under a given experiment and threshold context.\n\n            Parameters\n            ----------\n            df_ticks : pd.DataFrame\n                Full tick-level feature-engineered dataset.\n            threshold : float\n                The DC threshold value used for this processing run.\n            project_id : str\n                GCP project ID where the table resides.\n            dataset_id : str\n                BigQuery dataset ID (e.g. 'coindesk').\n            table_id : str\n                BigQuery table ID (e.g. 'feature_engineered_ticks').\n\n            Returns\n            -------\n            df_ticks : pd.DataFrame\n                The same DataFrame, returned after successful load.\n            \"\"\"\n\n            if df_ticks.empty:\n                logging.warning(\"[FE->BQ] No rows to load (empty DataFrame).\")\n                return None\n\n            # Ensure timestamps are parsed and UTC-normalized\n            C = self.C\n            try:\n                df_ticks[C[\"ticks_time\"]] = pd.to_datetime(\n                    df_ticks[C[\"ticks_time\"]], utc=True, format=\"mixed\", errors=\"raise\"\n                )\n            except Exception as e:\n                raise ValueError(f\"[FE->BQ] Timestamp normalization failed: {e}\")    \n\n            # Initialize BigQuery client\n            client = bigquery.Client(project=project_id)\n            table_ref = client.dataset(dataset_id).table(table_id)\n\n            # Define BigQuery job config\n            job_config = bigquery.LoadJobConfig(\n                write_disposition=\"WRITE_TRUNCATE\",\n                # source_format=bigquery.SourceFormat.PARQUET,\n                time_partitioning=bigquery.TimePartitioning(field=self.C[\"ticks_time\"]),\n                # clustering_fields=[\n                #     C[\"ticks_price\"],\n                #     C[\"col_pdcc_up\"], C[\"col_pdcc_down\"],\n                #     C[\"col_osv_up\"], f\"{C['col_osv_up']}_clamped\",\n                #     C[\"col_osv_down\"], f\"{C['col_osv_down']}_clamped\",\n                # ],\n            )\n\n            # Load DataFrame to BigQuery\n            try:\n                job = client.load_table_from_dataframe(df_ticks, table_ref, job_config=job_config)\n                job.result()  # Wait for the job to finish\n                logging.info(f\"[FE->BQ] Loaded {len(df_ticks)} ticks into {dataset_id}.{table_id}\")\n            except Exception as e:\n                raise RuntimeError(f\"[FE->BQ] Failed to load feature-engineered ticks: {e}\")\n\n            return df_ticks\n\n        def run(self):\n            \"\"\"\n            Run the complete feature engineering workflow:\n                1. Load data\n                2. Prepare PDCC anchors\n                3. Merge anchors with ticks\n                4. Compute DC features\n                5. Collect metrics\n                6. Save engineered output\n            \"\"\"\n            ticks_df, dc_df = self._load_inputs()\n            anchors = self._prepare_dc_anchors(dc_df)\n            merged = self._merge_state(ticks_df, anchors)\n\n            engineered = self._compute_features(merged, self.C)\n            self._collect_metrics(ticks_df, dc_df, anchors, engineered)\n\n            thr_str = str(self.threshold).replace('.', 'p')\n\n            os.makedirs(self.output_dataset.path, exist_ok=True)\n            out_file = os.path.join(self.output_dataset.path, f\"feat_eng_{thr_str}.csv\")\n            engineered.to_csv(out_file, index=False, encoding=\"utf-8\")\n            logging.info(f\"[FE] Saved {len(engineered)} rows to {out_file}\")\n\n\n            self._load_to_bq(\n                df_ticks=engineered,\n                project_id=\"derivatives-417104\",\n                dataset_id=\"coindesk\",\n                table_id=f\"feat_eng_ticks_{thr_str}\",\n            )\n            logging.info(f\"[FE] Loaded {len(engineered)} ticks to BigQuery\")\n\n            return engineered\n\n    fe = FeatureEngineer(\n        ticks_path=raw_ticks.path,\n        dc_path=dc_summary.path,\n        threshold=threshold,\n        columns_json=columns_json,\n        metrics=metrics,\n        output_dataset=dc_ticks,\n    )\n    fe.run()\n\n"
          ],
          "image": "python:3.11"
        }
      },
      "exec-tf-data-splitter": {
        "container": {
          "args": [
            "--executor_input",
            "{{$}}",
            "--function_to_execute",
            "tf_data_splitter"
          ],
          "command": [
            "sh",
            "-c",
            "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location '--upgrade' 'pip' 'setuptools' 'wheel' 'cython<3.0.0' '--no-build-isolation' 'pyyaml==5.4.1' 'pandas>=1.5.0' 'numpy>=1.21.0' 'scikit-learn>=1.0.0' 'gcsfs>=2023.0.0' 'pyarrow>=12.0.0' 'fastavro>=1.7.0' 'joblib>=1.3.0' 'tensorflow>=2.12.0'  &&  python3 -m pip install --quiet --no-warn-script-location 'kfp==2.15.2' '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"' && \"$0\" \"$@\"\n",
            "sh",
            "-ec",
            "program_path=$(mktemp -d)\n\nprintf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\n_KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n",
            "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import *\n\ndef tf_data_splitter(\n    dataset: Input[Dataset],\n    train_data: Output[Dataset],\n    valid_data: Output[Dataset],\n    test_data: Output[Dataset],\n    scaler_artifact: Output[Artifact],\n    num_shards: int = 16,\n    compress: bool = True,\n    columns_json: str = \"\",\n    scale_indicators: bool = False,\n):\n    \"\"\"\n    Split extracted data into train/validation/test with feature scaling.\n    Saves splits to Output[Dataset]s as sharded TFRecords (.tfrecord[.gz])\n    and persists a StandardScaler as an artifact.\n\n    Args:\n        dataset: Input dataset dir (expects files under dataset.uri)\n        train_data: Output training dataset (70%)\n        valid_data: Output validation dataset (20%)\n        test_data: Output test dataset (10%)\n        scaler_artifact: Output artifact directory to store scaler.joblib + metadata\n        num_shards: Number of TFRecord shards per split (default 16)\n        compress: If True, uses GZIP compression for TFRecords\n        columns_json: Optional JSON mapping of column names to adapt to other schemas.\n    \"\"\"\n    import logging\n    from pathlib import Path\n    import json\n\n    import pandas as pd\n    from sklearn.preprocessing import StandardScaler\n    import joblib\n\n    import gcsfs\n    import pyarrow.parquet as pq\n    from fastavro import reader as avro_reader\n\n    import tensorflow as tf\n    import numpy as np\n\n    # ---------- Schema mapping ----------\n    default_cols = {\n        \"time_col\": \"trade_ts\",     # main ordering/timestamp column\n        \"feature_price\": \"PRICE\",\n        \"feature_vol_quote\": \"vol_quote\",\n        \"feature_cvd_quote\": \"cvd_quote\",\n        \"feature_pdcc_down\": \"PDCC_Down\",\n        \"feature_osv_down\": \"OSV_Down\",\n        \"feature_pdcc_up\": \"PDCC2_UP\",\n        \"feature_osv_up\": \"OSV_Up\",\n        \"feature_regime_up\": \"regime_up\",\n        \"feature_regime_down\": \"regime_down\",\n\n    }\n    overrides = json.loads(columns_json) if columns_json else {}\n    C = {**default_cols, **overrides}\n\n    # ---------- Timestamp normalization helper ----------\n    def _to_utc_naive(series: pd.Series) -> pd.Series:\n        \"\"\"Normalize timestamp-like column to UTC naive.\"\"\"\n        try:\n            series = series.astype(str).str.replace(\" UTC\", \"\", regex=False)\n            s = pd.to_datetime(series, utc=True, errors=\"coerce\", format=\"mixed\")\n            return s.dt.tz_convert(\"UTC\").dt.tz_localize(None)\n        except Exception as e:\n            raise ValueError(f\"[tf_data_splitter] Timestamp conversion failed: {e}\")\n\n    fs = gcsfs.GCSFileSystem()\n    logging.info(f\"Reading from: {dataset.uri}\")\n\n    # ---------------------------------------------------------\n    # Read input files (CSV / Parquet / Avro / extension-less)\n    # ---------------------------------------------------------\n    all_files = fs.ls(dataset.uri)\n    logging.info(f\"Files in dataset: {all_files}\")\n\n    csv_files = [f for f in all_files if f.endswith(\".csv\")]\n    parquet_files = [f for f in all_files if f.endswith(\".parquet\")]\n    avro_files = [f for f in all_files if f.endswith(\".avro\")]\n    untyped_files = [\n        f\n        for f in all_files\n        if not any(f.endswith(ext) for ext in [\".csv\", \".parquet\", \".avro\"])\n    ]\n\n    df = None\n    if csv_files:\n        logging.info(\"Reading CSV file(s)\")\n        df = pd.concat(\n            [pd.read_csv(fs.open(f, \"rb\")) for f in csv_files], ignore_index=True\n        )\n    elif parquet_files:\n        logging.info(\"Reading Parquet file(s)\")\n        df = pd.concat(\n            [pq.read_table(fs.open(f, \"rb\")).to_pandas() for f in parquet_files],\n            ignore_index=True,\n        )\n    elif avro_files:\n        logging.info(\"Reading Avro file(s)\")\n        with fs.open(avro_files[0], \"rb\") as f:\n            records = list(avro_reader(f))\n            df = pd.DataFrame.from_records(records)\n    elif untyped_files:\n        logging.warning(\"No extension detected \u2014 attempting to read first file as CSV\")\n        with fs.open(untyped_files[0], \"rb\") as f:\n            head = f.read(2048).decode(\"utf-8\", errors=\"ignore\")\n            if \",\" in head and \"\\n\" in head:\n                f.seek(0)\n                df = pd.read_csv(f)\n            else:\n                raise ValueError(\"Untyped file does not look like CSV.\")\n    else:\n        raise ValueError(f\"No supported data files found in {dataset.uri}\")\n\n    if df is None or df.empty:\n        raise ValueError(\"Loaded dataframe is empty.\")\n\n    # -----------------------------------------\n    # Required columns & basic ordering\n    # -----------------------------------------\n    required_cols = list(C.values())\n    missing_cols = [c for c in required_cols if c not in df.columns]\n    if missing_cols:\n        raise ValueError(f\"Missing required columns: {missing_cols}\")\n\n    # Normalize timestamp column to UTC naive\n    df[C[\"time_col\"]] = _to_utc_naive(df[C[\"time_col\"]])\n    df = df.sort_values(C[\"time_col\"]).reset_index(drop=True)\n\n    # -----------------------------------------\n    # Time-based split 70/20/10\n    # -----------------------------------------\n    n = len(df)\n    if n < 100:\n        raise ValueError(f\"Not enough rows ({n}) to split into train/val/test.\")\n\n    i_train = int(n * 0.7)\n    i_val = int(n * 0.9)\n\n    train_df = df.iloc[:i_train].copy()\n    val_df = df.iloc[i_train:i_val].copy()\n    test_df = df.iloc[i_val:].copy()\n\n    logging.info(\n        \"Split sizes -> Train: %s, Val: %s, Test: %s\",\n        len(train_df),\n        len(val_df),\n        len(test_df),\n    )\n\n    # -----------------------------------------\n    # Feature scaling (fit on TRAIN only)\n    # -----------------------------------------\n    feature_cols = [\n        C[\"feature_price\"],\n        C[\"feature_vol_quote\"],\n        C[\"feature_cvd_quote\"],\n        C[\"feature_pdcc_down\"],\n        C[\"feature_osv_down\"],\n        C[\"feature_pdcc_up\"],\n        C[\"feature_osv_up\"],\n        C[\"feature_regime_up\"],\n        C[\"feature_regime_down\"],\n    ]\n\n    # Separate indicator-type columns (binary flags)\n    indicator_cols = [\n        C[\"feature_regime_up\"],\n        C[\"feature_regime_down\"],\n        C[\"feature_pdcc_up\"],\n        C[\"feature_pdcc_down\"],\n    ]\n    # Continuous columns = everything else\n    continuous_cols = [c for c in feature_cols if c not in indicator_cols]\n\n    std_cols = [f\"{c}_std\" for c in feature_cols]\n\n    # Coerce to numeric & simple imputation (ffill/bfill) to avoid NaNs in scaler\n    for split in (train_df, val_df, test_df):\n        split[feature_cols] = split[feature_cols].apply(pd.to_numeric, errors=\"coerce\")\n        split[feature_cols] = split[feature_cols].ffill().bfill()\n\n    scaler = StandardScaler().fit(train_df[continuous_cols].to_numpy())\n\n    def _apply_scaling(split_df):\n        split_df = split_df.copy()\n        if scale_indicators:\n            # Apply same scaler to continuous + indicator cols\n            scaled = scaler.transform(split_df[continuous_cols])\n            scaled_df = split_df.copy()\n            scaled_df[continuous_cols] = scaled\n            # Indicators are linearly shifted/scaled by same scaler instance\n            scaled_df[indicator_cols] = (split_df[indicator_cols] - 0.5) * 2.0  # Optional symmetric scaling\n        else:\n            # Scale continuous only; leave indicators as raw 0/1\n            scaled_cont = scaler.transform(split_df[continuous_cols])\n            scaled_df = split_df.copy()\n            scaled_df[continuous_cols] = scaled_cont\n        return scaled_df\n\n    # Columns to write (timestamps + raw + standardized)\n    out_cols = [C[\"time_col\"]] + feature_cols + std_cols\n\n    # -----------------------------------------\n    # Helpers: TFExample serialization\n    # -----------------------------------------\n    def _bytes_feature(v: bytes) -> tf.train.Feature:\n        return tf.train.Feature(bytes_list=tf.train.BytesList(value=[v]))\n\n    def _float_feature_list(values) -> tf.train.Feature:\n        return tf.train.Feature(\n            float_list=tf.train.FloatList(value=[float(x) for x in values])\n        )\n\n    def _float_feature(v: float) -> tf.train.Feature:\n        return tf.train.Feature(float_list=tf.train.FloatList(value=[float(v)]))\n\n    def _to_example(row_dict: dict) -> bytes:\n        # Ensure timestamp-like fields are strings (preserve your original meaning)\n        time_str = str(row_dict[C[\"time_col\"]])\n        feat = {C[\"time_col\"]: _bytes_feature(time_str.encode(\"utf-8\"))}\n        for c in feature_cols:\n            feat[c] = _float_feature(row_dict[c])\n\n        # Standardized features\n        for c in std_cols:\n            feat[c] = _float_feature(row_dict[c])\n\n        example = tf.train.Example(features=tf.train.Features(feature=feat))\n        return example.SerializeToString()\n\n    def _scale_and_write_tfrecords(split_df: pd.DataFrame, out_dir: str, name: str):\n        # transform (standardize) using fitted scaler\n        # scaled = scaler.transform(split_df[feature_cols].to_numpy())\n        # split_df = split_df.copy()  # avoid SettingWithCopy on caller\n        # split_df.loc[:, std_cols] = scaled\n        split_df = _apply_scaling(split_df)\n        split_df.loc[:, std_cols] = split_df[feature_cols].to_numpy()\n\n        # Only keep expected columns (same as CSV version)\n        cols_present = [c for c in out_cols if c in split_df.columns]\n        split_df = split_df.loc[:, cols_present]\n\n        # Prepare output directory and shard writers\n        tf.io.gfile.makedirs(out_dir)\n        options = tf.io.TFRecordOptions(compression_type=\"GZIP\") if compress else None\n        suffix = \".tfrecord.gz\" if compress else \".tfrecord\"\n\n        writers = []\n        for i in range(max(1, num_shards)):\n            shard_path = f\"{out_dir}/part-{i:05d}{suffix}\"\n            w = tf.io.TFRecordWriter(shard_path, options=options)\n            writers.append(w)\n\n        # Write rows to shards\n        total = len(split_df)\n        for idx, (_, row) in enumerate(split_df.iterrows()):\n            ex = _to_example(row.to_dict())\n            writers[idx % len(writers)].write(ex)\n\n        for w in writers:\n            w.close()\n\n        logging.info(\n            \"Saved %s TFRecords to %s (rows=%s, shards=%s, gzip=%s)\",\n            name,\n            out_dir,\n            total,\n            len(writers),\n            compress,\n        )\n\n        if name == \"train\":\n            means = split_df[std_cols].mean(numeric_only=True)\n            stds = split_df[std_cols].std(numeric_only=True, ddof=0)\n            for c in std_cols:\n                logging.info(f\"  {c}: mean={means[c]:.4f}, std={stds[c]:.4f}\")\n\n        # Emit a small schema/metadata JSON to help\n        # downstream parsing (optional but useful)\n        meta = {\n            # --- Core TFRecord schema ---\n            \"string_features\": [C[\"time_col\"]],\n            \"float_features\": feature_cols + std_cols,\n            \"compression\": \"GZIP\" if compress else \"NONE\",\n            \"num_shards\": int(len(writers)),\n            \"rows\": int(total),\n            \"standardized_cols\": std_cols,\n            \"raw_feature_cols\": feature_cols,\n\n            # --- Scaler / experiment info ---\n            \"framework\": \"scikit-learn\",\n            \"version\": \">=1.0.0\",\n            \"type\": \"StandardScaler\",\n            \"feature_order\": feature_cols,\n            \"mean_\": scaler.mean_.tolist(),\n            \"scale_\": scaler.scale_.tolist(),\n            \"fitted_on_rows\": int(len(train_df)),\n            \"std_feature_order\": std_cols,\n            \"scale_indicators\": bool(scale_indicators),\n        }\n\n\n\n\n        with tf.io.gfile.GFile(f\"{out_dir}/features_metadata.json\", \"w\") as f:\n            f.write(json.dumps(meta, indent=2))\n\n        return meta\n\n    # -----------------------------------------\n    # Save splits as TFRecords (replaces CSV)\n    # -----------------------------------------\n    meta_train = _scale_and_write_tfrecords(train_df, train_data.path, \"train\")\n    _scale_and_write_tfrecords(val_df, valid_data.path, \"val\")\n    _scale_and_write_tfrecords(test_df, test_data.path, \"test\")\n\n    # -----------------------------------------\n    # Persist scaler as an artifact (joblib + metadata)\n    # -----------------------------------------\n    scaler_dir = Path(scaler_artifact.path)\n    scaler_dir.mkdir(parents=True, exist_ok=True)\n\n    scaler_bin_path = scaler_dir / \"scaler.joblib\"\n    joblib.dump(scaler, scaler_bin_path)\n\n    # meta = {\n    #     \"framework\": \"scikit-learn\",\n    #     \"version\": \">=1.0.0\",\n    #     \"type\": \"StandardScaler\",\n    #     \"feature_order\": feature_cols,\n    #     \"mean_\": scaler.mean_.tolist(),\n    #     \"scale_\": scaler.scale_.tolist(),\n    #     \"fitted_on_rows\": int(len(train_df)),\n    #     # Helpful for the trainer to know shapes/order:\n    #     \"std_feature_order\": std_cols,\n    #     \"string_features\": [C[\"time_col\"]],\n    #     \"compression\": \"GZIP\" if compress else \"NONE\",\n    #     \"num_shards_per_split\": int(max(1, num_shards)),\n    # }\n    with open(scaler_dir / \"scaler_metadata.json\", \"w\") as f:\n        json.dump(meta_train, f, indent=2)\n\n    # Optional: surface metadata in UI\n    scaler_artifact.metadata[\"type\"] = \"StandardScaler\"\n    scaler_artifact.metadata[\"n_features\"] = len(feature_cols)\n    scaler_artifact.metadata[\"feature_order\"] = feature_cols\n    scaler_artifact.uri = scaler_dir.as_posix()\n\n"
          ],
          "image": "python:3.10"
        }
      },
      "exec-tf-data-splitter-2": {
        "container": {
          "args": [
            "--executor_input",
            "{{$}}",
            "--function_to_execute",
            "tf_data_splitter"
          ],
          "command": [
            "sh",
            "-c",
            "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location '--upgrade' 'pip' 'setuptools' 'wheel' 'cython<3.0.0' '--no-build-isolation' 'pyyaml==5.4.1' 'pandas>=1.5.0' 'numpy>=1.21.0' 'scikit-learn>=1.0.0' 'gcsfs>=2023.0.0' 'pyarrow>=12.0.0' 'fastavro>=1.7.0' 'joblib>=1.3.0' 'tensorflow>=2.12.0'  &&  python3 -m pip install --quiet --no-warn-script-location 'kfp==2.15.2' '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"' && \"$0\" \"$@\"\n",
            "sh",
            "-ec",
            "program_path=$(mktemp -d)\n\nprintf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\n_KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n",
            "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import *\n\ndef tf_data_splitter(\n    dataset: Input[Dataset],\n    train_data: Output[Dataset],\n    valid_data: Output[Dataset],\n    test_data: Output[Dataset],\n    scaler_artifact: Output[Artifact],\n    num_shards: int = 16,\n    compress: bool = True,\n    columns_json: str = \"\",\n    scale_indicators: bool = False,\n):\n    \"\"\"\n    Split extracted data into train/validation/test with feature scaling.\n    Saves splits to Output[Dataset]s as sharded TFRecords (.tfrecord[.gz])\n    and persists a StandardScaler as an artifact.\n\n    Args:\n        dataset: Input dataset dir (expects files under dataset.uri)\n        train_data: Output training dataset (70%)\n        valid_data: Output validation dataset (20%)\n        test_data: Output test dataset (10%)\n        scaler_artifact: Output artifact directory to store scaler.joblib + metadata\n        num_shards: Number of TFRecord shards per split (default 16)\n        compress: If True, uses GZIP compression for TFRecords\n        columns_json: Optional JSON mapping of column names to adapt to other schemas.\n    \"\"\"\n    import logging\n    from pathlib import Path\n    import json\n\n    import pandas as pd\n    from sklearn.preprocessing import StandardScaler\n    import joblib\n\n    import gcsfs\n    import pyarrow.parquet as pq\n    from fastavro import reader as avro_reader\n\n    import tensorflow as tf\n    import numpy as np\n\n    # ---------- Schema mapping ----------\n    default_cols = {\n        \"time_col\": \"trade_ts\",     # main ordering/timestamp column\n        \"feature_price\": \"PRICE\",\n        \"feature_vol_quote\": \"vol_quote\",\n        \"feature_cvd_quote\": \"cvd_quote\",\n        \"feature_pdcc_down\": \"PDCC_Down\",\n        \"feature_osv_down\": \"OSV_Down\",\n        \"feature_pdcc_up\": \"PDCC2_UP\",\n        \"feature_osv_up\": \"OSV_Up\",\n        \"feature_regime_up\": \"regime_up\",\n        \"feature_regime_down\": \"regime_down\",\n\n    }\n    overrides = json.loads(columns_json) if columns_json else {}\n    C = {**default_cols, **overrides}\n\n    # ---------- Timestamp normalization helper ----------\n    def _to_utc_naive(series: pd.Series) -> pd.Series:\n        \"\"\"Normalize timestamp-like column to UTC naive.\"\"\"\n        try:\n            series = series.astype(str).str.replace(\" UTC\", \"\", regex=False)\n            s = pd.to_datetime(series, utc=True, errors=\"coerce\", format=\"mixed\")\n            return s.dt.tz_convert(\"UTC\").dt.tz_localize(None)\n        except Exception as e:\n            raise ValueError(f\"[tf_data_splitter] Timestamp conversion failed: {e}\")\n\n    fs = gcsfs.GCSFileSystem()\n    logging.info(f\"Reading from: {dataset.uri}\")\n\n    # ---------------------------------------------------------\n    # Read input files (CSV / Parquet / Avro / extension-less)\n    # ---------------------------------------------------------\n    all_files = fs.ls(dataset.uri)\n    logging.info(f\"Files in dataset: {all_files}\")\n\n    csv_files = [f for f in all_files if f.endswith(\".csv\")]\n    parquet_files = [f for f in all_files if f.endswith(\".parquet\")]\n    avro_files = [f for f in all_files if f.endswith(\".avro\")]\n    untyped_files = [\n        f\n        for f in all_files\n        if not any(f.endswith(ext) for ext in [\".csv\", \".parquet\", \".avro\"])\n    ]\n\n    df = None\n    if csv_files:\n        logging.info(\"Reading CSV file(s)\")\n        df = pd.concat(\n            [pd.read_csv(fs.open(f, \"rb\")) for f in csv_files], ignore_index=True\n        )\n    elif parquet_files:\n        logging.info(\"Reading Parquet file(s)\")\n        df = pd.concat(\n            [pq.read_table(fs.open(f, \"rb\")).to_pandas() for f in parquet_files],\n            ignore_index=True,\n        )\n    elif avro_files:\n        logging.info(\"Reading Avro file(s)\")\n        with fs.open(avro_files[0], \"rb\") as f:\n            records = list(avro_reader(f))\n            df = pd.DataFrame.from_records(records)\n    elif untyped_files:\n        logging.warning(\"No extension detected \u2014 attempting to read first file as CSV\")\n        with fs.open(untyped_files[0], \"rb\") as f:\n            head = f.read(2048).decode(\"utf-8\", errors=\"ignore\")\n            if \",\" in head and \"\\n\" in head:\n                f.seek(0)\n                df = pd.read_csv(f)\n            else:\n                raise ValueError(\"Untyped file does not look like CSV.\")\n    else:\n        raise ValueError(f\"No supported data files found in {dataset.uri}\")\n\n    if df is None or df.empty:\n        raise ValueError(\"Loaded dataframe is empty.\")\n\n    # -----------------------------------------\n    # Required columns & basic ordering\n    # -----------------------------------------\n    required_cols = list(C.values())\n    missing_cols = [c for c in required_cols if c not in df.columns]\n    if missing_cols:\n        raise ValueError(f\"Missing required columns: {missing_cols}\")\n\n    # Normalize timestamp column to UTC naive\n    df[C[\"time_col\"]] = _to_utc_naive(df[C[\"time_col\"]])\n    df = df.sort_values(C[\"time_col\"]).reset_index(drop=True)\n\n    # -----------------------------------------\n    # Time-based split 70/20/10\n    # -----------------------------------------\n    n = len(df)\n    if n < 100:\n        raise ValueError(f\"Not enough rows ({n}) to split into train/val/test.\")\n\n    i_train = int(n * 0.7)\n    i_val = int(n * 0.9)\n\n    train_df = df.iloc[:i_train].copy()\n    val_df = df.iloc[i_train:i_val].copy()\n    test_df = df.iloc[i_val:].copy()\n\n    logging.info(\n        \"Split sizes -> Train: %s, Val: %s, Test: %s\",\n        len(train_df),\n        len(val_df),\n        len(test_df),\n    )\n\n    # -----------------------------------------\n    # Feature scaling (fit on TRAIN only)\n    # -----------------------------------------\n    feature_cols = [\n        C[\"feature_price\"],\n        C[\"feature_vol_quote\"],\n        C[\"feature_cvd_quote\"],\n        C[\"feature_pdcc_down\"],\n        C[\"feature_osv_down\"],\n        C[\"feature_pdcc_up\"],\n        C[\"feature_osv_up\"],\n        C[\"feature_regime_up\"],\n        C[\"feature_regime_down\"],\n    ]\n\n    # Separate indicator-type columns (binary flags)\n    indicator_cols = [\n        C[\"feature_regime_up\"],\n        C[\"feature_regime_down\"],\n        C[\"feature_pdcc_up\"],\n        C[\"feature_pdcc_down\"],\n    ]\n    # Continuous columns = everything else\n    continuous_cols = [c for c in feature_cols if c not in indicator_cols]\n\n    std_cols = [f\"{c}_std\" for c in feature_cols]\n\n    # Coerce to numeric & simple imputation (ffill/bfill) to avoid NaNs in scaler\n    for split in (train_df, val_df, test_df):\n        split[feature_cols] = split[feature_cols].apply(pd.to_numeric, errors=\"coerce\")\n        split[feature_cols] = split[feature_cols].ffill().bfill()\n\n    scaler = StandardScaler().fit(train_df[continuous_cols].to_numpy())\n\n    def _apply_scaling(split_df):\n        split_df = split_df.copy()\n        if scale_indicators:\n            # Apply same scaler to continuous + indicator cols\n            scaled = scaler.transform(split_df[continuous_cols])\n            scaled_df = split_df.copy()\n            scaled_df[continuous_cols] = scaled\n            # Indicators are linearly shifted/scaled by same scaler instance\n            scaled_df[indicator_cols] = (split_df[indicator_cols] - 0.5) * 2.0  # Optional symmetric scaling\n        else:\n            # Scale continuous only; leave indicators as raw 0/1\n            scaled_cont = scaler.transform(split_df[continuous_cols])\n            scaled_df = split_df.copy()\n            scaled_df[continuous_cols] = scaled_cont\n        return scaled_df\n\n    # Columns to write (timestamps + raw + standardized)\n    out_cols = [C[\"time_col\"]] + feature_cols + std_cols\n\n    # -----------------------------------------\n    # Helpers: TFExample serialization\n    # -----------------------------------------\n    def _bytes_feature(v: bytes) -> tf.train.Feature:\n        return tf.train.Feature(bytes_list=tf.train.BytesList(value=[v]))\n\n    def _float_feature_list(values) -> tf.train.Feature:\n        return tf.train.Feature(\n            float_list=tf.train.FloatList(value=[float(x) for x in values])\n        )\n\n    def _float_feature(v: float) -> tf.train.Feature:\n        return tf.train.Feature(float_list=tf.train.FloatList(value=[float(v)]))\n\n    def _to_example(row_dict: dict) -> bytes:\n        # Ensure timestamp-like fields are strings (preserve your original meaning)\n        time_str = str(row_dict[C[\"time_col\"]])\n        feat = {C[\"time_col\"]: _bytes_feature(time_str.encode(\"utf-8\"))}\n        for c in feature_cols:\n            feat[c] = _float_feature(row_dict[c])\n\n        # Standardized features\n        for c in std_cols:\n            feat[c] = _float_feature(row_dict[c])\n\n        example = tf.train.Example(features=tf.train.Features(feature=feat))\n        return example.SerializeToString()\n\n    def _scale_and_write_tfrecords(split_df: pd.DataFrame, out_dir: str, name: str):\n        # transform (standardize) using fitted scaler\n        # scaled = scaler.transform(split_df[feature_cols].to_numpy())\n        # split_df = split_df.copy()  # avoid SettingWithCopy on caller\n        # split_df.loc[:, std_cols] = scaled\n        split_df = _apply_scaling(split_df)\n        split_df.loc[:, std_cols] = split_df[feature_cols].to_numpy()\n\n        # Only keep expected columns (same as CSV version)\n        cols_present = [c for c in out_cols if c in split_df.columns]\n        split_df = split_df.loc[:, cols_present]\n\n        # Prepare output directory and shard writers\n        tf.io.gfile.makedirs(out_dir)\n        options = tf.io.TFRecordOptions(compression_type=\"GZIP\") if compress else None\n        suffix = \".tfrecord.gz\" if compress else \".tfrecord\"\n\n        writers = []\n        for i in range(max(1, num_shards)):\n            shard_path = f\"{out_dir}/part-{i:05d}{suffix}\"\n            w = tf.io.TFRecordWriter(shard_path, options=options)\n            writers.append(w)\n\n        # Write rows to shards\n        total = len(split_df)\n        for idx, (_, row) in enumerate(split_df.iterrows()):\n            ex = _to_example(row.to_dict())\n            writers[idx % len(writers)].write(ex)\n\n        for w in writers:\n            w.close()\n\n        logging.info(\n            \"Saved %s TFRecords to %s (rows=%s, shards=%s, gzip=%s)\",\n            name,\n            out_dir,\n            total,\n            len(writers),\n            compress,\n        )\n\n        if name == \"train\":\n            means = split_df[std_cols].mean(numeric_only=True)\n            stds = split_df[std_cols].std(numeric_only=True, ddof=0)\n            for c in std_cols:\n                logging.info(f\"  {c}: mean={means[c]:.4f}, std={stds[c]:.4f}\")\n\n        # Emit a small schema/metadata JSON to help\n        # downstream parsing (optional but useful)\n        meta = {\n            # --- Core TFRecord schema ---\n            \"string_features\": [C[\"time_col\"]],\n            \"float_features\": feature_cols + std_cols,\n            \"compression\": \"GZIP\" if compress else \"NONE\",\n            \"num_shards\": int(len(writers)),\n            \"rows\": int(total),\n            \"standardized_cols\": std_cols,\n            \"raw_feature_cols\": feature_cols,\n\n            # --- Scaler / experiment info ---\n            \"framework\": \"scikit-learn\",\n            \"version\": \">=1.0.0\",\n            \"type\": \"StandardScaler\",\n            \"feature_order\": feature_cols,\n            \"mean_\": scaler.mean_.tolist(),\n            \"scale_\": scaler.scale_.tolist(),\n            \"fitted_on_rows\": int(len(train_df)),\n            \"std_feature_order\": std_cols,\n            \"scale_indicators\": bool(scale_indicators),\n        }\n\n\n\n\n        with tf.io.gfile.GFile(f\"{out_dir}/features_metadata.json\", \"w\") as f:\n            f.write(json.dumps(meta, indent=2))\n\n        return meta\n\n    # -----------------------------------------\n    # Save splits as TFRecords (replaces CSV)\n    # -----------------------------------------\n    meta_train = _scale_and_write_tfrecords(train_df, train_data.path, \"train\")\n    _scale_and_write_tfrecords(val_df, valid_data.path, \"val\")\n    _scale_and_write_tfrecords(test_df, test_data.path, \"test\")\n\n    # -----------------------------------------\n    # Persist scaler as an artifact (joblib + metadata)\n    # -----------------------------------------\n    scaler_dir = Path(scaler_artifact.path)\n    scaler_dir.mkdir(parents=True, exist_ok=True)\n\n    scaler_bin_path = scaler_dir / \"scaler.joblib\"\n    joblib.dump(scaler, scaler_bin_path)\n\n    # meta = {\n    #     \"framework\": \"scikit-learn\",\n    #     \"version\": \">=1.0.0\",\n    #     \"type\": \"StandardScaler\",\n    #     \"feature_order\": feature_cols,\n    #     \"mean_\": scaler.mean_.tolist(),\n    #     \"scale_\": scaler.scale_.tolist(),\n    #     \"fitted_on_rows\": int(len(train_df)),\n    #     # Helpful for the trainer to know shapes/order:\n    #     \"std_feature_order\": std_cols,\n    #     \"string_features\": [C[\"time_col\"]],\n    #     \"compression\": \"GZIP\" if compress else \"NONE\",\n    #     \"num_shards_per_split\": int(max(1, num_shards)),\n    # }\n    with open(scaler_dir / \"scaler_metadata.json\", \"w\") as f:\n        json.dump(meta_train, f, indent=2)\n\n    # Optional: surface metadata in UI\n    scaler_artifact.metadata[\"type\"] = \"StandardScaler\"\n    scaler_artifact.metadata[\"n_features\"] = len(feature_cols)\n    scaler_artifact.metadata[\"feature_order\"] = feature_cols\n    scaler_artifact.uri = scaler_dir.as_posix()\n\n"
          ],
          "image": "python:3.10"
        }
      },
      "exec-window-dataset": {
        "container": {
          "args": [
            "--executor_input",
            "{{$}}",
            "--function_to_execute",
            "window_dataset"
          ],
          "command": [
            "sh",
            "-c",
            "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location 'pandas' 'numpy' 'tensorflow' 'google-cloud-logging' 'matplotlib'  &&  python3 -m pip install --quiet --no-warn-script-location 'kfp==2.15.2' '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"' && \"$0\" \"$@\"\n",
            "sh",
            "-ec",
            "program_path=$(mktemp -d)\n\nprintf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\n_KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n",
            "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import *\n\ndef window_dataset(\n    train_data: Input[Dataset],\n    valid_data: Input[Dataset],\n    test_data: Input[Dataset],\n    train_windowed: Output[Dataset],\n    valid_windowed: Output[Dataset],\n    test_windowed: Output[Dataset],\n    plots_output: Output[HTML],\n    metrics: Output[Metrics],\n    feature_names: List[str] = [\n        \"PRICE_std\",\n        \"vol_quote_std\",\n        \"cvd_quote_std\",\n        \"PDCC_Down\",\n        \"OSV_Down_std\",\n        \"PDCC2_UP\",\n        \"regime_up\",\n        \"regime_down\",\n    ],\n    label_columns: List[str] = [\"PRICE_std\"],\n    input_width: int = 50,\n    label_width: int = 1,\n    shift: int = 50,\n    batch_size: int = 32,\n):\n    from typing import Dict, List, Optional\n    import tensorflow as tf\n    import numpy as np\n    import pandas as pd\n    import os\n    import logging\n    import google.cloud.logging\n\n    import io\n    import base64\n\n    import matplotlib\n    matplotlib.use(\"Agg\")\n    import matplotlib.pyplot as plt\n    # Set up logging\n    client = google.cloud.logging.Client()\n    client.setup_logging()\n\n    def _tfrecord_feature_spec(\n        float_feature_names: List[str],\n    ) -> Dict[str, tf.io.FixedLenFeature]:\n        \"\"\"\n        Build a TFRecord feature spec for parsing float features.\n\n        Parameters\n        ----------\n        float_feature_names : list of str\n            Names of scalar float features stored in each Example. Each will be\n            parsed as a `tf.float32` FixedLenFeature of shape [] (scalar).\n\n        Returns\n        -------\n        dict[str, tf.io.FixedLenFeature]\n            A mapping `{feature_name: tf.io.FixedLenFeature([], tf.float32)}` that\n            can be passed to `tf.io.parse_single_example`.\n\n        Notes\n        -----\n        - Only float features are included. If you later need string timestamps\n        (e.g., `\"start_time\"`), add:\n        `spec[\"start_time\"] = tf.io.FixedLenFeature([], tf.string)`.\n\n        Examples\n        --------\n        >>> spec = _tfrecord_feature_spec([\"PRICE_std\", \"vol_quote_std\"])\n        >>> isinstance(spec[\"PRICE_std\"], tf.io.FixedLenFeature)\n        True\n        >>> spec[\"PRICE_std\"].dtype == tf.float32\n        True\n        \"\"\"\n        # We parse only what we need for training; string timestamps are ignored.\n        spec = {name: tf.io.FixedLenFeature([], tf.float32) for name in float_feature_names}\n        # If you ever need timestamps, you can add:\n        # spec[\"start_time\"] = tf.io.FixedLenFeature([], tf.string)\n        # spec[\"load_time_toronto\"] = tf.io.FixedLenFeature([], tf.string)\n        return spec\n\n    def _parse_example(serialized, feature_spec, feature_order: List[str]):\n        \"\"\"\n        Parse a single serialized TF Example into a fixed-order feature vector.\n\n        Parameters\n        ----------\n        serialized : tf.Tensor (scalar string)\n            A single serialized `tf.train.Example` proto.\n        feature_spec : dict\n            Mapping of feature names to `tf.io.FixedLenFeature` (e.g. from\n            `_tfrecord_feature_spec`).\n        feature_order : list of str\n            The exact order of feature names to stack into the output vector.\n\n        Returns\n        -------\n        tf.Tensor\n            Tensor of shape `[n_features]` (`tf.float32`), where `n_features = len(feature_order)`.\n\n        Notes\n        -----\n        - Ensures deterministic column ordering by stacking features in `feature_order`.\n        - Suitable for feeding into subsequent windowing ops.\n\n        Examples\n        --------\n        >>> spec = _tfrecord_feature_spec([\"PRICE_std\", \"vol_quote_std\"])\n        >>> # ds_raw yields serialized Examples (strings)\n        >>> # x = ds_raw.map(lambda s: _parse_example(s, spec, [\"PRICE_std\", \"vol_quote_std\"]))\n        >>> # `x` elements will each have shape [2] (PRICE_std, vol_quote_std)\n        \"\"\"\n        parsed = tf.io.parse_single_example(serialized, feature_spec)\n        # Assemble features in a fixed order -> vector [n_features]\n        x = tf.stack([parsed[name] for name in feature_order], axis=-1)\n        return x  # shape: [n_features], dtype float32\n\n\n    def _make_forecasting_dataset_from_tfrecord_dir(\n        data_dir: str,\n        *,\n        input_width: int,          # L\n        label_width: int,          # H (multi-step length); use 1 for single-point\n        feature_names: List[str],\n        label_name: str,           # e.g. hparams[\"label_name\"] == \"PRICE_std\"\n        shift: int = 0,            # gap between input end and label start (0 => labels start immediately after inputs)\n        batch_size: int = 128,\n        shuffle_sequences: bool = True,\n        compression_type: str = \"GZIP\",\n    ) -> tf.data.Dataset:\n        \"\"\"\n        Create a windowed forecasting dataset `(x, y)` from TFRecord shards.\n\n        Parameters\n        ----------\n        data_dir : str\n            Directory containing `*.tfrecord[.gz]` shards.\n        input_width : int\n            Number of timesteps in the input window `L`.\n        label_width : int\n            Number of future timesteps to predict `H` (multi-step horizon).\n            This **is** the `H` you see in `y` shape `[batch, H]`.\n        feature_names : list of str\n            All float feature names to parse. Order defines column order of `x`.\n        label_name : str\n            Name of the **single** target feature to extract for `y`.\n        shift : int, default 0\n            Number of timesteps between end of the input window and the start of labels.\n            `shift=0` makes labels start immediately after inputs.\n        batch_size : int, default 128\n            Batch size after windowing.\n        shuffle_sequences : bool, default True\n            If True, shuffle windowed sequences (use False for deterministic inspection).\n        compression_type : {\"GZIP\", None}, default \"GZIP\"\n            Compression used when writing TFRecords.\n\n        Returns\n        -------\n        tf.data.Dataset\n            Dataset yielding tuples `(x, y)` where:\n            - `x` has shape `[batch, input_width, n_features]`\n            - `y` has shape `[batch, label_width]` (only the single target feature)\n\n        Windowing Semantics\n        -------------------\n        Let `L = input_width`, `H = label_width` and `S = shift`.\n        Each contiguous **chunk** has length `L + S + H`.\n        We split it as:\n        inputs: indices `[0 .. L-1]`\n        gap:    indices `[L .. L+S-1]` (ignored)\n        labels: indices `[L+S .. L+S+H-1]`  (from `label_name` only)\n        \"\"\"\n        total_window = input_width + shift + label_width\n        label_idx = feature_names.index(label_name)\n\n        pattern = os.path.join(\n            data_dir, \"*.tfrecord.gz\" if compression_type == \"GZIP\" else \"*.tfrecord\"\n        )\n        logging.info(f\"Reading TFRecords from pattern: {pattern}\")\n\n        # Important for time series: keep file order deterministic (no shuffling at the\n        # file level).\n        # https://www.tensorflow.org/api_docs/python/tf/data/Dataset#list_files\n        files = tf.data.Dataset.list_files(pattern, shuffle=False)\n\n        # Stream the shards deterministically\n        # https://www.tensorflow.org/api_docs/python/tf/data/Dataset#interleave\n        ds = files.interleave(\n            lambda fp: tf.data.TFRecordDataset(fp, compression_type=compression_type),\n            cycle_length=1, # keep in-order; bump if cross-shard mixing is OK\n            num_parallel_calls=tf.data.AUTOTUNE,\n            deterministic=True,\n        )\n\n        feature_spec = _tfrecord_feature_spec(feature_names)\n        ds = ds.map(\n            lambda s: _parse_example(s, feature_spec, feature_names),  # -> [n_features]\n            num_parallel_calls=tf.data.AUTOTUNE,\n        )\n\n        # Sliding windows of consecutive rows -> [seq_length, n_features]\n        # window() keeps order; drop_remainder enforces fixed length\n        ds = ds.window(size=total_window, shift=1, drop_remainder=True)\n        ds = ds.flat_map(lambda w: w.batch(total_window, drop_remainder=True))  # -> [T_total, F]\n\n        # Split into (inputs, labels)\n        def split_window(chunk: tf.Tensor):\n            # chunk: [total_window, n_features]\n            x = chunk[:input_width, :]  # [L, F]\n            # labels start after input_width + shift\n            y_start = input_width + shift\n            y = chunk[y_start : y_start + label_width, label_idx]  # [H]\n            return x, y\n\n        ds = ds.map(split_window, num_parallel_calls=tf.data.AUTOTUNE)\n\n        # Shuffle windows for training\n        if shuffle_sequences:\n            ds = ds.shuffle(buffer_size=10000, reshuffle_each_iteration=True)\n\n        # Batch and prefetch\n        ds = ds.batch(batch_size, drop_remainder=True)\n        ds = ds.prefetch(tf.data.AUTOTUNE)\n        return ds\n\n    # -------------------------------------------------------------------------\n    # Write windowed dataset to TFRecords under an output artifact directory\n    # -------------------------------------------------------------------------\n    def _write_windowed_dataset(\n        ds: tf.data.Dataset,\n        out_dir: str,\n        *,\n        compression_type: str = \"GZIP\",\n        shard_prefix: str = \"data\",\n        max_examples: int = None,  # None = all\n    ) -> int:\n        \"\"\"Materialize (x, y) windowed dataset as TFRecords. Returns num_examples.\"\"\"\n        os.makedirs(out_dir, exist_ok=True)\n        suffix = \".tfrecord.gz\" if compression_type == \"GZIP\" else \".tfrecord\"\n        shard_path = os.path.join(out_dir, f\"{shard_prefix}-00000-of-00001{suffix}\")\n\n        options = tf.io.TFRecordOptions(compression_type=compression_type) if compression_type else None\n        writer = tf.io.TFRecordWriter(shard_path, options=options)\n\n        def _float_feature(values):\n            return tf.train.Feature(float_list=tf.train.FloatList(value=values))\n\n        def _int_feature(values):\n            return tf.train.Feature(int64_list=tf.train.Int64List(value=values))\n\n        n = 0\n        for x_batch, y_batch in ds:\n            x_np = x_batch.numpy()  # [B, L, F]\n            y_np = y_batch.numpy()  # [B, H]\n            B, L, F = x_np.shape\n            H = y_np.shape[1]\n\n            for i in range(B):\n                x_flat = x_np[i].reshape(-1).astype(np.float32)  # L*F\n                y_flat = y_np[i].astype(np.float32)              # H\n                example = tf.train.Example(\n                    features=tf.train.Features(\n                        feature={\n                            \"x_flat\": _float_feature(x_flat.tolist()),\n                            \"x_shape\": _int_feature([L, F]),\n                            \"y\": _float_feature(y_flat.tolist()),\n                            \"y_shape\": _int_feature([H]),\n                        }\n                    )\n                )\n                writer.write(example.SerializeToString())\n                n += 1\n                if max_examples is not None and n >= max_examples:\n                    break\n            if max_examples is not None and n >= max_examples:\n                break\n\n        writer.close()\n        logging.info(f\"Wrote {n} examples to {shard_path}\")\n        return n\n\n    train_xy = _make_forecasting_dataset_from_tfrecord_dir(\n        data_dir=train_data.path,\n        input_width=input_width,\n        label_width=label_width,\n        shift=shift,\n        batch_size=batch_size,\n        feature_names=feature_names,\n        label_name=label_columns[0],\n        shuffle_sequences=False,\n        compression_type=\"GZIP\",\n    )\n    val_xy = _make_forecasting_dataset_from_tfrecord_dir(\n        data_dir=valid_data.path,\n        input_width=input_width,\n        label_width=label_width,\n        shift=shift,\n        batch_size=batch_size,\n        feature_names=feature_names,\n        label_name=label_columns[0],\n        shuffle_sequences=False,\n        compression_type=\"GZIP\",\n    )\n    test_xy = _make_forecasting_dataset_from_tfrecord_dir(\n        data_dir=test_data.path,\n        input_width=input_width,\n        label_width=label_width,\n        shift=shift,\n        batch_size=batch_size,\n        feature_names=feature_names,\n        label_name=label_columns[0],\n        shuffle_sequences=False,\n        compression_type=\"GZIP\",\n    )\n\n    # -------------------------------------------------------------------------\n    # Metrics: number of records across all batches\n    # -------------------------------------------------------------------------\n    logging.info(\"Computing metrics...\")\n    # ------------- Count records (streaming, no materialization) -------------\n    def count_records(ds: tf.data.Dataset) -> int:\n        # ds is batched: each element is (x_b, y_b) with x_b.shape[0] = batch size\n        return int(\n            ds.map(lambda x, y: tf.shape(x)[0])\n              .reduce(tf.constant(0, tf.int64), lambda acc, b: acc + tf.cast(b, tf.int64))\n              .numpy()\n        )\n\n    train_n = count_records(train_xy)\n    val_n = count_records(val_xy)\n    test_n  = count_records(test_xy)\n\n    # Log metrics\n    metrics.log_metric(\"train_num_records\", float(train_n))\n    metrics.log_metric(\"valid_num_records\", float(val_n))\n    metrics.log_metric(\"test_num_records\", float(test_n))\n    metrics.log_metric(\"total_window_size\", float(input_width + shift + label_width))\n    metrics.log_metric(\"batch_size\", float(batch_size))\n    metrics.log_metric(\"input_width\", float(input_width))\n    metrics.log_metric(\"num_features\", float(len(feature_names)))\n    metrics.log_metric(\"label_width\", float(label_width))\n    metrics.log_metric(\"shift\", float(shift))\n\n\n    # -------------------------------------------------------------------------\n    # Write output datasets (materialize to TFRecords)\n    # -------------------------------------------------------------------------\n    # _ = _write_windowed_dataset(train_xy, train_windowed.path, compression_type=\"GZIP\")\n    # _ = _write_windowed_dataset(val_xy,   valid_windowed.path, compression_type=\"GZIP\")\n    # _ = _write_windowed_dataset(test_xy,  test_windowed.path,  compression_type=\"GZIP\")\n\n    # ------------- Save datasets (most scalable & simple) -------------\n    # Save as tuple (x, y). The loader must pass the same element_spec.\n    # ------------- element_spec for save/load -------------\n    # Each element is (x: [L,F], y: [H])\n    # element_spec = (\n    #     tf.TensorSpec(shape=(input_width, len(feature_names)), dtype=tf.float32),\n    #     tf.TensorSpec(shape=(label_width,), dtype=tf.float32),\n    # )\n    # tf.data.experimental.save(train_xy.unbatch(), train_windowed.path, compression=\"GZIP\")  # unbatch -> per-example\n    # tf.data.experimental.save(val_xy.unbatch(), valid_windowed.path, compression=\"GZIP\")\n    # tf.data.experimental.save(test_xy.unbatch(),  test_windowed.path,  compression=\"GZIP\")\n    # https://www.tensorflow.org/api_docs/python/tf/data/Dataset#save\n    train_xy.unbatch().save(train_windowed.path, compression=\"GZIP\")\n    val_xy.unbatch().save(valid_windowed.path, compression=\"GZIP\")\n    test_xy.unbatch().save(test_windowed.path, compression=\"GZIP\")\n\n    # Plot examples\n    # ------------- Plot 3 windows from first train batch -> HTML -------------\n    first_xb, first_yb = next(iter(train_xy.take(1)))\n    B, L, F = first_xb.shape\n    H = first_yb.shape[1]\n    metrics.log_metric(\"observed_batch_size\", float(B))\n\n    label_idx = feature_names.index(label_columns[0])\n\n    def plot_png(x_seq, y_seq, target, L, S, H):\n        xs = np.arange(L)\n        ys = np.arange(L + S, L + S + H)\n        fig = plt.figure(figsize=(10, 4))\n        plt.plot(xs, x_seq[:, label_idx], marker=\"o\")\n        plt.axvline(x=L - 0.5, linestyle=\"--\")\n        if S > 0:\n            plt.axvspan(L - 0.5, L + S - 0.5, alpha=0.15)\n        plt.scatter(ys, y_seq, zorder=3)\n        for xi, yi, v in zip(ys, y_seq, y_seq):\n            plt.text(xi, yi, f\"{float(v):.4f}\", fontsize=8, rotation=90, va=\"bottom\")\n        plt.title(f\"Window L={L}, shift={S}, H={H}\")\n        plt.xlabel(\"Relative timestep\"); plt.ylabel(target); plt.tight_layout()\n        buf = io.BytesIO(); fig.savefig(buf, format=\"png\", dpi=150, bbox_inches=\"tight\"); plt.close(fig)\n        buf.seek(0); return base64.b64encode(buf.read()).decode(\"ascii\")\n\n    imgs = []\n    for i in range(min(3, B)):\n        imgs.append(plot_png(first_xb[i].numpy(), first_yb[i].numpy(),\n                             label_columns[0], input_width, shift, label_width))\n\n    html = f\"\"\"\n    <html><body style=\"font-family:Inter,system-ui,Arial,sans-serif;padding:16px;\">\n      <h2>Windowing Preview (Train)</h2>\n      <p><b>x shape:</b> [{B}, {L}, {F}] &nbsp; <b>y shape:</b> [{B}, {H}]</p>\n      <table style=\"border-collapse:collapse;margin:10px 0;\">\n        <tr><th style=\"border:1px solid #ccc;padding:6px 10px;\">Split</th>\n            <th style=\"border:1px solid #ccc;padding:6px 10px;\">Records</th></tr>\n        <tr><td style=\"border:1px solid #ccc;padding:6px 10px;\">train</td><td style=\"border:1px solid #ccc;padding:6px 10px;\">{train_n}</td></tr>\n        <tr><td style=\"border:1px solid #ccc;padding:6px 10px;\">valid</td><td style=\"border:1px solid #ccc;padding:6px 10px;\">{val_n}</td></tr>\n        <tr><td style=\"border:1px solid #ccc;padding:6px 10px;\">test</td><td style=\"border:1px solid #ccc;padding:6px 10px;\">{test_n}</td></tr>\n      </table>\n      <h3>Three Example Windows</h3>\n      {''.join(f'<img style=\"max-width:100%;height:auto;border:1px solid #ddd;border-radius:8px;margin:6px 0;\" src=\"data:image/png;base64,{b64}\" />' for b64 in imgs)}\n    </body></html>\n    \"\"\"\n    os.makedirs(os.path.dirname(plots_output.path), exist_ok=True)\n    with open(plots_output.path, \"w\", encoding=\"utf-8\") as f:\n        f.write(html)\n\n"
          ],
          "image": "python:3.10"
        }
      },
      "exec-window-dataset-2": {
        "container": {
          "args": [
            "--executor_input",
            "{{$}}",
            "--function_to_execute",
            "window_dataset"
          ],
          "command": [
            "sh",
            "-c",
            "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location 'pandas' 'numpy' 'tensorflow' 'google-cloud-logging' 'matplotlib'  &&  python3 -m pip install --quiet --no-warn-script-location 'kfp==2.15.2' '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"' && \"$0\" \"$@\"\n",
            "sh",
            "-ec",
            "program_path=$(mktemp -d)\n\nprintf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\n_KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n",
            "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import *\n\ndef window_dataset(\n    train_data: Input[Dataset],\n    valid_data: Input[Dataset],\n    test_data: Input[Dataset],\n    train_windowed: Output[Dataset],\n    valid_windowed: Output[Dataset],\n    test_windowed: Output[Dataset],\n    plots_output: Output[HTML],\n    metrics: Output[Metrics],\n    feature_names: List[str] = [\n        \"PRICE_std\",\n        \"vol_quote_std\",\n        \"cvd_quote_std\",\n        \"PDCC_Down\",\n        \"OSV_Down_std\",\n        \"PDCC2_UP\",\n        \"regime_up\",\n        \"regime_down\",\n    ],\n    label_columns: List[str] = [\"PRICE_std\"],\n    input_width: int = 50,\n    label_width: int = 1,\n    shift: int = 50,\n    batch_size: int = 32,\n):\n    from typing import Dict, List, Optional\n    import tensorflow as tf\n    import numpy as np\n    import pandas as pd\n    import os\n    import logging\n    import google.cloud.logging\n\n    import io\n    import base64\n\n    import matplotlib\n    matplotlib.use(\"Agg\")\n    import matplotlib.pyplot as plt\n    # Set up logging\n    client = google.cloud.logging.Client()\n    client.setup_logging()\n\n    def _tfrecord_feature_spec(\n        float_feature_names: List[str],\n    ) -> Dict[str, tf.io.FixedLenFeature]:\n        \"\"\"\n        Build a TFRecord feature spec for parsing float features.\n\n        Parameters\n        ----------\n        float_feature_names : list of str\n            Names of scalar float features stored in each Example. Each will be\n            parsed as a `tf.float32` FixedLenFeature of shape [] (scalar).\n\n        Returns\n        -------\n        dict[str, tf.io.FixedLenFeature]\n            A mapping `{feature_name: tf.io.FixedLenFeature([], tf.float32)}` that\n            can be passed to `tf.io.parse_single_example`.\n\n        Notes\n        -----\n        - Only float features are included. If you later need string timestamps\n        (e.g., `\"start_time\"`), add:\n        `spec[\"start_time\"] = tf.io.FixedLenFeature([], tf.string)`.\n\n        Examples\n        --------\n        >>> spec = _tfrecord_feature_spec([\"PRICE_std\", \"vol_quote_std\"])\n        >>> isinstance(spec[\"PRICE_std\"], tf.io.FixedLenFeature)\n        True\n        >>> spec[\"PRICE_std\"].dtype == tf.float32\n        True\n        \"\"\"\n        # We parse only what we need for training; string timestamps are ignored.\n        spec = {name: tf.io.FixedLenFeature([], tf.float32) for name in float_feature_names}\n        # If you ever need timestamps, you can add:\n        # spec[\"start_time\"] = tf.io.FixedLenFeature([], tf.string)\n        # spec[\"load_time_toronto\"] = tf.io.FixedLenFeature([], tf.string)\n        return spec\n\n    def _parse_example(serialized, feature_spec, feature_order: List[str]):\n        \"\"\"\n        Parse a single serialized TF Example into a fixed-order feature vector.\n\n        Parameters\n        ----------\n        serialized : tf.Tensor (scalar string)\n            A single serialized `tf.train.Example` proto.\n        feature_spec : dict\n            Mapping of feature names to `tf.io.FixedLenFeature` (e.g. from\n            `_tfrecord_feature_spec`).\n        feature_order : list of str\n            The exact order of feature names to stack into the output vector.\n\n        Returns\n        -------\n        tf.Tensor\n            Tensor of shape `[n_features]` (`tf.float32`), where `n_features = len(feature_order)`.\n\n        Notes\n        -----\n        - Ensures deterministic column ordering by stacking features in `feature_order`.\n        - Suitable for feeding into subsequent windowing ops.\n\n        Examples\n        --------\n        >>> spec = _tfrecord_feature_spec([\"PRICE_std\", \"vol_quote_std\"])\n        >>> # ds_raw yields serialized Examples (strings)\n        >>> # x = ds_raw.map(lambda s: _parse_example(s, spec, [\"PRICE_std\", \"vol_quote_std\"]))\n        >>> # `x` elements will each have shape [2] (PRICE_std, vol_quote_std)\n        \"\"\"\n        parsed = tf.io.parse_single_example(serialized, feature_spec)\n        # Assemble features in a fixed order -> vector [n_features]\n        x = tf.stack([parsed[name] for name in feature_order], axis=-1)\n        return x  # shape: [n_features], dtype float32\n\n\n    def _make_forecasting_dataset_from_tfrecord_dir(\n        data_dir: str,\n        *,\n        input_width: int,          # L\n        label_width: int,          # H (multi-step length); use 1 for single-point\n        feature_names: List[str],\n        label_name: str,           # e.g. hparams[\"label_name\"] == \"PRICE_std\"\n        shift: int = 0,            # gap between input end and label start (0 => labels start immediately after inputs)\n        batch_size: int = 128,\n        shuffle_sequences: bool = True,\n        compression_type: str = \"GZIP\",\n    ) -> tf.data.Dataset:\n        \"\"\"\n        Create a windowed forecasting dataset `(x, y)` from TFRecord shards.\n\n        Parameters\n        ----------\n        data_dir : str\n            Directory containing `*.tfrecord[.gz]` shards.\n        input_width : int\n            Number of timesteps in the input window `L`.\n        label_width : int\n            Number of future timesteps to predict `H` (multi-step horizon).\n            This **is** the `H` you see in `y` shape `[batch, H]`.\n        feature_names : list of str\n            All float feature names to parse. Order defines column order of `x`.\n        label_name : str\n            Name of the **single** target feature to extract for `y`.\n        shift : int, default 0\n            Number of timesteps between end of the input window and the start of labels.\n            `shift=0` makes labels start immediately after inputs.\n        batch_size : int, default 128\n            Batch size after windowing.\n        shuffle_sequences : bool, default True\n            If True, shuffle windowed sequences (use False for deterministic inspection).\n        compression_type : {\"GZIP\", None}, default \"GZIP\"\n            Compression used when writing TFRecords.\n\n        Returns\n        -------\n        tf.data.Dataset\n            Dataset yielding tuples `(x, y)` where:\n            - `x` has shape `[batch, input_width, n_features]`\n            - `y` has shape `[batch, label_width]` (only the single target feature)\n\n        Windowing Semantics\n        -------------------\n        Let `L = input_width`, `H = label_width` and `S = shift`.\n        Each contiguous **chunk** has length `L + S + H`.\n        We split it as:\n        inputs: indices `[0 .. L-1]`\n        gap:    indices `[L .. L+S-1]` (ignored)\n        labels: indices `[L+S .. L+S+H-1]`  (from `label_name` only)\n        \"\"\"\n        total_window = input_width + shift + label_width\n        label_idx = feature_names.index(label_name)\n\n        pattern = os.path.join(\n            data_dir, \"*.tfrecord.gz\" if compression_type == \"GZIP\" else \"*.tfrecord\"\n        )\n        logging.info(f\"Reading TFRecords from pattern: {pattern}\")\n\n        # Important for time series: keep file order deterministic (no shuffling at the\n        # file level).\n        # https://www.tensorflow.org/api_docs/python/tf/data/Dataset#list_files\n        files = tf.data.Dataset.list_files(pattern, shuffle=False)\n\n        # Stream the shards deterministically\n        # https://www.tensorflow.org/api_docs/python/tf/data/Dataset#interleave\n        ds = files.interleave(\n            lambda fp: tf.data.TFRecordDataset(fp, compression_type=compression_type),\n            cycle_length=1, # keep in-order; bump if cross-shard mixing is OK\n            num_parallel_calls=tf.data.AUTOTUNE,\n            deterministic=True,\n        )\n\n        feature_spec = _tfrecord_feature_spec(feature_names)\n        ds = ds.map(\n            lambda s: _parse_example(s, feature_spec, feature_names),  # -> [n_features]\n            num_parallel_calls=tf.data.AUTOTUNE,\n        )\n\n        # Sliding windows of consecutive rows -> [seq_length, n_features]\n        # window() keeps order; drop_remainder enforces fixed length\n        ds = ds.window(size=total_window, shift=1, drop_remainder=True)\n        ds = ds.flat_map(lambda w: w.batch(total_window, drop_remainder=True))  # -> [T_total, F]\n\n        # Split into (inputs, labels)\n        def split_window(chunk: tf.Tensor):\n            # chunk: [total_window, n_features]\n            x = chunk[:input_width, :]  # [L, F]\n            # labels start after input_width + shift\n            y_start = input_width + shift\n            y = chunk[y_start : y_start + label_width, label_idx]  # [H]\n            return x, y\n\n        ds = ds.map(split_window, num_parallel_calls=tf.data.AUTOTUNE)\n\n        # Shuffle windows for training\n        if shuffle_sequences:\n            ds = ds.shuffle(buffer_size=10000, reshuffle_each_iteration=True)\n\n        # Batch and prefetch\n        ds = ds.batch(batch_size, drop_remainder=True)\n        ds = ds.prefetch(tf.data.AUTOTUNE)\n        return ds\n\n    # -------------------------------------------------------------------------\n    # Write windowed dataset to TFRecords under an output artifact directory\n    # -------------------------------------------------------------------------\n    def _write_windowed_dataset(\n        ds: tf.data.Dataset,\n        out_dir: str,\n        *,\n        compression_type: str = \"GZIP\",\n        shard_prefix: str = \"data\",\n        max_examples: int = None,  # None = all\n    ) -> int:\n        \"\"\"Materialize (x, y) windowed dataset as TFRecords. Returns num_examples.\"\"\"\n        os.makedirs(out_dir, exist_ok=True)\n        suffix = \".tfrecord.gz\" if compression_type == \"GZIP\" else \".tfrecord\"\n        shard_path = os.path.join(out_dir, f\"{shard_prefix}-00000-of-00001{suffix}\")\n\n        options = tf.io.TFRecordOptions(compression_type=compression_type) if compression_type else None\n        writer = tf.io.TFRecordWriter(shard_path, options=options)\n\n        def _float_feature(values):\n            return tf.train.Feature(float_list=tf.train.FloatList(value=values))\n\n        def _int_feature(values):\n            return tf.train.Feature(int64_list=tf.train.Int64List(value=values))\n\n        n = 0\n        for x_batch, y_batch in ds:\n            x_np = x_batch.numpy()  # [B, L, F]\n            y_np = y_batch.numpy()  # [B, H]\n            B, L, F = x_np.shape\n            H = y_np.shape[1]\n\n            for i in range(B):\n                x_flat = x_np[i].reshape(-1).astype(np.float32)  # L*F\n                y_flat = y_np[i].astype(np.float32)              # H\n                example = tf.train.Example(\n                    features=tf.train.Features(\n                        feature={\n                            \"x_flat\": _float_feature(x_flat.tolist()),\n                            \"x_shape\": _int_feature([L, F]),\n                            \"y\": _float_feature(y_flat.tolist()),\n                            \"y_shape\": _int_feature([H]),\n                        }\n                    )\n                )\n                writer.write(example.SerializeToString())\n                n += 1\n                if max_examples is not None and n >= max_examples:\n                    break\n            if max_examples is not None and n >= max_examples:\n                break\n\n        writer.close()\n        logging.info(f\"Wrote {n} examples to {shard_path}\")\n        return n\n\n    train_xy = _make_forecasting_dataset_from_tfrecord_dir(\n        data_dir=train_data.path,\n        input_width=input_width,\n        label_width=label_width,\n        shift=shift,\n        batch_size=batch_size,\n        feature_names=feature_names,\n        label_name=label_columns[0],\n        shuffle_sequences=False,\n        compression_type=\"GZIP\",\n    )\n    val_xy = _make_forecasting_dataset_from_tfrecord_dir(\n        data_dir=valid_data.path,\n        input_width=input_width,\n        label_width=label_width,\n        shift=shift,\n        batch_size=batch_size,\n        feature_names=feature_names,\n        label_name=label_columns[0],\n        shuffle_sequences=False,\n        compression_type=\"GZIP\",\n    )\n    test_xy = _make_forecasting_dataset_from_tfrecord_dir(\n        data_dir=test_data.path,\n        input_width=input_width,\n        label_width=label_width,\n        shift=shift,\n        batch_size=batch_size,\n        feature_names=feature_names,\n        label_name=label_columns[0],\n        shuffle_sequences=False,\n        compression_type=\"GZIP\",\n    )\n\n    # -------------------------------------------------------------------------\n    # Metrics: number of records across all batches\n    # -------------------------------------------------------------------------\n    logging.info(\"Computing metrics...\")\n    # ------------- Count records (streaming, no materialization) -------------\n    def count_records(ds: tf.data.Dataset) -> int:\n        # ds is batched: each element is (x_b, y_b) with x_b.shape[0] = batch size\n        return int(\n            ds.map(lambda x, y: tf.shape(x)[0])\n              .reduce(tf.constant(0, tf.int64), lambda acc, b: acc + tf.cast(b, tf.int64))\n              .numpy()\n        )\n\n    train_n = count_records(train_xy)\n    val_n = count_records(val_xy)\n    test_n  = count_records(test_xy)\n\n    # Log metrics\n    metrics.log_metric(\"train_num_records\", float(train_n))\n    metrics.log_metric(\"valid_num_records\", float(val_n))\n    metrics.log_metric(\"test_num_records\", float(test_n))\n    metrics.log_metric(\"total_window_size\", float(input_width + shift + label_width))\n    metrics.log_metric(\"batch_size\", float(batch_size))\n    metrics.log_metric(\"input_width\", float(input_width))\n    metrics.log_metric(\"num_features\", float(len(feature_names)))\n    metrics.log_metric(\"label_width\", float(label_width))\n    metrics.log_metric(\"shift\", float(shift))\n\n\n    # -------------------------------------------------------------------------\n    # Write output datasets (materialize to TFRecords)\n    # -------------------------------------------------------------------------\n    # _ = _write_windowed_dataset(train_xy, train_windowed.path, compression_type=\"GZIP\")\n    # _ = _write_windowed_dataset(val_xy,   valid_windowed.path, compression_type=\"GZIP\")\n    # _ = _write_windowed_dataset(test_xy,  test_windowed.path,  compression_type=\"GZIP\")\n\n    # ------------- Save datasets (most scalable & simple) -------------\n    # Save as tuple (x, y). The loader must pass the same element_spec.\n    # ------------- element_spec for save/load -------------\n    # Each element is (x: [L,F], y: [H])\n    # element_spec = (\n    #     tf.TensorSpec(shape=(input_width, len(feature_names)), dtype=tf.float32),\n    #     tf.TensorSpec(shape=(label_width,), dtype=tf.float32),\n    # )\n    # tf.data.experimental.save(train_xy.unbatch(), train_windowed.path, compression=\"GZIP\")  # unbatch -> per-example\n    # tf.data.experimental.save(val_xy.unbatch(), valid_windowed.path, compression=\"GZIP\")\n    # tf.data.experimental.save(test_xy.unbatch(),  test_windowed.path,  compression=\"GZIP\")\n    # https://www.tensorflow.org/api_docs/python/tf/data/Dataset#save\n    train_xy.unbatch().save(train_windowed.path, compression=\"GZIP\")\n    val_xy.unbatch().save(valid_windowed.path, compression=\"GZIP\")\n    test_xy.unbatch().save(test_windowed.path, compression=\"GZIP\")\n\n    # Plot examples\n    # ------------- Plot 3 windows from first train batch -> HTML -------------\n    first_xb, first_yb = next(iter(train_xy.take(1)))\n    B, L, F = first_xb.shape\n    H = first_yb.shape[1]\n    metrics.log_metric(\"observed_batch_size\", float(B))\n\n    label_idx = feature_names.index(label_columns[0])\n\n    def plot_png(x_seq, y_seq, target, L, S, H):\n        xs = np.arange(L)\n        ys = np.arange(L + S, L + S + H)\n        fig = plt.figure(figsize=(10, 4))\n        plt.plot(xs, x_seq[:, label_idx], marker=\"o\")\n        plt.axvline(x=L - 0.5, linestyle=\"--\")\n        if S > 0:\n            plt.axvspan(L - 0.5, L + S - 0.5, alpha=0.15)\n        plt.scatter(ys, y_seq, zorder=3)\n        for xi, yi, v in zip(ys, y_seq, y_seq):\n            plt.text(xi, yi, f\"{float(v):.4f}\", fontsize=8, rotation=90, va=\"bottom\")\n        plt.title(f\"Window L={L}, shift={S}, H={H}\")\n        plt.xlabel(\"Relative timestep\"); plt.ylabel(target); plt.tight_layout()\n        buf = io.BytesIO(); fig.savefig(buf, format=\"png\", dpi=150, bbox_inches=\"tight\"); plt.close(fig)\n        buf.seek(0); return base64.b64encode(buf.read()).decode(\"ascii\")\n\n    imgs = []\n    for i in range(min(3, B)):\n        imgs.append(plot_png(first_xb[i].numpy(), first_yb[i].numpy(),\n                             label_columns[0], input_width, shift, label_width))\n\n    html = f\"\"\"\n    <html><body style=\"font-family:Inter,system-ui,Arial,sans-serif;padding:16px;\">\n      <h2>Windowing Preview (Train)</h2>\n      <p><b>x shape:</b> [{B}, {L}, {F}] &nbsp; <b>y shape:</b> [{B}, {H}]</p>\n      <table style=\"border-collapse:collapse;margin:10px 0;\">\n        <tr><th style=\"border:1px solid #ccc;padding:6px 10px;\">Split</th>\n            <th style=\"border:1px solid #ccc;padding:6px 10px;\">Records</th></tr>\n        <tr><td style=\"border:1px solid #ccc;padding:6px 10px;\">train</td><td style=\"border:1px solid #ccc;padding:6px 10px;\">{train_n}</td></tr>\n        <tr><td style=\"border:1px solid #ccc;padding:6px 10px;\">valid</td><td style=\"border:1px solid #ccc;padding:6px 10px;\">{val_n}</td></tr>\n        <tr><td style=\"border:1px solid #ccc;padding:6px 10px;\">test</td><td style=\"border:1px solid #ccc;padding:6px 10px;\">{test_n}</td></tr>\n      </table>\n      <h3>Three Example Windows</h3>\n      {''.join(f'<img style=\"max-width:100%;height:auto;border:1px solid #ddd;border-radius:8px;margin:6px 0;\" src=\"data:image/png;base64,{b64}\" />' for b64 in imgs)}\n    </body></html>\n    \"\"\"\n    os.makedirs(os.path.dirname(plots_output.path), exist_ok=True)\n    with open(plots_output.path, \"w\", encoding=\"utf-8\") as f:\n        f.write(html)\n\n"
          ],
          "image": "python:3.10"
        }
      }
    }
  },
  "pipelineInfo": {
    "description": "3-arm DC feature ablation pipeline (v2: equal capacity + regularization).\nTrains models with bottleneck projection layer on:\n  1. Baseline: 3 common features only (no DC)\n  2. Single-DC: 3 common + 6 DC features from one threshold\n  3. Multi-DC: 3 common + 6*4 DC features from all thresholds\n\nAll arms use: Flatten -> Dense(32, bottleneck) -> Dropout(0.3) -> Dense(64, l2) ->\nDropout(0.3) -> Dense(32, l2) -> Dense(1). The bottleneck equalizes effective\ncapacity regardless of input feature count.\n\nThen compares all three arms with statistical tests.",
    "name": "dc-ablation-pipeline"
  },
  "root": {
    "dag": {
      "tasks": {
        "bq-query-to-table": {
          "cachingOptions": {
            "enableCache": true
          },
          "componentRef": {
            "name": "comp-bq-query-to-table"
          },
          "inputs": {
            "parameters": {
              "bq_client_project_id": {
                "componentInputParameter": "project_id"
              },
              "dataset_id": {
                "componentInputParameter": "dataset_id"
              },
              "dataset_location": {
                "componentInputParameter": "dataset_location"
              },
              "destination_project_id": {
                "componentInputParameter": "project_id"
              },
              "pipelinechannel--end_time": {
                "componentInputParameter": "end_time"
              },
              "pipelinechannel--ingestion_dataset_id": {
                "componentInputParameter": "ingestion_dataset_id"
              },
              "pipelinechannel--ingestion_project_id": {
                "componentInputParameter": "ingestion_project_id"
              },
              "pipelinechannel--instrument": {
                "componentInputParameter": "instrument"
              },
              "pipelinechannel--start_time": {
                "componentInputParameter": "start_time"
              },
              "query": {
                "runtimeValue": {
                  "constant": "/* =============================================================================\nMicrostructure Extract (Event-Time, Deduped) \u2014 Price, Volume, CVD\nSource: {project}.{dataset_id}.{table_id}\nDialect: BigQuery Standard SQL\n\nPURPOSE\n- Event-time ordered trade stream with price, volumes, and CVD for modeling.\n\nCHOICES\n- Event time for ordering; LOAD_TIME only for partition pruning & tiebreaks.\n- Dedup by (MARKET, INSTRUMENT, trade_id), keeping earliest LOAD_TIME.\n\nOUTPUT (per trade)\n- MARKET, INSTRUMENT, trade_id, trade_ts, trade_time_toronto\n- price, vol_base, vol_quote, cvd_base, cvd_quote, log_return, inter_trade_duration_seconds\n============================================================================= */\n\nWITH t AS (\n  SELECT\n    MARKET,\n    INSTRUMENT,\n    CAST(LAST_PROCESSED_TRADE_CCSEQ AS STRING) AS trade_id,  -- venue trade sequence/id\n    TIMESTAMP_MICROS(\n      CAST(LAST_PROCESSED_TRADE_TS AS INT64) * 1000000\n      + CAST(DIV(LAST_PROCESSED_TRADE_TS_NS, 1000) AS INT64)\n    ) AS trade_ts,                                            -- event timestamp (\u00b5s)\n    LAST_PROCESSED_TRADE_PRICE          AS price,             -- executed trade price\n    LAST_PROCESSED_TRADE_QUANTITY       AS vol_base,          -- base units (e.g., BTC)\n    LAST_PROCESSED_TRADE_QUOTE_QUANTITY AS vol_quote,         -- quote units (\u2248 vol_base * price)\n    LAST_PROCESSED_TRADE_SIDE           AS side,              -- 'BUY' / 'SELL'\n    LOAD_TIME\n  FROM `{{$.inputs.parameters['pipelinechannel--ingestion_project_id']}}.{{$.inputs.parameters['pipelinechannel--ingestion_dataset_id']}}.coindesk_latest_tick`\n  WHERE INSTRUMENT = '{{$.inputs.parameters['pipelinechannel--instrument']}}'\n    AND LOAD_TIME BETWEEN '{{$.inputs.parameters['pipelinechannel--start_time']}}' AND '{{$.inputs.parameters['pipelinechannel--end_time']}}'          -- partition prune\n    -- AND MOD(ABS(FARM_FINGERPRINT(CAST(LAST_PROCESSED_TRADE_CCSEQ AS STRING))), 100) < 10\n),\n\nbase AS (\n  SELECT *\n  FROM t\n  WHERE trade_ts BETWEEN '{{$.inputs.parameters['pipelinechannel--start_time']}}' AND '{{$.inputs.parameters['pipelinechannel--end_time']}}'           -- event-time window\n  QUALIFY ROW_NUMBER() OVER (                                -- dedup snapshots\n    PARTITION BY MARKET, INSTRUMENT, trade_id\n    ORDER BY trade_ts, LOAD_TIME\n  ) = 1\n),\n\nw AS (\n  SELECT\n    b.*,\n    CASE WHEN UPPER(side) = 'BUY'  THEN  1\n         WHEN UPPER(side) = 'SELL' THEN -1\n         ELSE 0 END AS sgn,\n    LAG(price) OVER (\n      PARTITION BY MARKET, INSTRUMENT\n      ORDER BY trade_ts, trade_id, LOAD_TIME\n    ) AS prev_price,\n    LAG(trade_ts) OVER (\n      PARTITION BY MARKET, INSTRUMENT\n      ORDER BY trade_ts, trade_id, LOAD_TIME\n    ) AS prev_ts\n  FROM base b\n)\n\nSELECT\n  MARKET,\n  INSTRUMENT,\n  trade_id,\n  trade_ts,\n  DATETIME(trade_ts, \"America/Toronto\") AS load_time_toronto,\n  price AS PRICE,\n  vol_base,\n  vol_quote,\n\n  -- CVD per venue (base & quote units)\n  SUM(sgn * vol_base)  OVER (\n    PARTITION BY MARKET, INSTRUMENT\n    ORDER BY trade_ts, trade_id, LOAD_TIME\n  ) AS cvd_base,\n  SUM(sgn * vol_quote) OVER (\n    PARTITION BY MARKET, INSTRUMENT\n    ORDER BY trade_ts, trade_id, LOAD_TIME\n  ) AS cvd_quote,\n\n  -- Microstructure extras\n  SAFE.LN(price / prev_price) AS log_return,\n  TIMESTAMP_DIFF(trade_ts, prev_ts, MICROSECOND) / 1e6 AS inter_trade_duration_seconds\n\nFROM w\nORDER BY trade_ts, trade_id"
                }
              },
              "query_job_config": {
                "runtimeValue": {
                  "constant": {
                    "write_disposition": "WRITE_TRUNCATE"
                  }
                }
              },
              "table_id": {
                "runtimeValue": {
                  "constant": "extracted_ticks_ablation"
                }
              }
            }
          },
          "taskInfo": {
            "name": "Extract Ticks to BQ"
          }
        },
        "compare-forecasts": {
          "cachingOptions": {
            "enableCache": true
          },
          "componentRef": {
            "name": "comp-compare-forecasts"
          },
          "dependentTasks": [
            "custom-forecast-train-job",
            "custom-forecast-train-job-2",
            "custom-forecast-train-job-3"
          ],
          "inputs": {
            "artifacts": {
              "baseline_predictions": {
                "taskOutputArtifact": {
                  "outputArtifactKey": "predictions",
                  "producerTask": "custom-forecast-train-job"
                }
              },
              "multi_dc_predictions": {
                "taskOutputArtifact": {
                  "outputArtifactKey": "predictions",
                  "producerTask": "custom-forecast-train-job-3"
                }
              },
              "single_dc_predictions": {
                "taskOutputArtifact": {
                  "outputArtifactKey": "predictions",
                  "producerTask": "custom-forecast-train-job-2"
                }
              }
            }
          },
          "taskInfo": {
            "name": "Compare Forecasts"
          }
        },
        "concat-threshold-features": {
          "cachingOptions": {
            "enableCache": true
          },
          "componentRef": {
            "name": "comp-concat-threshold-features"
          },
          "dependentTasks": [
            "for-loop-1"
          ],
          "inputs": {
            "artifacts": {
              "test_datasets": {
                "taskOutputArtifact": {
                  "outputArtifactKey": "pipelinechannel--window-dataset-test_windowed",
                  "producerTask": "for-loop-1"
                }
              },
              "train_datasets": {
                "taskOutputArtifact": {
                  "outputArtifactKey": "pipelinechannel--window-dataset-train_windowed",
                  "producerTask": "for-loop-1"
                }
              },
              "valid_datasets": {
                "taskOutputArtifact": {
                  "outputArtifactKey": "pipelinechannel--window-dataset-valid_windowed",
                  "producerTask": "for-loop-1"
                }
              }
            },
            "parameters": {
              "common_feature_names": {
                "runtimeValue": {
                  "constant": [
                    "PRICE_std",
                    "vol_quote_std",
                    "cvd_quote_std"
                  ]
                }
              },
              "feature_names": {
                "runtimeValue": {
                  "constant": [
                    "PRICE_std",
                    "vol_quote_std",
                    "cvd_quote_std",
                    "PDCC_Down",
                    "OSV_Down_std",
                    "OSV_Up_std",
                    "PDCC2_UP",
                    "regime_up",
                    "regime_down"
                  ]
                }
              },
              "per_threshold_feature_names": {
                "runtimeValue": {
                  "constant": []
                }
              }
            }
          },
          "taskInfo": {
            "name": "Baseline Features (3)"
          }
        },
        "concat-threshold-features-2": {
          "cachingOptions": {
            "enableCache": true
          },
          "componentRef": {
            "name": "comp-concat-threshold-features-2"
          },
          "dependentTasks": [
            "for-loop-1"
          ],
          "inputs": {
            "artifacts": {
              "test_datasets": {
                "taskOutputArtifact": {
                  "outputArtifactKey": "pipelinechannel--window-dataset-test_windowed",
                  "producerTask": "for-loop-1"
                }
              },
              "train_datasets": {
                "taskOutputArtifact": {
                  "outputArtifactKey": "pipelinechannel--window-dataset-train_windowed",
                  "producerTask": "for-loop-1"
                }
              },
              "valid_datasets": {
                "taskOutputArtifact": {
                  "outputArtifactKey": "pipelinechannel--window-dataset-valid_windowed",
                  "producerTask": "for-loop-1"
                }
              }
            },
            "parameters": {
              "common_feature_names": {
                "runtimeValue": {
                  "constant": [
                    "PRICE_std",
                    "vol_quote_std",
                    "cvd_quote_std"
                  ]
                }
              },
              "feature_names": {
                "runtimeValue": {
                  "constant": [
                    "PRICE_std",
                    "vol_quote_std",
                    "cvd_quote_std",
                    "PDCC_Down",
                    "OSV_Down_std",
                    "OSV_Up_std",
                    "PDCC2_UP",
                    "regime_up",
                    "regime_down"
                  ]
                }
              },
              "per_threshold_feature_names": {
                "runtimeValue": {
                  "constant": [
                    "PDCC_Down",
                    "OSV_Down_std",
                    "OSV_Up_std",
                    "PDCC2_UP",
                    "regime_up",
                    "regime_down"
                  ]
                }
              }
            }
          },
          "taskInfo": {
            "name": "Multi-DC Features (27)"
          }
        },
        "custom-forecast-train-job": {
          "cachingOptions": {
            "enableCache": true
          },
          "componentRef": {
            "name": "comp-custom-forecast-train-job"
          },
          "dependentTasks": [
            "concat-threshold-features"
          ],
          "inputs": {
            "artifacts": {
              "test_data": {
                "taskOutputArtifact": {
                  "outputArtifactKey": "joint_test",
                  "producerTask": "concat-threshold-features"
                }
              },
              "train_data": {
                "taskOutputArtifact": {
                  "outputArtifactKey": "joint_train",
                  "producerTask": "concat-threshold-features"
                }
              },
              "valid_data": {
                "taskOutputArtifact": {
                  "outputArtifactKey": "joint_valid",
                  "producerTask": "concat-threshold-features"
                }
              }
            },
            "parameters": {
              "hparams": {
                "runtimeValue": {
                  "constant": {
                    "batch_size": 100.0,
                    "bottleneck_dim": 128.0,
                    "distribute_strategy": "single",
                    "dropout_rate": 0.0,
                    "epochs": 20.0,
                    "hidden_units": [
                      {
                        "activation": "relu",
                        "units": 64.0
                      },
                      {
                        "activation": "relu",
                        "units": 32.0
                      }
                    ],
                    "l2_reg": 0.0,
                    "learning_rate": 0.01,
                    "loss_fn": "MeanAbsoluteError",
                    "model_label": "baseline",
                    "optimizer": "AdamW",
                    "patience": 5.0
                  }
                }
              },
              "model_display_name": {
                "runtimeValue": {
                  "constant": "ablation_baseline"
                }
              },
              "pipelinechannel--pipeline_files_gcs_path": {
                "componentInputParameter": "pipeline_files_gcs_path"
              },
              "project_id": {
                "componentInputParameter": "project_id"
              },
              "project_location": {
                "componentInputParameter": "project_location"
              },
              "requirements": {
                "runtimeValue": {
                  "constant": [
                    "google-cloud-logging",
                    "google-cloud-aiplatform[cloud_profiler]",
                    "tensorboard_plugin_profile"
                  ]
                }
              },
              "serving_container_uri": {
                "runtimeValue": {
                  "constant": "us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-15:latest"
                }
              },
              "staging_bucket": {
                "componentInputParameter": "staging_bucket"
              },
              "threshold": {
                "runtimeValue": {
                  "constant": 0.0
                }
              },
              "train_container_uri": {
                "runtimeValue": {
                  "constant": "us-docker.pkg.dev/vertex-ai/training/tf-cpu.2-17.py310:latest"
                }
              },
              "train_script_uri": {
                "runtimeValue": {
                  "constant": "{{$.inputs.parameters['pipelinechannel--pipeline_files_gcs_path']}}/training/assets/tftrain_tf_fast_model.py"
                }
              }
            }
          },
          "taskInfo": {
            "name": "Train Baseline (3 features)"
          }
        },
        "custom-forecast-train-job-2": {
          "cachingOptions": {
            "enableCache": true
          },
          "componentRef": {
            "name": "comp-custom-forecast-train-job-2"
          },
          "dependentTasks": [
            "window-dataset-2"
          ],
          "inputs": {
            "artifacts": {
              "test_data": {
                "taskOutputArtifact": {
                  "outputArtifactKey": "test_windowed",
                  "producerTask": "window-dataset-2"
                }
              },
              "train_data": {
                "taskOutputArtifact": {
                  "outputArtifactKey": "train_windowed",
                  "producerTask": "window-dataset-2"
                }
              },
              "valid_data": {
                "taskOutputArtifact": {
                  "outputArtifactKey": "valid_windowed",
                  "producerTask": "window-dataset-2"
                }
              }
            },
            "parameters": {
              "hparams": {
                "runtimeValue": {
                  "constant": {
                    "batch_size": 100.0,
                    "bottleneck_dim": 128.0,
                    "distribute_strategy": "single",
                    "dropout_rate": 0.0,
                    "epochs": 20.0,
                    "hidden_units": [
                      {
                        "activation": "relu",
                        "units": 64.0
                      },
                      {
                        "activation": "relu",
                        "units": 32.0
                      }
                    ],
                    "l2_reg": 0.0,
                    "learning_rate": 0.01,
                    "loss_fn": "MeanAbsoluteError",
                    "model_label": "single_dc",
                    "optimizer": "AdamW",
                    "patience": 5.0,
                    "threshold": "{{$.inputs.parameters['pipelinechannel--single_dc_threshold']}}"
                  }
                }
              },
              "model_display_name": {
                "runtimeValue": {
                  "constant": "ablation_single_dc"
                }
              },
              "pipelinechannel--pipeline_files_gcs_path": {
                "componentInputParameter": "pipeline_files_gcs_path"
              },
              "pipelinechannel--single_dc_threshold": {
                "componentInputParameter": "single_dc_threshold"
              },
              "project_id": {
                "componentInputParameter": "project_id"
              },
              "project_location": {
                "componentInputParameter": "project_location"
              },
              "requirements": {
                "runtimeValue": {
                  "constant": [
                    "google-cloud-logging",
                    "google-cloud-aiplatform[cloud_profiler]",
                    "tensorboard_plugin_profile"
                  ]
                }
              },
              "serving_container_uri": {
                "runtimeValue": {
                  "constant": "us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-15:latest"
                }
              },
              "staging_bucket": {
                "componentInputParameter": "staging_bucket"
              },
              "threshold": {
                "componentInputParameter": "single_dc_threshold"
              },
              "train_container_uri": {
                "runtimeValue": {
                  "constant": "us-docker.pkg.dev/vertex-ai/training/tf-cpu.2-17.py310:latest"
                }
              },
              "train_script_uri": {
                "runtimeValue": {
                  "constant": "{{$.inputs.parameters['pipelinechannel--pipeline_files_gcs_path']}}/training/assets/tftrain_tf_fast_model.py"
                }
              }
            }
          },
          "taskInfo": {
            "name": "Train Single-DC (9 features)"
          }
        },
        "custom-forecast-train-job-3": {
          "cachingOptions": {
            "enableCache": true
          },
          "componentRef": {
            "name": "comp-custom-forecast-train-job-3"
          },
          "dependentTasks": [
            "concat-threshold-features-2"
          ],
          "inputs": {
            "artifacts": {
              "test_data": {
                "taskOutputArtifact": {
                  "outputArtifactKey": "joint_test",
                  "producerTask": "concat-threshold-features-2"
                }
              },
              "train_data": {
                "taskOutputArtifact": {
                  "outputArtifactKey": "joint_train",
                  "producerTask": "concat-threshold-features-2"
                }
              },
              "valid_data": {
                "taskOutputArtifact": {
                  "outputArtifactKey": "joint_valid",
                  "producerTask": "concat-threshold-features-2"
                }
              }
            },
            "parameters": {
              "hparams": {
                "runtimeValue": {
                  "constant": {
                    "batch_size": 100.0,
                    "bottleneck_dim": 128.0,
                    "distribute_strategy": "single",
                    "dropout_rate": 0.0,
                    "epochs": 20.0,
                    "hidden_units": [
                      {
                        "activation": "relu",
                        "units": 64.0
                      },
                      {
                        "activation": "relu",
                        "units": 32.0
                      }
                    ],
                    "l2_reg": 0.0,
                    "learning_rate": 0.01,
                    "loss_fn": "MeanAbsoluteError",
                    "model_label": "multi_dc",
                    "optimizer": "AdamW",
                    "patience": 5.0
                  }
                }
              },
              "model_display_name": {
                "runtimeValue": {
                  "constant": "ablation_multi_dc"
                }
              },
              "pipelinechannel--pipeline_files_gcs_path": {
                "componentInputParameter": "pipeline_files_gcs_path"
              },
              "project_id": {
                "componentInputParameter": "project_id"
              },
              "project_location": {
                "componentInputParameter": "project_location"
              },
              "requirements": {
                "runtimeValue": {
                  "constant": [
                    "google-cloud-logging",
                    "google-cloud-aiplatform[cloud_profiler]",
                    "tensorboard_plugin_profile"
                  ]
                }
              },
              "serving_container_uri": {
                "runtimeValue": {
                  "constant": "us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-15:latest"
                }
              },
              "staging_bucket": {
                "componentInputParameter": "staging_bucket"
              },
              "threshold": {
                "runtimeValue": {
                  "constant": 9.9
                }
              },
              "train_container_uri": {
                "runtimeValue": {
                  "constant": "us-docker.pkg.dev/vertex-ai/training/tf-cpu.2-17.py310:latest"
                }
              },
              "train_script_uri": {
                "runtimeValue": {
                  "constant": "{{$.inputs.parameters['pipelinechannel--pipeline_files_gcs_path']}}/training/assets/tftrain_tf_fast_model.py"
                }
              }
            }
          },
          "taskInfo": {
            "name": "Train Multi-DC (27 features)"
          }
        },
        "directional-change-detector-2": {
          "cachingOptions": {
            "enableCache": true
          },
          "componentRef": {
            "name": "comp-directional-change-detector-2"
          },
          "dependentTasks": [
            "extract-bq-to-dataset"
          ],
          "inputs": {
            "artifacts": {
              "df": {
                "taskOutputArtifact": {
                  "outputArtifactKey": "dataset",
                  "producerTask": "extract-bq-to-dataset"
                }
              }
            },
            "parameters": {
              "dataset_id": {
                "componentInputParameter": "dataset_id"
              },
              "fast_plot": {
                "runtimeValue": {
                  "constant": true
                }
              },
              "load_to_bq": {
                "runtimeValue": {
                  "constant": false
                }
              },
              "max_event_markers": {
                "runtimeValue": {
                  "constant": 10.0
                }
              },
              "price_col": {
                "runtimeValue": {
                  "constant": "PRICE"
                }
              },
              "project_id": {
                "componentInputParameter": "project_id"
              },
              "thresholds": {
                "componentInputParameter": "single_dc_threshold"
              },
              "time_col": {
                "runtimeValue": {
                  "constant": "trade_ts"
                }
              }
            }
          },
          "taskInfo": {
            "name": "DC Detection (single)"
          }
        },
        "extract-bq-to-dataset": {
          "cachingOptions": {
            "enableCache": true
          },
          "componentRef": {
            "name": "comp-extract-bq-to-dataset"
          },
          "dependentTasks": [
            "bq-query-to-table"
          ],
          "inputs": {
            "parameters": {
              "bq_client_project_id": {
                "componentInputParameter": "project_id"
              },
              "dataset_id": {
                "componentInputParameter": "dataset_id"
              },
              "dataset_location": {
                "componentInputParameter": "dataset_location"
              },
              "source_project_id": {
                "componentInputParameter": "project_id"
              },
              "table_name": {
                "runtimeValue": {
                  "constant": "extracted_ticks_ablation"
                }
              }
            }
          },
          "taskInfo": {
            "name": "Extract Ticks to GCS"
          }
        },
        "feature-engineering-2": {
          "cachingOptions": {
            "enableCache": true
          },
          "componentRef": {
            "name": "comp-feature-engineering-2"
          },
          "dependentTasks": [
            "directional-change-detector-2",
            "extract-bq-to-dataset"
          ],
          "inputs": {
            "artifacts": {
              "dc_summary": {
                "taskOutputArtifact": {
                  "outputArtifactKey": "directional_change_events",
                  "producerTask": "directional-change-detector-2"
                }
              },
              "raw_ticks": {
                "taskOutputArtifact": {
                  "outputArtifactKey": "dataset",
                  "producerTask": "extract-bq-to-dataset"
                }
              }
            },
            "parameters": {
              "threshold": {
                "componentInputParameter": "single_dc_threshold"
              }
            }
          },
          "taskInfo": {
            "name": "Feature Eng (single)"
          }
        },
        "for-loop-1": {
          "componentRef": {
            "name": "comp-for-loop-1"
          },
          "dependentTasks": [
            "extract-bq-to-dataset"
          ],
          "inputs": {
            "artifacts": {
              "pipelinechannel--extract-bq-to-dataset-dataset": {
                "taskOutputArtifact": {
                  "outputArtifactKey": "dataset",
                  "producerTask": "extract-bq-to-dataset"
                }
              }
            },
            "parameters": {
              "pipelinechannel--dataset_id": {
                "componentInputParameter": "dataset_id"
              },
              "pipelinechannel--dc_thresholds": {
                "componentInputParameter": "dc_thresholds"
              },
              "pipelinechannel--input_width": {
                "componentInputParameter": "input_width"
              },
              "pipelinechannel--label_width": {
                "componentInputParameter": "label_width"
              },
              "pipelinechannel--project_id": {
                "componentInputParameter": "project_id"
              },
              "pipelinechannel--shift": {
                "componentInputParameter": "shift"
              }
            }
          },
          "parameterIterator": {
            "itemInput": "pipelinechannel--dc_thresholds-loop-item",
            "items": {
              "inputParameter": "pipelinechannel--dc_thresholds"
            }
          },
          "taskInfo": {
            "name": "for-loop-1"
          }
        },
        "tf-data-splitter-2": {
          "cachingOptions": {
            "enableCache": true
          },
          "componentRef": {
            "name": "comp-tf-data-splitter-2"
          },
          "dependentTasks": [
            "feature-engineering-2"
          ],
          "inputs": {
            "artifacts": {
              "dataset": {
                "taskOutputArtifact": {
                  "outputArtifactKey": "dc_ticks",
                  "producerTask": "feature-engineering-2"
                }
              }
            }
          },
          "taskInfo": {
            "name": "Split (single DC)"
          }
        },
        "window-dataset-2": {
          "cachingOptions": {
            "enableCache": true
          },
          "componentRef": {
            "name": "comp-window-dataset-2"
          },
          "dependentTasks": [
            "tf-data-splitter-2"
          ],
          "inputs": {
            "artifacts": {
              "test_data": {
                "taskOutputArtifact": {
                  "outputArtifactKey": "test_data",
                  "producerTask": "tf-data-splitter-2"
                }
              },
              "train_data": {
                "taskOutputArtifact": {
                  "outputArtifactKey": "train_data",
                  "producerTask": "tf-data-splitter-2"
                }
              },
              "valid_data": {
                "taskOutputArtifact": {
                  "outputArtifactKey": "valid_data",
                  "producerTask": "tf-data-splitter-2"
                }
              }
            },
            "parameters": {
              "batch_size": {
                "runtimeValue": {
                  "constant": 32.0
                }
              },
              "feature_names": {
                "runtimeValue": {
                  "constant": [
                    "PRICE_std",
                    "vol_quote_std",
                    "cvd_quote_std",
                    "PDCC_Down",
                    "OSV_Down_std",
                    "OSV_Up_std",
                    "PDCC2_UP",
                    "regime_up",
                    "regime_down"
                  ]
                }
              },
              "input_width": {
                "componentInputParameter": "input_width"
              },
              "label_columns": {
                "runtimeValue": {
                  "constant": [
                    "PRICE_std"
                  ]
                }
              },
              "label_width": {
                "componentInputParameter": "label_width"
              },
              "shift": {
                "componentInputParameter": "shift"
              }
            }
          },
          "taskInfo": {
            "name": "Window Single-DC (9 features)"
          }
        }
      }
    },
    "inputDefinitions": {
      "parameters": {
        "dataset_id": {
          "defaultValue": "coindesk",
          "isOptional": true,
          "parameterType": "STRING"
        },
        "dataset_location": {
          "defaultValue": "US",
          "isOptional": true,
          "parameterType": "STRING"
        },
        "dc_thresholds": {
          "defaultValue": [
            0.001,
            0.005,
            0.01,
            0.015
          ],
          "isOptional": true,
          "parameterType": "LIST"
        },
        "end_time": {
          "defaultValue": "2025-08-01 00:00:00",
          "isOptional": true,
          "parameterType": "STRING"
        },
        "ingestion_dataset_id": {
          "defaultValue": "coindesk",
          "isOptional": true,
          "parameterType": "STRING"
        },
        "ingestion_project_id": {
          "defaultValue": "derivatives-417104",
          "isOptional": true,
          "parameterType": "STRING"
        },
        "input_width": {
          "defaultValue": 50.0,
          "isOptional": true,
          "parameterType": "NUMBER_INTEGER"
        },
        "instrument": {
          "defaultValue": "BTC-USD",
          "isOptional": true,
          "parameterType": "STRING"
        },
        "label_width": {
          "defaultValue": 1.0,
          "isOptional": true,
          "parameterType": "NUMBER_INTEGER"
        },
        "pipeline_files_gcs_path": {
          "defaultValue": "gs://derivatives-417104-pl-assets",
          "isOptional": true,
          "parameterType": "STRING"
        },
        "project_id": {
          "defaultValue": "derivatives-417104",
          "isOptional": true,
          "parameterType": "STRING"
        },
        "project_location": {
          "defaultValue": "us-central1",
          "isOptional": true,
          "parameterType": "STRING"
        },
        "shift": {
          "defaultValue": 10.0,
          "isOptional": true,
          "parameterType": "NUMBER_INTEGER"
        },
        "single_dc_threshold": {
          "defaultValue": 0.001,
          "isOptional": true,
          "parameterType": "NUMBER_DOUBLE"
        },
        "staging_bucket": {
          "defaultValue": "gs://derivatives-417104-pl-root",
          "isOptional": true,
          "parameterType": "STRING"
        },
        "start_time": {
          "defaultValue": "2025-05-01 00:00:00",
          "isOptional": true,
          "parameterType": "STRING"
        }
      }
    }
  },
  "schemaVersion": "2.1.0",
  "sdkVersion": "kfp-2.15.2"
}